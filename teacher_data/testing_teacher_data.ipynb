{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1faadc5-8ace-405f-adba-50a1b19c1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from shared_utils.data import CSVPromptDataset\n",
    "\n",
    "# Load the dataset to check for duplicates\n",
    "dataset_path = \"../results_and_data/early_exit_sft_dataset/test/data.csv\"\n",
    "prompt_config_path = \"../results_and_data/early_exit_sft_dataset/test/prompt_config.json\"\n",
    "\n",
    "# Read the CSV to analyze duplicates\n",
    "df = pd.read_csv(dataset_path)\n",
    "print(f\"Original dataset size: {len(df)} rows\")\n",
    "\n",
    "# Check for duplicates in the story column\n",
    "print(f\"Number of unique stories: {df['story'].nunique()}\")\n",
    "print(f\"Number of duplicate story entries: {len(df) - df['story'].nunique()}\")\n",
    "\n",
    "# Show example of duplicates\n",
    "duplicate_stories = df[df.duplicated(subset=['story'], keep=False)]\n",
    "if len(duplicate_stories) > 0:\n",
    "    print(f\"\\nExample of duplicate stories:\")\n",
    "    # Show first duplicate story group\n",
    "    first_dup_story = duplicate_stories.iloc[0]['story']\n",
    "    examples = df[df['story'] == first_dup_story][['story', 'question']].head()\n",
    "    print(examples)\n",
    "\n",
    "# Remove duplicates by keeping one random question per story\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(2)\n",
    "\n",
    "# Randomly shuffle the dataframe\n",
    "df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Keep only the first occurrence of each story (which is now random due to shuffle)\n",
    "df_deduplicated = df_shuffled.drop_duplicates(subset=['story'], keep='first')\n",
    "\n",
    "print(f\"\\nAfter deduplication: {len(df_deduplicated)} rows\")\n",
    "print(f\"Removed {len(df) - len(df_deduplicated)} duplicate entries\")\n",
    "\n",
    "# Save the deduplicated dataset\n",
    "output_path = dataset_path.replace('.csv', '_deduplicated.csv')\n",
    "df_deduplicated.to_csv(output_path, index=False)\n",
    "print(f\"\\nSaved deduplicated dataset to: {output_path}\")\n",
    "\n",
    "# Verify the deduplication\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Original unique stories: {df['story'].nunique()}\")\n",
    "print(f\"Deduplicated unique stories: {df_deduplicated['story'].nunique()}\")\n",
    "print(f\"These should be equal: {df['story'].nunique() == df_deduplicated['story'].nunique()}\")\n",
    "\n",
    "# Show distribution of questions retained\n",
    "if 'question' in df.columns:\n",
    "    print(f\"\\nQuestion distribution in deduplicated data:\")\n",
    "    # If questions have patterns or types, show distribution\n",
    "    # This is just an example - adjust based on your question format\n",
    "    question_starts = df_deduplicated['question'].str.split().str[0].value_counts().head(10)\n",
    "    print(question_starts)\n",
    "\n",
    "# Update your dataset path for the generation process\n",
    "print(f\"\\n⚠️  UPDATE YOUR CODE:\")\n",
    "print(f\"Change: dataset_path = '{dataset_path}'\")\n",
    "print(f\"To:     dataset_path = '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10947c94-fd01-431f-9574-21ed273eaa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def analyze_merged_teacher_data(file_path: str, max_samples: int = 2):\n",
    "\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"File path: {file_path}\")\n",
    "    print(f\"File size: {file_size / 1e9:.2f} GB ({file_size / 1e6:.2f} MB)\")\n",
    "    \n",
    "    sample_count = 0\n",
    "    total_size_estimate = 0\n",
    "    tensor_info = {}\n",
    "    \n",
    "    with gzip.open(file_path, \"rb\") as f:\n",
    "        # Read header\n",
    "        header = pickle.load(f)\n",
    "        print(\"Header/Metadata:\")\n",
    "        if 'metadata' in header:\n",
    "            for key, value in header['metadata'].items():\n",
    "                if isinstance(value, str) and len(value) > 100:\n",
    "                    print(f\"  {key}: {value[:100]}...\")\n",
    "                elif isinstance(value, dict):\n",
    "                    print(f\"  {key}: dict with {len(value)} keys\")\n",
    "                else:\n",
    "                    print(f\"  {key}: {value}\")\n",
    "        \n",
    "        while sample_count < max_samples:\n",
    "            try:\n",
    "                sample = pickle.load(f)\n",
    "                \n",
    "                # Check if it's the end marker\n",
    "                if isinstance(sample, dict) and sample.get('_end'):\n",
    "                    print(f\"\\nReached end marker. Total samples: {sample.get('num_samples', 'unknown')}\")\n",
    "                    break\n",
    "                \n",
    "                sample_count += 1\n",
    "                print(f\"\\nSample {sample_count}:\")\n",
    "                print(f\"  Type: {type(sample)}\")\n",
    "                \n",
    "                if isinstance(sample, dict):\n",
    "                    print(f\"  Keys: {list(sample.keys())}\")\n",
    "                    \n",
    "                    sample_size_mb = 0\n",
    "                    for key, value in sample.items():\n",
    "                        if isinstance(value, torch.Tensor):\n",
    "                            size_mb = (value.numel() * value.element_size()) / (1024 * 1024)\n",
    "                            sample_size_mb += size_mb\n",
    "                            \n",
    "                            # Track tensor info\n",
    "                            if key not in tensor_info:\n",
    "                                tensor_info[key] = {\n",
    "                                    'shape': value.shape,\n",
    "                                    'dtype': value.dtype,\n",
    "                                    'size_mb': size_mb,\n",
    "                                    'has_vocab_dim': any(dim > 10000 for dim in value.shape)\n",
    "                                }\n",
    "                            \n",
    "                            print(f\"    {key}:\")\n",
    "                            print(f\"      Shape: {value.shape}\")\n",
    "                            print(f\"      Dtype: {value.dtype}\")\n",
    "                            print(f\"      Size: {size_mb:.2f} MB\")\n",
    "                            \n",
    "                            # Check for vocabulary dimension\n",
    "                            if any(dim > 10000 for dim in value.shape):\n",
    "                                print(f\"      *** Has vocabulary dimension: {max(value.shape)}\")\n",
    "                        \n",
    "                        elif isinstance(value, str):\n",
    "                            print(f\"    {key}: string ({len(value)} chars)\")\n",
    "                            if len(value) < 200:\n",
    "                                print(f\"      Content: {value}\")\n",
    "                        \n",
    "                        elif isinstance(value, (int, float)):\n",
    "                            print(f\"    {key}: {type(value).__name__} = {value}\")\n",
    "                        \n",
    "                        elif isinstance(value, list):\n",
    "                            print(f\"    {key}: list with {len(value)} items\")\n",
    "                            if value and isinstance(value[0], torch.Tensor):\n",
    "                                print(f\"      First tensor shape: {value[0].shape}\")\n",
    "                        \n",
    "                        else:\n",
    "                            print(f\"    {key}: {type(value)}\")\n",
    "                    \n",
    "                    print(f\"  Total sample size: ~{sample_size_mb:.2f} MB\")\n",
    "                    total_size_estimate += sample_size_mb\n",
    "                \n",
    "                # Clean up\n",
    "                del sample\n",
    "                gc.collect()\n",
    "                \n",
    "            except EOFError:\n",
    "                print(\"\\nReached end of file\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError reading sample: {e}\")\n",
    "                break\n",
    "\n",
    "def main():\n",
    "    file_path = '/workspace/data/teacher_generated_data_gzip/merged_teacher_data.pkl.gz'\n",
    "    analyze_merged_teacher_data(file_path, max_samples=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42927d7-7ebf-42d3-8589-d6b1cac4c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_merged_teacher_data(file_path: str, max_samples: int = 10):\n",
    "    \n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"File path: {file_path}\")\n",
    "    print(f\"File size: {file_size / 1e9:.2f} GB ({file_size / 1e6:.2f} MB)\")\n",
    "    \n",
    "    sample_count = 0\n",
    "    total_size_estimate = 0\n",
    "    tensor_info = {}\n",
    "    \n",
    "    # Collect logprob statistics\n",
    "    all_logprob_samples = []\n",
    "    \n",
    "    with gzip.open(file_path, \"rb\") as f:\n",
    "        # Read header\n",
    "        header = pickle.load(f)\n",
    "        print(\"Header/Metadata:\")\n",
    "        if 'metadata' in header:\n",
    "            for key, value in header['metadata'].items():\n",
    "                if isinstance(value, str) and len(value) > 100:\n",
    "                    print(f\"  {key}: {value[:100]}...\")\n",
    "                elif isinstance(value, dict):\n",
    "                    print(f\"  {key}: dict with {len(value)} keys\")\n",
    "                else:\n",
    "                    print(f\"  {key}: {value}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Analyzing first {max_samples} samples for logprob distribution...\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        while sample_count < max_samples:\n",
    "            try:\n",
    "                sample = pickle.load(f)\n",
    "                \n",
    "                # Check if it's the end marker\n",
    "                if isinstance(sample, dict) and sample.get('_end'):\n",
    "                    print(f\"\\nReached end marker. Total samples: {sample.get('num_samples', 'unknown')}\")\n",
    "                    break\n",
    "                \n",
    "                sample_count += 1\n",
    "                print(f\"\\nSample {sample_count}:\")\n",
    "                print(f\"  Type: {type(sample)}\")\n",
    "                \n",
    "                if isinstance(sample, dict):\n",
    "                    print(f\"  Keys: {list(sample.keys())}\")\n",
    "                    \n",
    "                    sample_size_mb = 0\n",
    "                    for key, value in sample.items():\n",
    "                        if isinstance(value, torch.Tensor):\n",
    "                            size_mb = (value.numel() * value.element_size()) / (1024 * 1024)\n",
    "                            sample_size_mb += size_mb\n",
    "                            \n",
    "                            # Track tensor info\n",
    "                            if key not in tensor_info:\n",
    "                                tensor_info[key] = {\n",
    "                                    'shape': value.shape,\n",
    "                                    'dtype': value.dtype,\n",
    "                                    'size_mb': size_mb,\n",
    "                                    'has_vocab_dim': any(dim > 10000 for dim in value.shape)\n",
    "                                }\n",
    "                            \n",
    "                            print(f\"    {key}:\")\n",
    "                            print(f\"      Shape: {value.shape}\")\n",
    "                            print(f\"      Dtype: {value.dtype}\")\n",
    "                            print(f\"      Size: {size_mb:.2f} MB\")\n",
    "                            \n",
    "                            # Analyze logprob distribution\n",
    "                            if key == 'sft_teacher_final_layer_logprobs':\n",
    "                                print(f\"      *** Analyzing logprob distribution...\")\n",
    "                                \n",
    "                                # Sample some values to analyze\n",
    "                                # Take every 10th position to avoid memory issues\n",
    "                                sampled_logprobs = value[:, ::10, :].flatten()\n",
    "                                \n",
    "                                # Random sample if still too large\n",
    "                                if sampled_logprobs.numel() > 1_000_000:\n",
    "                                    indices = torch.randperm(sampled_logprobs.numel())[:1_000_000]\n",
    "                                    sampled_logprobs = sampled_logprobs[indices]\n",
    "                                \n",
    "                                all_logprob_samples.append(sampled_logprobs.cpu())\n",
    "                                \n",
    "                                # Quick stats\n",
    "                                print(f\"      Min: {value.min().item():.2f}\")\n",
    "                                print(f\"      Max: {value.max().item():.2f}\")\n",
    "                                print(f\"      Mean: {value.mean().item():.2f}\")\n",
    "                                print(f\"      Median: {value.median().item():.2f}\")\n",
    "                            \n",
    "                            # Check for vocabulary dimension\n",
    "                            if any(dim > 10000 for dim in value.shape):\n",
    "                                print(f\"      *** Has vocabulary dimension: {max(value.shape)}\")\n",
    "                        \n",
    "                        elif isinstance(value, str):\n",
    "                            print(f\"    {key}: string ({len(value)} chars)\")\n",
    "                        \n",
    "                        elif isinstance(value, (int, float)):\n",
    "                            print(f\"    {key}: {type(value).__name__} = {value}\")\n",
    "                        \n",
    "                        elif isinstance(value, list):\n",
    "                            print(f\"    {key}: list with {len(value)} items\")\n",
    "                        \n",
    "                        else:\n",
    "                            print(f\"    {key}: {type(value)}\")\n",
    "                    \n",
    "                    print(f\"  Total sample size: ~{sample_size_mb:.2f} MB\")\n",
    "                    total_size_estimate += sample_size_mb\n",
    "                \n",
    "                # Clean up\n",
    "                del sample\n",
    "                gc.collect()\n",
    "                \n",
    "            except EOFError:\n",
    "                print(\"\\nReached end of file\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError reading sample: {e}\")\n",
    "                break\n",
    "    \n",
    "    # Analyze combined logprob distribution\n",
    "    if all_logprob_samples:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"LOGPROB DISTRIBUTION ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Combine all samples and convert to float32 for analysis\n",
    "        all_logprobs = torch.cat(all_logprob_samples).float()\n",
    "        print(f\"\\nTotal logprob values analyzed: {all_logprobs.numel():,}\")\n",
    "        \n",
    "        # Calculate percentiles\n",
    "        percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99, 99.9, 99.99]\n",
    "        print(\"\\nPercentiles:\")\n",
    "        for p in percentiles:\n",
    "            val = torch.quantile(all_logprobs, p/100).item()\n",
    "            prob = np.exp(val)\n",
    "            print(f\"  {p:6.2f}%: {val:8.2f} (prob: {prob:.2e})\")\n",
    "        \n",
    "        # Threshold analysis\n",
    "        print(\"\\nSparsity at different thresholds:\")\n",
    "        thresholds = [0, -1, -2, -5, -8, -10, -12, -15, -20, -25, -30]\n",
    "        for thresh in thresholds:\n",
    "            sparsity = (all_logprobs <= thresh).float().mean().item()\n",
    "            keep_ratio = 1 - sparsity\n",
    "            compression = 1 / keep_ratio if keep_ratio > 0 else float('inf')\n",
    "            print(f\"  Threshold {thresh:3d}: {sparsity:6.1%} sparse ({keep_ratio:6.1%} kept, {compression:6.1f}x compression)\")\n",
    "        \n",
    "        # Find threshold for specific compression ratios\n",
    "        print(\"\\nThresholds for target compression ratios:\")\n",
    "        target_keep_ratios = [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "        for keep_ratio in target_keep_ratios:\n",
    "            threshold = torch.quantile(all_logprobs, 1 - keep_ratio).item()\n",
    "            compression = 1 / keep_ratio\n",
    "            print(f\"  {compression:6.0f}x compression (keep {keep_ratio*100:5.2f}%): threshold = {threshold:6.2f}\")\n",
    "        \n",
    "        # Recommendation\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RECOMMENDATION\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"\"\"\n",
    "Based on the distribution:\n",
    "- For ~1000x compression: use threshold around -11 to -12\n",
    "- For ~500x compression: use threshold around -10\n",
    "- For ~100x compression: use threshold around -7\n",
    "\n",
    "Remember: Lower threshold = more aggressive filtering = better compression but more information loss\n",
    "\"\"\")\n",
    "        \n",
    "        # Optional: Create histogram\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(all_logprobs.numpy(), bins=100, alpha=0.7, edgecolor='black')\n",
    "            plt.axvline(-10, color='red', linestyle='--', label='threshold=-10')\n",
    "            plt.axvline(-15, color='orange', linestyle='--', label='threshold=-15')\n",
    "            plt.xlabel('Log Probability')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title('Distribution of Teacher Log Probabilities')\n",
    "            plt.legend()\n",
    "            plt.savefig('/workspace/logprob_distribution.png')\n",
    "            print(\"\\nHistogram saved to /workspace/logprob_distribution.png\")\n",
    "        except:\n",
    "            print(\"\\nCouldn't create histogram (matplotlib might not be available)\")\n",
    "\n",
    "def main():\n",
    "    file_path = '/workspace/data/teacher_generated_data_gzip/merged_teacher_data.pkl.gz'\n",
    "    analyze_merged_teacher_data(file_path, max_samples=10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a23627-411f-47cf-abc2-bdaca9490a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing KL divergence with threshold=-15.0\n",
      "================================================================================\n",
      "\n",
      "Sample 1:\n",
      "  Shape: torch.Size([1, 271, 151936])\n",
      "  Sparsity: 99.29% (292,580 non-zero out of 41,174,656)\n",
      "  Compression: 140.7x\n",
      "    Position 266: KL=0.001491, Prob mass kept=0.9999, Tokens kept=57/151936\n",
      "    Position 10: KL=0.000193, Prob mass kept=1.0000, Tokens kept=16/151936\n",
      "    Position 169: KL=0.010000, Prob mass kept=0.9996, Tokens kept=363/151936\n",
      "    Position 163: KL=0.003043, Prob mass kept=0.9999, Tokens kept=84/151936\n",
      "    Position 146: KL=0.000054, Prob mass kept=1.0000, Tokens kept=8/151936\n",
      "    Position 126: KL=0.006999, Prob mass kept=0.9998, Tokens kept=699/151936\n",
      "    Position 153: KL=0.000040, Prob mass kept=1.0000, Tokens kept=8/151936\n",
      "    Position 62: KL=0.088796, Prob mass kept=0.9969, Tokens kept=2821/151936\n",
      "    Position 121: KL=0.000695, Prob mass kept=1.0000, Tokens kept=30/151936\n",
      "    Position 208: KL=0.027490, Prob mass kept=0.9990, Tokens kept=1059/151936\n",
      "  Average KL divergence: 0.013880\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 191/271\n",
      "    Original prob for generated: min=0.000000, mean=0.056705\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.056727\n",
      "\n",
      "Sample 2:\n",
      "  Shape: torch.Size([1, 342, 151936])\n",
      "  Sparsity: 98.60% (725,904 non-zero out of 51,962,112)\n",
      "  Compression: 71.6x\n",
      "    Position 341: KL=0.000931, Prob mass kept=1.0000, Tokens kept=46/151936\n",
      "    Position 248: KL=0.023888, Prob mass kept=0.9992, Tokens kept=1345/151936\n",
      "    Position 33: KL=0.002050, Prob mass kept=0.9999, Tokens kept=154/151936\n",
      "    Position 274: KL=0.005612, Prob mass kept=0.9998, Tokens kept=587/151936\n",
      "    Position 94: KL=0.000090, Prob mass kept=1.0000, Tokens kept=12/151936\n",
      "    Position 269: KL=0.051861, Prob mass kept=0.9982, Tokens kept=2256/151936\n",
      "    Position 307: KL=0.005860, Prob mass kept=0.9998, Tokens kept=369/151936\n",
      "    Position 271: KL=0.000066, Prob mass kept=1.0000, Tokens kept=16/151936\n",
      "    Position 283: KL=0.000700, Prob mass kept=1.0000, Tokens kept=105/151936\n",
      "    Position 262: KL=0.005646, Prob mass kept=0.9998, Tokens kept=413/151936\n",
      "  Average KL divergence: 0.009671\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 234/342\n",
      "    Original prob for generated: min=0.000000, mean=0.011952\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.011964\n",
      "\n",
      "Sample 3:\n",
      "  Shape: torch.Size([1, 346, 151936])\n",
      "  Sparsity: 98.98% (538,690 non-zero out of 52,569,856)\n",
      "  Compression: 97.6x\n",
      "    Position 219: KL=0.004376, Prob mass kept=0.9998, Tokens kept=354/151936\n",
      "    Position 162: KL=0.039590, Prob mass kept=0.9986, Tokens kept=917/151936\n",
      "    Position 205: KL=0.023052, Prob mass kept=0.9992, Tokens kept=927/151936\n",
      "    Position 297: KL=0.132122, Prob mass kept=0.9954, Tokens kept=7219/151936\n",
      "    Position 244: KL=0.073869, Prob mass kept=0.9974, Tokens kept=1953/151936\n",
      "    Position 312: KL=0.004204, Prob mass kept=0.9999, Tokens kept=451/151936\n",
      "    Position 260: KL=0.065057, Prob mass kept=0.9977, Tokens kept=2081/151936\n",
      "    Position 133: KL=0.201673, Prob mass kept=0.9930, Tokens kept=12808/151936\n",
      "    Position 141: KL=0.005512, Prob mass kept=0.9998, Tokens kept=531/151936\n",
      "    Position 88: KL=0.070526, Prob mass kept=0.9976, Tokens kept=3778/151936\n",
      "  Average KL divergence: 0.061998\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 210/346\n",
      "    Original prob for generated: min=0.000000, mean=0.028841\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.028869\n",
      "\n",
      "Sample 4:\n",
      "  Shape: torch.Size([1, 399, 151936])\n",
      "  Sparsity: 98.32% (1,018,614 non-zero out of 60,622,464)\n",
      "  Compression: 59.5x\n",
      "    Position 121: KL=0.003355, Prob mass kept=0.9999, Tokens kept=317/151936\n",
      "    Position 351: KL=0.053284, Prob mass kept=0.9982, Tokens kept=2674/151936\n",
      "    Position 322: KL=0.000040, Prob mass kept=1.0000, Tokens kept=10/151936\n",
      "    Position 3: KL=0.001298, Prob mass kept=1.0000, Tokens kept=132/151936\n",
      "    Position 340: KL=0.007885, Prob mass kept=0.9997, Tokens kept=900/151936\n",
      "    Position 20: KL=0.000136, Prob mass kept=1.0000, Tokens kept=7/151936\n",
      "    Position 137: KL=0.199101, Prob mass kept=0.9931, Tokens kept=14486/151936\n",
      "    Position 135: KL=0.006351, Prob mass kept=0.9998, Tokens kept=542/151936\n",
      "    Position 33: KL=0.005010, Prob mass kept=0.9998, Tokens kept=483/151936\n",
      "    Position 171: KL=0.000094, Prob mass kept=1.0000, Tokens kept=25/151936\n",
      "  Average KL divergence: 0.027655\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 253/399\n",
      "    Original prob for generated: min=0.000000, mean=0.033475\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.033507\n",
      "\n",
      "Sample 5:\n",
      "  Shape: torch.Size([1, 284, 151936])\n",
      "  Sparsity: 99.04% (416,027 non-zero out of 43,149,824)\n",
      "  Compression: 103.7x\n",
      "    Position 81: KL=0.002775, Prob mass kept=0.9999, Tokens kept=199/151936\n",
      "    Position 147: KL=0.025940, Prob mass kept=0.9991, Tokens kept=1748/151936\n",
      "    Position 123: KL=0.116647, Prob mass kept=0.9959, Tokens kept=9209/151936\n",
      "    Position 32: KL=0.009710, Prob mass kept=0.9997, Tokens kept=765/151936\n",
      "    Position 185: KL=0.045344, Prob mass kept=0.9984, Tokens kept=2376/151936\n",
      "    Position 160: KL=0.132642, Prob mass kept=0.9954, Tokens kept=7788/151936\n",
      "    Position 113: KL=0.000307, Prob mass kept=1.0000, Tokens kept=29/151936\n",
      "    Position 87: KL=0.020193, Prob mass kept=0.9993, Tokens kept=635/151936\n",
      "    Position 199: KL=0.063685, Prob mass kept=0.9978, Tokens kept=1854/151936\n",
      "    Position 262: KL=0.004244, Prob mass kept=0.9999, Tokens kept=497/151936\n",
      "  Average KL divergence: 0.042149\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 215/284\n",
      "    Original prob for generated: min=0.000000, mean=0.010823\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.010827\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average sparsity: 98.84%\n",
      "Average compression: 94.6x\n",
      "Average KL divergence: 0.031071\n",
      "KL divergence range: [0.000040, 0.201673]\n",
      "\n",
      "Interpretation:\n",
      "- KL < 0.01: Excellent reconstruction\n",
      "- KL < 0.1: Good reconstruction\n",
      "- KL < 1.0: Acceptable for most purposes\n",
      "- KL > 1.0: Significant information loss\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def test_kl_divergence_sparse(file_path: str, num_samples: int = 30, threshold: float = -12.0):\n",
    "    \"\"\"Test KL divergence between original and sparse representations\"\"\"\n",
    "    \n",
    "    print(f\"Testing KL divergence with threshold={threshold}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    sample_count = 0\n",
    "    kl_results = []\n",
    "    \n",
    "    with gzip.open(file_path, \"rb\") as f:\n",
    "        # Skip header\n",
    "        header = pickle.load(f)\n",
    "        \n",
    "        while sample_count < num_samples:\n",
    "            try:\n",
    "                sample = pickle.load(f)\n",
    "                \n",
    "                # Check if it's the end marker\n",
    "                if isinstance(sample, dict) and sample.get('_end'):\n",
    "                    break\n",
    "                \n",
    "                if 'sft_teacher_final_layer_logprobs' in sample:\n",
    "                    sample_count += 1\n",
    "                    \n",
    "                    # Get original logprobs\n",
    "                    original_logprobs = sample['sft_teacher_final_layer_logprobs']\n",
    "                    batch_size, seq_len, vocab_size = original_logprobs.shape\n",
    "                    \n",
    "                    print(f\"\\nSample {sample_count}:\")\n",
    "                    print(f\"  Shape: {original_logprobs.shape}\")\n",
    "                    \n",
    "                    # Create sparse version\n",
    "                    sparse_logprobs = (original_logprobs * (original_logprobs > threshold)).to_sparse()\n",
    "                    \n",
    "                    # Calculate sparsity\n",
    "                    nnz = sparse_logprobs._nnz()\n",
    "                    total_elements = batch_size * seq_len * vocab_size\n",
    "                    sparsity = 1 - (nnz / total_elements)\n",
    "                    print(f\"  Sparsity: {sparsity:.2%} ({nnz:,} non-zero out of {total_elements:,})\")\n",
    "                    print(f\"  Compression: {total_elements/nnz:.1f}x\")\n",
    "                    \n",
    "                    # Convert back to dense for comparison\n",
    "                    reconstructed_logprobs = sparse_logprobs.to_dense()\n",
    "                    \n",
    "                    # Fill zeros with very negative value (representing zero probability)\n",
    "                    mask_zeros = (reconstructed_logprobs == 0)\n",
    "                    reconstructed_logprobs[mask_zeros] = -50.0\n",
    "                    \n",
    "                    # Convert to probabilities\n",
    "                    original_probs = F.softmax(original_logprobs.float(), dim=-1)\n",
    "                    reconstructed_probs = F.softmax(reconstructed_logprobs.float(), dim=-1)\n",
    "                    \n",
    "                    # Calculate KL divergence for each position\n",
    "                    # KL(P||Q) = sum(P * log(P/Q))\n",
    "                    # We'll calculate for a few random positions to avoid memory issues\n",
    "                    \n",
    "                    num_positions_to_test = min(10, seq_len)\n",
    "                    position_indices = torch.randperm(seq_len)[:num_positions_to_test]\n",
    "                    \n",
    "                    position_kls = []\n",
    "                    for pos_idx in position_indices:\n",
    "                        p = original_probs[0, pos_idx]  # Original distribution\n",
    "                        q = reconstructed_probs[0, pos_idx]  # Sparse reconstruction\n",
    "                        \n",
    "                        # Use PyTorch's KL divergence function which handles numerical issues\n",
    "                        # First convert back to log space\n",
    "                        log_p = torch.log(p + 1e-20)\n",
    "                        log_q = torch.log(q + 1e-20)\n",
    "                        \n",
    "                        # KL divergence using stable computation\n",
    "                        kl_div = F.kl_div(log_q, p, reduction='sum', log_target=False).item()\n",
    "                        \n",
    "                        # Alternative manual calculation with better numerical stability\n",
    "                        # Only calculate KL where p > 0 to avoid numerical issues\n",
    "                        valid_mask = p > 1e-10\n",
    "                        if valid_mask.sum() > 0:\n",
    "                            p_valid = p[valid_mask]\n",
    "                            q_valid = q[valid_mask]\n",
    "                            kl_manual = (p_valid * torch.log(p_valid / (q_valid + 1e-20))).sum().item()\n",
    "                        else:\n",
    "                            kl_manual = 0.0\n",
    "                        \n",
    "                        # Use the manual calculation as it's more stable\n",
    "                        position_kls.append(kl_manual)\n",
    "                        \n",
    "                        # Also calculate how much probability mass we kept\n",
    "                        kept_mask = original_logprobs[0, pos_idx] > threshold\n",
    "                        prob_mass_kept = p[kept_mask].sum().item()\n",
    "                        num_kept = kept_mask.sum().item()\n",
    "                        \n",
    "                        print(f\"    Position {pos_idx}: KL={kl_manual:.6f}, Prob mass kept={prob_mass_kept:.4f}, Tokens kept={num_kept}/{vocab_size}\")\n",
    "                    \n",
    "                    # Average KL for this sample\n",
    "                    avg_kl = np.mean(position_kls)\n",
    "                    print(f\"  Average KL divergence: {avg_kl:.6f}\")\n",
    "                    \n",
    "                    # Also test with generated tokens specifically\n",
    "                    if 'sft_teacher_generated_tokens' in sample:\n",
    "                        generated_tokens = sample['sft_teacher_generated_tokens']\n",
    "                        print(f\"\\n  Testing on generated tokens specifically:\")\n",
    "                        \n",
    "                        # Get probabilities for the actually generated tokens\n",
    "                        batch_indices = torch.arange(batch_size).unsqueeze(1)\n",
    "                        seq_indices = torch.arange(min(seq_len, generated_tokens.shape[1]))\n",
    "                        \n",
    "                        # Get the probabilities assigned to the generated tokens\n",
    "                        orig_probs_for_generated = original_probs[batch_indices, seq_indices, generated_tokens[0, :seq_len]]\n",
    "                        recon_probs_for_generated = reconstructed_probs[batch_indices, seq_indices, generated_tokens[0, :seq_len]]\n",
    "                        \n",
    "                        # Check if any generated tokens were filtered out\n",
    "                        filtered_out = (original_logprobs[batch_indices, seq_indices, generated_tokens[0, :seq_len]] <= threshold).sum()\n",
    "                        print(f\"    Generated tokens filtered out: {filtered_out}/{seq_len}\")\n",
    "                        print(f\"    Original prob for generated: min={orig_probs_for_generated.min():.6f}, mean={orig_probs_for_generated.mean():.6f}\")\n",
    "                        print(f\"    Reconstructed prob for generated: min={recon_probs_for_generated.min():.6f}, mean={recon_probs_for_generated.mean():.6f}\")\n",
    "                    \n",
    "                    kl_results.append({\n",
    "                        'sample_id': sample_count,\n",
    "                        'shape': original_logprobs.shape,\n",
    "                        'sparsity': sparsity,\n",
    "                        'compression': total_elements/nnz,\n",
    "                        'avg_kl': avg_kl,\n",
    "                        'position_kls': position_kls\n",
    "                    })\n",
    "                    \n",
    "            except EOFError:\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                break\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if kl_results:\n",
    "        avg_sparsity = np.mean([r['sparsity'] for r in kl_results])\n",
    "        avg_compression = np.mean([r['compression'] for r in kl_results])\n",
    "        avg_kl = np.mean([r['avg_kl'] for r in kl_results])\n",
    "        all_position_kls = [kl for r in kl_results for kl in r['position_kls']]\n",
    "        \n",
    "        print(f\"Average sparsity: {avg_sparsity:.2%}\")\n",
    "        print(f\"Average compression: {avg_compression:.1f}x\")\n",
    "        print(f\"Average KL divergence: {avg_kl:.6f}\")\n",
    "        print(f\"KL divergence range: [{min(all_position_kls):.6f}, {max(all_position_kls):.6f}]\")\n",
    "        \n",
    "        print(f\"\\nInterpretation:\")\n",
    "        print(f\"- KL < 0.01: Excellent reconstruction\")\n",
    "        print(f\"- KL < 0.1: Good reconstruction\")\n",
    "        print(f\"- KL < 1.0: Acceptable for most purposes\")\n",
    "        print(f\"- KL > 1.0: Significant information loss\")\n",
    "\n",
    "def main():\n",
    "    file_path = '/workspace/data/teacher_generated_data_gzip/merged_teacher_data.pkl.gz'\n",
    "    test_kl_divergence_sparse(file_path, num_samples=5, threshold=-15.0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff7d8546-0eef-4254-a042-87b965880303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing KL divergence with threshold=-14.0\n",
      "================================================================================\n",
      "\n",
      "Sample 1:\n",
      "  Shape: torch.Size([1, 271, 151936])\n",
      "  Sparsity: 99.69% (127,153 non-zero out of 41,174,656)\n",
      "  Compression: 323.8x\n",
      "    Position 266: KL=0.001869, Prob mass kept=0.9999, Tokens kept=32/151936\n",
      "    Position 10: KL=0.000292, Prob mass kept=1.0000, Tokens kept=9/151936\n",
      "    Position 169: KL=0.012788, Prob mass kept=0.9995, Tokens kept=180/151936\n",
      "    Position 163: KL=0.003699, Prob mass kept=0.9999, Tokens kept=41/151936\n",
      "    Position 146: KL=0.000137, Prob mass kept=1.0000, Tokens kept=3/151936\n",
      "    Position 126: KL=0.011046, Prob mass kept=0.9996, Tokens kept=440/151936\n",
      "    Position 153: KL=0.000101, Prob mass kept=1.0000, Tokens kept=4/151936\n",
      "    Position 62: KL=0.117000, Prob mass kept=0.9960, Tokens kept=921/151936\n",
      "    Position 121: KL=0.000800, Prob mass kept=1.0000, Tokens kept=22/151936\n",
      "    Position 208: KL=0.036588, Prob mass kept=0.9987, Tokens kept=449/151936\n",
      "  Average KL divergence: 0.018432\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 199/271\n",
      "    Original prob for generated: min=0.000000, mean=0.056705\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.056736\n",
      "\n",
      "Sample 2:\n",
      "  Shape: torch.Size([1, 342, 151936])\n",
      "  Sparsity: 99.37% (325,274 non-zero out of 51,962,112)\n",
      "  Compression: 159.7x\n",
      "    Position 341: KL=0.001219, Prob mass kept=1.0000, Tokens kept=28/151936\n",
      "    Position 248: KL=0.033267, Prob mass kept=0.9989, Tokens kept=729/151936\n",
      "    Position 33: KL=0.002840, Prob mass kept=0.9999, Tokens kept=101/151936\n",
      "    Position 274: KL=0.008805, Prob mass kept=0.9997, Tokens kept=370/151936\n",
      "    Position 94: KL=0.000138, Prob mass kept=1.0000, Tokens kept=9/151936\n",
      "    Position 269: KL=0.069202, Prob mass kept=0.9976, Tokens kept=1084/151936\n",
      "    Position 307: KL=0.008247, Prob mass kept=0.9997, Tokens kept=217/151936\n",
      "    Position 271: KL=0.000108, Prob mass kept=1.0000, Tokens kept=12/151936\n",
      "    Position 283: KL=0.001395, Prob mass kept=1.0000, Tokens kept=63/151936\n",
      "    Position 262: KL=0.008330, Prob mass kept=0.9997, Tokens kept=237/151936\n",
      "  Average KL divergence: 0.013355\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 252/342\n",
      "    Original prob for generated: min=0.000000, mean=0.011952\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.011969\n",
      "\n",
      "Sample 3:\n",
      "  Shape: torch.Size([1, 346, 151936])\n",
      "  Sparsity: 99.55% (236,112 non-zero out of 52,569,856)\n",
      "  Compression: 222.6x\n",
      "    Position 219: KL=0.006263, Prob mass kept=0.9998, Tokens kept=235/151936\n",
      "    Position 162: KL=0.048711, Prob mass kept=0.9983, Tokens kept=286/151936\n",
      "    Position 205: KL=0.030244, Prob mass kept=0.9990, Tokens kept=456/151936\n",
      "    Position 297: KL=0.196265, Prob mass kept=0.9933, Tokens kept=2924/151936\n",
      "    Position 244: KL=0.092989, Prob mass kept=0.9968, Tokens kept=653/151936\n",
      "    Position 312: KL=0.006993, Prob mass kept=0.9998, Tokens kept=270/151936\n",
      "    Position 260: KL=0.085553, Prob mass kept=0.9971, Tokens kept=717/151936\n",
      "    Position 133: KL=0.320567, Prob mass kept=0.9891, Tokens kept=4886/151936\n",
      "    Position 141: KL=0.008340, Prob mass kept=0.9997, Tokens kept=349/151936\n",
      "    Position 88: KL=0.101671, Prob mass kept=0.9965, Tokens kept=1704/151936\n",
      "  Average KL divergence: 0.089760\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 233/346\n",
      "    Original prob for generated: min=0.000000, mean=0.028841\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.028883\n",
      "\n",
      "Sample 4:\n",
      "  Shape: torch.Size([1, 399, 151936])\n",
      "  Sparsity: 99.25% (457,547 non-zero out of 60,622,464)\n",
      "  Compression: 132.5x\n",
      "    Position 121: KL=0.005324, Prob mass kept=0.9998, Tokens kept=192/151936\n",
      "    Position 351: KL=0.075642, Prob mass kept=0.9974, Tokens kept=1200/151936\n",
      "    Position 322: KL=0.000113, Prob mass kept=1.0000, Tokens kept=6/151936\n",
      "    Position 3: KL=0.002162, Prob mass kept=0.9999, Tokens kept=74/151936\n",
      "    Position 340: KL=0.012826, Prob mass kept=0.9996, Tokens kept=584/151936\n",
      "    Position 20: KL=0.000173, Prob mass kept=1.0000, Tokens kept=4/151936\n",
      "    Position 137: KL=0.330653, Prob mass kept=0.9888, Tokens kept=5765/151936\n",
      "    Position 135: KL=0.009220, Prob mass kept=0.9997, Tokens kept=343/151936\n",
      "    Position 33: KL=0.008014, Prob mass kept=0.9997, Tokens kept=291/151936\n",
      "    Position 171: KL=0.000232, Prob mass kept=1.0000, Tokens kept=17/151936\n",
      "  Average KL divergence: 0.044436\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 272/399\n",
      "    Original prob for generated: min=0.000000, mean=0.033475\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.033522\n",
      "\n",
      "Sample 5:\n",
      "  Shape: torch.Size([1, 284, 151936])\n",
      "  Sparsity: 99.56% (191,219 non-zero out of 43,149,824)\n",
      "  Compression: 225.7x\n",
      "    Position 81: KL=0.003931, Prob mass kept=0.9999, Tokens kept=127/151936\n",
      "    Position 147: KL=0.038178, Prob mass kept=0.9987, Tokens kept=958/151936\n",
      "    Position 123: KL=0.187588, Prob mass kept=0.9936, Tokens kept=4512/151936\n",
      "    Position 32: KL=0.014700, Prob mass kept=0.9995, Tokens kept=447/151936\n",
      "    Position 185: KL=0.064023, Prob mass kept=0.9978, Tokens kept=1148/151936\n",
      "    Position 160: KL=0.196513, Prob mass kept=0.9933, Tokens kept=3546/151936\n",
      "    Position 113: KL=0.000515, Prob mass kept=1.0000, Tokens kept=18/151936\n",
      "    Position 87: KL=0.025614, Prob mass kept=0.9991, Tokens kept=275/151936\n",
      "    Position 199: KL=0.081912, Prob mass kept=0.9972, Tokens kept=634/151936\n",
      "    Position 262: KL=0.007460, Prob mass kept=0.9997, Tokens kept=300/151936\n",
      "  Average KL divergence: 0.062043\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 228/284\n",
      "    Original prob for generated: min=0.000000, mean=0.010823\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.010828\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average sparsity: 99.48%\n",
      "Average compression: 212.9x\n",
      "Average KL divergence: 0.045605\n",
      "KL divergence range: [0.000101, 0.330653]\n",
      "\n",
      "Interpretation:\n",
      "- KL < 0.01: Excellent reconstruction\n",
      "- KL < 0.1: Good reconstruction\n",
      "- KL < 1.0: Acceptable for most purposes\n",
      "- KL > 1.0: Significant information loss\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    file_path = '/workspace/data/teacher_generated_data_gzip/merged_teacher_data.pkl.gz'\n",
    "    test_kl_divergence_sparse(file_path, num_samples=5, threshold=-14.0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f85337e-801c-438d-9aba-65a4a7b60a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing KL divergence with threshold=-13.0\n",
      "================================================================================\n",
      "\n",
      "Sample 1:\n",
      "  Shape: torch.Size([1, 271, 151936])\n",
      "  Sparsity: 99.86% (56,197 non-zero out of 41,174,656)\n",
      "  Compression: 732.7x\n",
      "    Position 266: KL=0.002586, Prob mass kept=0.9999, Tokens kept=17/151936\n",
      "    Position 10: KL=0.000341, Prob mass kept=1.0000, Tokens kept=8/151936\n",
      "    Position 169: KL=0.016743, Prob mass kept=0.9994, Tokens kept=89/151936\n",
      "    Position 163: KL=0.004540, Prob mass kept=0.9998, Tokens kept=22/151936\n",
      "    Position 146: KL=0.000206, Prob mass kept=1.0000, Tokens kept=2/151936\n",
      "    Position 126: KL=0.018748, Prob mass kept=0.9994, Tokens kept=261/151936\n",
      "    Position 153: KL=0.000101, Prob mass kept=1.0000, Tokens kept=4/151936\n",
      "    Position 62: KL=0.141363, Prob mass kept=0.9952, Tokens kept=332/151936\n",
      "    Position 121: KL=0.001039, Prob mass kept=1.0000, Tokens kept=15/151936\n",
      "    Position 208: KL=0.046690, Prob mass kept=0.9984, Tokens kept=212/151936\n",
      "  Average KL divergence: 0.023236\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 212/271\n",
      "    Original prob for generated: min=0.000000, mean=0.056705\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.056747\n",
      "\n",
      "Sample 2:\n",
      "  Shape: torch.Size([1, 342, 151936])\n",
      "  Sparsity: 99.72% (147,435 non-zero out of 51,962,112)\n",
      "  Compression: 352.4x\n",
      "    Position 341: KL=0.001683, Prob mass kept=0.9999, Tokens kept=18/151936\n",
      "    Position 248: KL=0.048654, Prob mass kept=0.9984, Tokens kept=372/151936\n",
      "    Position 33: KL=0.004571, Prob mass kept=0.9998, Tokens kept=57/151936\n",
      "    Position 274: KL=0.014424, Prob mass kept=0.9995, Tokens kept=236/151936\n",
      "    Position 94: KL=0.000311, Prob mass kept=1.0000, Tokens kept=5/151936\n",
      "    Position 269: KL=0.090830, Prob mass kept=0.9969, Tokens kept=579/151936\n",
      "    Position 307: KL=0.012386, Prob mass kept=0.9996, Tokens kept=121/151936\n",
      "    Position 271: KL=0.000298, Prob mass kept=1.0000, Tokens kept=8/151936\n",
      "    Position 283: KL=0.002277, Prob mass kept=0.9999, Tokens kept=44/151936\n",
      "    Position 262: KL=0.012370, Prob mass kept=0.9996, Tokens kept=146/151936\n",
      "  Average KL divergence: 0.018780\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 270/342\n",
      "    Original prob for generated: min=0.000000, mean=0.011952\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.011975\n",
      "\n",
      "Sample 3:\n",
      "  Shape: torch.Size([1, 346, 151936])\n",
      "  Sparsity: 99.79% (108,984 non-zero out of 52,569,856)\n",
      "  Compression: 482.4x\n",
      "    Position 219: KL=0.011260, Prob mass kept=0.9996, Tokens kept=120/151936\n",
      "    Position 162: KL=0.055978, Prob mass kept=0.9981, Tokens kept=109/151936\n",
      "    Position 205: KL=0.039326, Prob mass kept=0.9987, Tokens kept=244/151936\n",
      "    Position 297: KL=0.266350, Prob mass kept=0.9911, Tokens kept=1252/151936\n",
      "    Position 244: KL=0.109766, Prob mass kept=0.9963, Tokens kept=237/151936\n",
      "    Position 312: KL=0.013046, Prob mass kept=0.9996, Tokens kept=135/151936\n",
      "    Position 260: KL=0.103495, Prob mass kept=0.9965, Tokens kept=281/151936\n",
      "    Position 133: KL=0.450137, Prob mass kept=0.9850, Tokens kept=1788/151936\n",
      "    Position 141: KL=0.013834, Prob mass kept=0.9995, Tokens kept=225/151936\n",
      "    Position 88: KL=0.141769, Prob mass kept=0.9953, Tokens kept=742/151936\n",
      "  Average KL divergence: 0.120496\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 251/346\n",
      "    Original prob for generated: min=0.000000, mean=0.028841\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.028899\n",
      "\n",
      "Sample 4:\n",
      "  Shape: torch.Size([1, 399, 151936])\n",
      "  Sparsity: 99.66% (205,900 non-zero out of 60,622,464)\n",
      "  Compression: 294.4x\n",
      "    Position 121: KL=0.008294, Prob mass kept=0.9997, Tokens kept=127/151936\n",
      "    Position 351: KL=0.101690, Prob mass kept=0.9966, Tokens kept=589/151936\n",
      "    Position 322: KL=0.000113, Prob mass kept=1.0000, Tokens kept=6/151936\n",
      "    Position 3: KL=0.003625, Prob mass kept=0.9999, Tokens kept=42/151936\n",
      "    Position 340: KL=0.021890, Prob mass kept=0.9993, Tokens kept=377/151936\n",
      "    Position 20: KL=0.000173, Prob mass kept=1.0000, Tokens kept=4/151936\n",
      "    Position 137: KL=0.481095, Prob mass kept=0.9840, Tokens kept=2174/151936\n",
      "    Position 135: KL=0.014718, Prob mass kept=0.9995, Tokens kept=217/151936\n",
      "    Position 33: KL=0.013153, Prob mass kept=0.9996, Tokens kept=175/151936\n",
      "    Position 171: KL=0.000354, Prob mass kept=1.0000, Tokens kept=14/151936\n",
      "  Average KL divergence: 0.064511\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 287/399\n",
      "    Original prob for generated: min=0.000000, mean=0.033475\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.033541\n",
      "\n",
      "Sample 5:\n",
      "  Shape: torch.Size([1, 284, 151936])\n",
      "  Sparsity: 99.79% (92,585 non-zero out of 43,149,824)\n",
      "  Compression: 466.1x\n",
      "    Position 81: KL=0.006027, Prob mass kept=0.9998, Tokens kept=80/151936\n",
      "    Position 147: KL=0.055970, Prob mass kept=0.9981, Tokens kept=529/151936\n",
      "    Position 123: KL=0.281985, Prob mass kept=0.9906, Tokens kept=2286/151936\n",
      "    Position 32: KL=0.021508, Prob mass kept=0.9993, Tokens kept=287/151936\n",
      "    Position 185: KL=0.088191, Prob mass kept=0.9971, Tokens kept=575/151936\n",
      "    Position 160: KL=0.272447, Prob mass kept=0.9909, Tokens kept=1746/151936\n",
      "    Position 113: KL=0.000671, Prob mass kept=1.0000, Tokens kept=15/151936\n",
      "    Position 87: KL=0.032226, Prob mass kept=0.9989, Tokens kept=127/151936\n",
      "    Position 199: KL=0.098748, Prob mass kept=0.9967, Tokens kept=233/151936\n",
      "    Position 262: KL=0.011618, Prob mass kept=0.9996, Tokens kept=207/151936\n",
      "  Average KL divergence: 0.086939\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 233/284\n",
      "    Original prob for generated: min=0.000000, mean=0.010823\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.010830\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average sparsity: 99.76%\n",
      "Average compression: 465.6x\n",
      "Average KL divergence: 0.062792\n",
      "KL divergence range: [0.000101, 0.481095]\n",
      "\n",
      "Interpretation:\n",
      "- KL < 0.01: Excellent reconstruction\n",
      "- KL < 0.1: Good reconstruction\n",
      "- KL < 1.0: Acceptable for most purposes\n",
      "- KL > 1.0: Significant information loss\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    file_path = '/workspace/data/teacher_generated_data_gzip/merged_teacher_data.pkl.gz'\n",
    "    test_kl_divergence_sparse(file_path, num_samples=5, threshold=-13.0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a532c406-c732-4064-8d0e-368ceda409ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing KL divergence with threshold=-12.0\n",
      "================================================================================\n",
      "\n",
      "Sample 1:\n",
      "  Shape: torch.Size([1, 271, 151936])\n",
      "  Sparsity: 99.94% (26,741 non-zero out of 41,174,656)\n",
      "  Compression: 1539.8x\n",
      "    Position 266: KL=0.003577, Prob mass kept=0.9999, Tokens kept=9/151936\n",
      "    Position 10: KL=0.000516, Prob mass kept=1.0000, Tokens kept=6/151936\n",
      "    Position 169: KL=0.020890, Prob mass kept=0.9993, Tokens kept=53/151936\n",
      "    Position 163: KL=0.005183, Prob mass kept=0.9998, Tokens kept=17/151936\n",
      "    Position 146: KL=0.000206, Prob mass kept=1.0000, Tokens kept=2/151936\n",
      "    Position 126: KL=0.030562, Prob mass kept=0.9990, Tokens kept=168/151936\n",
      "    Position 153: KL=0.000524, Prob mass kept=1.0000, Tokens kept=1/151936\n",
      "    Position 62: KL=0.163167, Prob mass kept=0.9946, Tokens kept=137/151936\n",
      "    Position 121: KL=0.001843, Prob mass kept=0.9999, Tokens kept=9/151936\n",
      "    Position 208: KL=0.062464, Prob mass kept=0.9979, Tokens kept=79/151936\n",
      "  Average KL divergence: 0.028893\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 217/271\n",
      "    Original prob for generated: min=0.000000, mean=0.056705\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.056759\n",
      "\n",
      "Sample 2:\n",
      "  Shape: torch.Size([1, 342, 151936])\n",
      "  Sparsity: 99.87% (70,135 non-zero out of 51,962,112)\n",
      "  Compression: 740.9x\n",
      "    Position 341: KL=0.002200, Prob mass kept=0.9999, Tokens kept=13/151936\n",
      "    Position 248: KL=0.069570, Prob mass kept=0.9977, Tokens kept=200/151936\n",
      "    Position 33: KL=0.008313, Prob mass kept=0.9997, Tokens kept=28/151936\n",
      "    Position 274: KL=0.024347, Prob mass kept=0.9992, Tokens kept=150/151936\n",
      "    Position 94: KL=0.000587, Prob mass kept=1.0000, Tokens kept=3/151936\n",
      "    Position 269: KL=0.122315, Prob mass kept=0.9960, Tokens kept=313/151936\n",
      "    Position 307: KL=0.018971, Prob mass kept=0.9994, Tokens kept=64/151936\n",
      "    Position 271: KL=0.000943, Prob mass kept=1.0000, Tokens kept=4/151936\n",
      "    Position 283: KL=0.004201, Prob mass kept=0.9999, Tokens kept=29/151936\n",
      "    Position 262: KL=0.019598, Prob mass kept=0.9994, Tokens kept=90/151936\n",
      "  Average KL divergence: 0.027105\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 281/342\n",
      "    Original prob for generated: min=0.000000, mean=0.011952\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.011982\n",
      "\n",
      "Sample 3:\n",
      "  Shape: torch.Size([1, 346, 151936])\n",
      "  Sparsity: 99.90% (54,438 non-zero out of 52,569,856)\n",
      "  Compression: 965.7x\n",
      "    Position 219: KL=0.018092, Prob mass kept=0.9994, Tokens kept=67/151936\n",
      "    Position 162: KL=0.063459, Prob mass kept=0.9979, Tokens kept=41/151936\n",
      "    Position 205: KL=0.051089, Prob mass kept=0.9983, Tokens kept=146/151936\n",
      "    Position 297: KL=0.346714, Prob mass kept=0.9886, Tokens kept=565/151936\n",
      "    Position 244: KL=0.126909, Prob mass kept=0.9958, Tokens kept=87/151936\n",
      "    Position 312: KL=0.020455, Prob mass kept=0.9993, Tokens kept=75/151936\n",
      "    Position 260: KL=0.124759, Prob mass kept=0.9958, Tokens kept=98/151936\n",
      "    Position 133: KL=0.580609, Prob mass kept=0.9810, Tokens kept=667/151936\n",
      "    Position 141: KL=0.026125, Prob mass kept=0.9992, Tokens kept=126/151936\n",
      "    Position 88: KL=0.192372, Prob mass kept=0.9937, Tokens kept=316/151936\n",
      "  Average KL divergence: 0.155058\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 257/346\n",
      "    Original prob for generated: min=0.000000, mean=0.028841\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.028917\n",
      "\n",
      "Sample 4:\n",
      "  Shape: torch.Size([1, 399, 151936])\n",
      "  Sparsity: 99.84% (97,015 non-zero out of 60,622,464)\n",
      "  Compression: 624.9x\n",
      "    Position 121: KL=0.013763, Prob mass kept=0.9996, Tokens kept=82/151936\n",
      "    Position 351: KL=0.131657, Prob mass kept=0.9957, Tokens kept=331/151936\n",
      "    Position 322: KL=0.000113, Prob mass kept=1.0000, Tokens kept=6/151936\n",
      "    Position 3: KL=0.005512, Prob mass kept=0.9998, Tokens kept=26/151936\n",
      "    Position 340: KL=0.039245, Prob mass kept=0.9988, Tokens kept=234/151936\n",
      "    Position 20: KL=0.000287, Prob mass kept=1.0000, Tokens kept=3/151936\n",
      "    Position 137: KL=0.640532, Prob mass kept=0.9791, Tokens kept=812/151936\n",
      "    Position 135: KL=0.024957, Prob mass kept=0.9992, Tokens kept=131/151936\n",
      "    Position 33: KL=0.023115, Prob mass kept=0.9993, Tokens kept=98/151936\n",
      "    Position 171: KL=0.001355, Prob mass kept=1.0000, Tokens kept=8/151936\n",
      "  Average KL divergence: 0.088054\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 303/399\n",
      "    Original prob for generated: min=0.000000, mean=0.033475\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.033564\n",
      "\n",
      "Sample 5:\n",
      "  Shape: torch.Size([1, 284, 151936])\n",
      "  Sparsity: 99.89% (47,793 non-zero out of 43,149,824)\n",
      "  Compression: 902.8x\n",
      "    Position 81: KL=0.009396, Prob mass kept=0.9997, Tokens kept=50/151936\n",
      "    Position 147: KL=0.083983, Prob mass kept=0.9973, Tokens kept=301/151936\n",
      "    Position 123: KL=0.414573, Prob mass kept=0.9866, Tokens kept=1193/151936\n",
      "    Position 32: KL=0.035084, Prob mass kept=0.9989, Tokens kept=173/151936\n",
      "    Position 185: KL=0.122858, Prob mass kept=0.9960, Tokens kept=286/151936\n",
      "    Position 160: KL=0.365014, Prob mass kept=0.9880, Tokens kept=974/151936\n",
      "    Position 113: KL=0.001202, Prob mass kept=1.0000, Tokens kept=11/151936\n",
      "    Position 87: KL=0.039080, Prob mass kept=0.9987, Tokens kept=67/151936\n",
      "    Position 199: KL=0.113933, Prob mass kept=0.9962, Tokens kept=99/151936\n",
      "    Position 262: KL=0.020030, Prob mass kept=0.9994, Tokens kept=137/151936\n",
      "  Average KL divergence: 0.120515\n",
      "\n",
      "  Testing on generated tokens specifically:\n",
      "    Generated tokens filtered out: 248/284\n",
      "    Original prob for generated: min=0.000000, mean=0.010823\n",
      "    Reconstructed prob for generated: min=0.000000, mean=0.010831\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average sparsity: 99.89%\n",
      "Average compression: 954.8x\n",
      "Average KL divergence: 0.083925\n",
      "KL divergence range: [0.000113, 0.640532]\n",
      "\n",
      "Interpretation:\n",
      "- KL < 0.01: Excellent reconstruction\n",
      "- KL < 0.1: Good reconstruction\n",
      "- KL < 1.0: Acceptable for most purposes\n",
      "- KL > 1.0: Significant information loss\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    file_path = '/workspace/data/teacher_generated_data_gzip/merged_teacher_data.pkl.gz'\n",
    "    test_kl_divergence_sparse(file_path, num_samples=5, threshold=-12.0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc08b354-df32-4d94-97a3-0fc3c6db9206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
