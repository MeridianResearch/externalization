{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b72ac18-1889-451b-8335-22ac81882390",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import gc\n",
    "from typing import Iterator\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from shared_utils.data import CSVPromptDataset\n",
    "from shared_utils.load import get_tokenizer, configs_from_yaml\n",
    "from shared_utils.generate import generate_text\n",
    "\n",
    "from early_exit.util import get_model\n",
    "\n",
    "from early_exit.patching import replace_attention_layers, set_transformer_early_exit_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "254bcc66-c33b-49f2-bbc1-9505e3a3781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_exit_samples = 4\n",
    "device = \"cuda\"\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model_config_path = \"../config_deepseek.yaml\"\n",
    "dataset_path = \"../results_and_data/early_exit_sft_dataset/test/data_deduplicated.csv\"\n",
    "prompt_config_path = \"../results_and_data/early_exit_sft_dataset/test/prompt_config.json\"\n",
    "batch_size = 1\n",
    "chunk_size = 100 #for saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f6dc04e-5a54-4c68-b480-6d05fe5859c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_dir = \"teacher_generated_data\"\n",
    "output_dir = \"/workspace/data/teacher_generated_data_gzip\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1023f2ed-3622-4b0c-91d6-9beed3da3195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81d76f76621403a9ba544a6b319db71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bf6a2004fd4ef189d95b53710d420e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25a6c9f8ab54bd39720bf84d08d06db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbd5f2f29ed41ceb7a657562bf5082a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a540fed30b49f2bcb8caf61f6c0570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(model_name)\n",
    "config = configs_from_yaml(model_config_path, tokenizer.eos_token_id)\n",
    "model = get_model(model_name, config['model'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14744edc-b011-4cc4-8bb8-19d5af046092",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CSVPromptDataset(dataset_path, prompt_config_path)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe64f03e-5bce-45a6-9fca-826ba9b7cb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing layer model.layers.0\n",
      "replacing layer model.layers.1\n",
      "replacing layer model.layers.2\n",
      "replacing layer model.layers.3\n",
      "replacing layer model.layers.4\n",
      "replacing layer model.layers.5\n",
      "replacing layer model.layers.6\n",
      "replacing layer model.layers.7\n",
      "replacing layer model.layers.8\n",
      "replacing layer model.layers.9\n",
      "replacing layer model.layers.10\n",
      "replacing layer model.layers.11\n",
      "replacing layer model.layers.12\n",
      "replacing layer model.layers.13\n",
      "replacing layer model.layers.14\n",
      "replacing layer model.layers.15\n",
      "replacing layer model.layers.16\n",
      "replacing layer model.layers.17\n",
      "replacing layer model.layers.18\n",
      "replacing layer model.layers.19\n",
      "replacing layer model.layers.20\n",
      "replacing layer model.layers.21\n",
      "replacing layer model.layers.22\n",
      "replacing layer model.layers.23\n",
      "replacing layer model.layers.24\n",
      "replacing layer model.layers.25\n",
      "replacing layer model.layers.26\n",
      "replacing layer model.layers.27\n",
      "address this hack!\n",
      "trainable params: 2,179,072 || all params: 1,779,310,108 || trainable%: 0.1225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DynamicallyTypedModelWithReadout(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 1536)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x DynamicallyTypedLayerWithExit(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (early_exiter): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (early_exiter): Linear(in_features=1536, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (early_exiter): Linear(in_features=8, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (early_exiter): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (early_exiter): Linear(in_features=1536, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (early_exiter): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (early_exiter): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (early_exiter): Linear(in_features=1536, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (early_exiter): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (early_exiter): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (early_exiter): Linear(in_features=1536, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (early_exiter): Linear(in_features=8, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "              (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "            (early_exit_decision_weights): Linear(in_features=1536, out_features=1, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = replace_attention_layers(model, config['lora'], device)\n",
    "model.eval() #not training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a41f58a8-68f9-42a3-8dab-c4182be27255",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_teacher_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f287e822-0f98-4ee6-b939-ec20d4ab3718",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_chunk_data = []\n",
    "chunk_idx = 0\n",
    "total_samples_processed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6325042e-624d-484b-9d90-167c8724bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'model_name': model_name,\n",
    "    'dataset_path': dataset_path,\n",
    "    'prompt_config_path': prompt_config_path,\n",
    "    'config': config,\n",
    "    'chunk_size': chunk_size,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'system_prompt': dataset.system_prompt,\n",
    "    'prefiller': dataset.prefiller\n",
    "}\n",
    "metadata_path = os.path.join(output_dir, \"metadata.pkl.gz\")\n",
    "with gzip.open(metadata_path, 'wb', compresslevel=9) as f:\n",
    "    pickle.dump(metadata, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c0abd5-65c2-4e4a-bd9a-26d047eddef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunk(chunk_data, chunk_index, output_directory):\n",
    "    \"\"\"Save a chunk of data to disk with gzip compression\"\"\"\n",
    "    chunk_filename = os.path.join(output_directory, f\"chunk_{chunk_index:04d}.pkl.gz\")\n",
    "\n",
    "    #convert all float32 tensors to float16 before saving for size\n",
    "    compressed_data = []\n",
    "    for sample in chunk_data:\n",
    "        compressed_sample = {}\n",
    "        for key, value in sample.items():\n",
    "            if isinstance(value, torch.Tensor) and value.dtype == torch.float32:\n",
    "                if key == 'sft_teacher_final_layer_logprobs':\n",
    "                    compressed_sample[key] = (value * (value > -14.0)).to_sparse().half() #switched to a sparse tensor and converts to float16\n",
    "                else:\n",
    "                    compressed_sample[key] = value.half() #converts to float16\n",
    "            else:\n",
    "                compressed_sample[key] = value\n",
    "        compressed_data.append(compressed_sample)\n",
    "    \n",
    "    with gzip.open(chunk_filename, 'wb', compresslevel=9) as f: #play with compresslevel, 6 is moderate\n",
    "        pickle.dump({\n",
    "            'chunk_idx': chunk_index,\n",
    "            'data': compressed_data,\n",
    "            'num_samples': len(chunk_data)\n",
    "        }, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    file_size_mb = os.path.getsize(chunk_filename) / (1024 * 1024)\n",
    "    print(f\"Saved chunk {chunk_index} with {len(chunk_data)} samples\")\n",
    "    print(f\"  File: {chunk_filename}\")\n",
    "    print(f\"  Size: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0efa77b4-1f18-493a-b955-945881f565fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/619 (Total samples: 0)\n",
      "Processing batch 51/619 (Total samples: 50)\n",
      "Saved chunk 0 with 100 samples\n",
      "  File: /workspace/data/teacher_generated_data_gzip/chunk_0000.pkl.gz\n",
      "  Size: 83.52 MB\n",
      "Processing batch 101/619 (Total samples: 100)\n",
      "Processing batch 151/619 (Total samples: 150)\n",
      "Saved chunk 1 with 100 samples\n",
      "  File: /workspace/data/teacher_generated_data_gzip/chunk_0001.pkl.gz\n",
      "  Size: 76.17 MB\n",
      "Processing batch 201/619 (Total samples: 200)\n",
      "Processing batch 251/619 (Total samples: 250)\n",
      "Saved chunk 2 with 100 samples\n",
      "  File: /workspace/data/teacher_generated_data_gzip/chunk_0002.pkl.gz\n",
      "  Size: 79.44 MB\n",
      "Processing batch 301/619 (Total samples: 300)\n",
      "Processing batch 351/619 (Total samples: 350)\n",
      "Saved chunk 3 with 100 samples\n",
      "  File: /workspace/data/teacher_generated_data_gzip/chunk_0003.pkl.gz\n",
      "  Size: 83.44 MB\n",
      "Processing batch 401/619 (Total samples: 400)\n",
      "Processing batch 451/619 (Total samples: 450)\n",
      "Saved chunk 4 with 100 samples\n",
      "  File: /workspace/data/teacher_generated_data_gzip/chunk_0004.pkl.gz\n",
      "  Size: 71.81 MB\n",
      "Processing batch 501/619 (Total samples: 500)\n",
      "Processing batch 551/619 (Total samples: 550)\n",
      "Saved chunk 5 with 100 samples\n",
      "  File: /workspace/data/teacher_generated_data_gzip/chunk_0005.pkl.gz\n",
      "  Size: 66.19 MB\n",
      "Processing batch 601/619 (Total samples: 600)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, prompt_batch in enumerate(dataloader):\n",
    "    # Remove the testing limit if you want to process all data\n",
    "    #if total_samples_processed >= 30:\n",
    "    #    break\n",
    "    \n",
    "    if total_samples_processed % 50 == 0:\n",
    "        print(f\"Processing batch {total_samples_processed + 1}/{len(dataloader)} (Total samples: {total_samples_processed})\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate SFT targets\n",
    "        set_transformer_early_exit_mode(model, 'sft_teacher')\n",
    "        sft_teacher_response, (sft_teacher_generated_tokens, sft_teacher_final_layer_logprobs, gathered_early_exit_hidden_states) = \\\n",
    "            generate_text(\n",
    "                model=model, \n",
    "                prompt=prompt_batch.full_user_prompt, \n",
    "                system_prompt=dataset.system_prompt, \n",
    "                prefiller=dataset.prefiller, \n",
    "                tokenizer=tokenizer, \n",
    "                generation_config=config['generation'], \n",
    "                device=device\n",
    "            )\n",
    "        \n",
    "        # Compute early exit probabilities\n",
    "        early_output_log_probs = model.early_exit_hidden_state_readout(gathered_early_exit_hidden_states)\n",
    "        \n",
    "        # KL divergence calculations\n",
    "        teacher_expanded = sft_teacher_final_layer_logprobs.unsqueeze(1).exp()\n",
    "        early_output_probs = early_output_log_probs.exp()\n",
    "        eps = 1e-16\n",
    "        kl_div1 = - (teacher_expanded * (early_output_probs + eps).log()).sum(-1)\n",
    "        kl_div2 = (teacher_expanded * ((teacher_expanded + eps) / (early_output_probs + eps)).log()).sum(-1)\n",
    "        \n",
    "        # Store data for this batch\n",
    "        batch_data = {\n",
    "            'batch_idx': batch_idx, #in case shuffled, but don't think matters really\n",
    "            'prompt_idx': prompt_batch.idx[0] if hasattr(prompt_batch, 'idx') else batch_idx,\n",
    "            'full_user_prompt': prompt_batch.full_user_prompt,\n",
    "            'sft_teacher_response': sft_teacher_response, #full generated response text\n",
    "            'sft_teacher_generated_tokens': sft_teacher_generated_tokens.cpu(), #token IDs [batch, full_length]\n",
    "            'sft_teacher_final_layer_logprobs': sft_teacher_final_layer_logprobs.cpu(), #final layer logprobs [batch, gen_len, vocab]\n",
    "            'kl_div1_per_layer': kl_div1.cpu(), #cross-entropy KL divergence per layer to final [batch, num_layers, gen_len]\n",
    "            'kl_div2_per_layer': kl_div2.cpu(), #standard KL divergence per layer to final [batch, num_layers, gen_len]\n",
    "            'exitable_layer_idxs': model.exitable_layer_idxs.cpu(),\n",
    "        }\n",
    "        \n",
    "        current_chunk_data.append(batch_data)\n",
    "        total_samples_processed += 1\n",
    "        \n",
    "        # Save chunk if we've reached chunk_size\n",
    "        if len(current_chunk_data) >= chunk_size:\n",
    "            save_chunk(current_chunk_data, chunk_idx, output_dir)\n",
    "            current_chunk_data = []\n",
    "            chunk_idx += 1\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "024f4f59-489e-4ee8-8129-f7fde70e9145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving final chunk with 19 samples...\n",
      "Saved chunk 6 with 19 samples\n",
      "  File: /workspace/data/teacher_generated_data_gzip/chunk_0006.pkl.gz\n",
      "  Size: 11.80 MB\n"
     ]
    }
   ],
   "source": [
    "if current_chunk_data:\n",
    "    print(f\"\\nSaving final chunk with {len(current_chunk_data)} samples...\")\n",
    "    save_chunk(current_chunk_data, chunk_idx, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2804eb5e-1c56-45ac-977d-7993a0e887cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_teacher_data_chunks(\n",
    "    output_dir: str,\n",
    "    merged_filename: str = \"merged_teacher_data_sparse.pkl.gz\",\n",
    "    delete_chunks: bool = True,\n",
    "    compresslevel: int = 9\n",
    "):\n",
    "    print(f\"Merging chunks from {output_dir}...\")\n",
    "\n",
    "    #load metadata - now gzipped\n",
    "    meta_path = os.path.join(output_dir, \"metadata.pkl.gz\")\n",
    "    with gzip.open(meta_path, \"rb\") as f:\n",
    "        metadata = pickle.load(f)\n",
    "\n",
    "    chunk_files = sorted(\n",
    "        f for f in os.listdir(output_dir)\n",
    "        if f.startswith(\"chunk_\") and f.endswith(\".pkl.gz\")\n",
    "    )\n",
    "    print(f\"Found {len(chunk_files)} chunk files\")\n",
    "\n",
    "    merged_path = os.path.join(output_dir, merged_filename)\n",
    "    total = 0\n",
    "\n",
    "    with gzip.open(merged_path, \"wb\", compresslevel=compresslevel) as fout:\n",
    "        # write header once\n",
    "        pickle.dump({'metadata': metadata}, fout, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        for i, cf in enumerate(chunk_files, 1):\n",
    "            cpath = os.path.join(output_dir, cf)\n",
    "            print(f\"  [{i}/{len(chunk_files)}] {cf}\")\n",
    "            with gzip.open(cpath, \"rb\") as fin:\n",
    "                chunk = pickle.load(fin)          # {'chunk_idx','data',...}\n",
    "                for sample in chunk['data']:\n",
    "                    pickle.dump(sample, fout, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                total += len(chunk['data'])\n",
    "\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "\n",
    "            if delete_chunks:\n",
    "                os.remove(cpath)\n",
    "                print(f\"    deleted {cf}\")\n",
    "\n",
    "        pickle.dump({'_end': True, 'num_samples': total}, fout, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    size_mb = os.path.getsize(merged_path) / (1024 * 1024)\n",
    "    print(f\"\\nTotal samples: {total}\")\n",
    "    print(f\"Merged file size: {size_mb:.2f} MB\")\n",
    "    print(f\"Saved to: {merged_path}\")\n",
    "\n",
    "\n",
    "def iter_merged_teacher_data(merged_path: str) -> Iterator[dict]:\n",
    "    \"\"\"\n",
    "    Lazily iterate samples from the merged stream.\n",
    "    Skips header and footer.\n",
    "    \"\"\"\n",
    "    with gzip.open(merged_path, \"rb\") as f:\n",
    "        header = pickle.load(f)  # {'metadata': ...}\n",
    "        while True:\n",
    "            try:\n",
    "                obj = pickle.load(f)\n",
    "            except EOFError:\n",
    "                break\n",
    "            if isinstance(obj, dict) and obj.get('_end'):\n",
    "                break\n",
    "            yield obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64cd5bff-5da3-417b-8bd4-5be71f364860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_teacher_data_chunks(output_dir, merged_filename=\"merged_teacher_data.pkl.gz\"):\n",
    "    \"\"\"\n",
    "    Merge all chunk files into a single file\n",
    "    \"\"\"\n",
    "    print(f\"Merging chunks from {output_dir}...\")\n",
    "    \n",
    "    #load metadata - now gzipped\n",
    "    metadata_path = os.path.join(output_dir, \"metadata.pkl.gz\")\n",
    "    with gzip.open(metadata_path, 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    #find all chunk files - now with .gz extension\n",
    "    chunk_files = sorted([f for f in os.listdir(output_dir) if f.startswith(\"chunk_\") and f.endswith(\".pkl.gz\")])\n",
    "    print(f\"Found {len(chunk_files)} chunk files to merge\")\n",
    "    \n",
    "    all_data = []\n",
    "    for chunk_file in chunk_files:\n",
    "        chunk_path = os.path.join(output_dir, chunk_file)\n",
    "        print(f\"  Loading {chunk_file}...\")\n",
    "        with gzip.open(chunk_path, 'rb') as f:\n",
    "            chunk_data = pickle.load(f)\n",
    "            all_data.extend(chunk_data['data'])\n",
    "            print(f\"    Loaded {len(chunk_data['data'])} samples\")\n",
    "    \n",
    "    #save merged data with gzip\n",
    "    merged_path = os.path.join(output_dir, merged_filename)\n",
    "    print(f\"\\nSaving merged data to {merged_path}...\")\n",
    "    with gzip.open(merged_path, 'wb', compresslevel=9) as f:\n",
    "        pickle.dump({\n",
    "            'teacher_data': all_data,\n",
    "            'metadata': {\n",
    "                **metadata,\n",
    "                'num_samples': len(all_data),\n",
    "                'num_chunks_merged': len(chunk_files)\n",
    "            }\n",
    "        }, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    file_size_mb = os.path.getsize(merged_path) / (1024 * 1024)\n",
    "    print(f\"Total samples: {len(all_data)}\")\n",
    "    print(f\"Merged file size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"Saved to: {merged_path}\")\n",
    "    \n",
    "    #return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59eeb6b5-a32e-44c5-9a15-ea0633357bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_teacher_data_chunks(\n",
    "    output_dir: str,\n",
    "    merged_filename: str = \"merged_teacher_data_sparse_combtensor.pkl.gz\",\n",
    "    delete_chunks: bool = True,\n",
    "    compresslevel: int = 9\n",
    "):\n",
    "    print(f\"Merging chunks from {output_dir}...\")\n",
    "    \n",
    "    # Load metadata\n",
    "    meta_path = os.path.join(output_dir, \"metadata.pkl.gz\")\n",
    "    with gzip.open(meta_path, \"rb\") as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    chunk_files = sorted(\n",
    "        f for f in os.listdir(output_dir)\n",
    "        if f.startswith(\"chunk_\") and f.endswith(\".pkl.gz\")\n",
    "    )\n",
    "    print(f\"Found {len(chunk_files)} chunk files\")\n",
    "    \n",
    "    # First pass: collect all data and find max sequence length\n",
    "    all_samples = []\n",
    "    max_seq_len = 0\n",
    "    \n",
    "    print(\"First pass: collecting samples and finding max sequence length...\")\n",
    "    for i, cf in enumerate(chunk_files, 1):\n",
    "        cpath = os.path.join(output_dir, cf)\n",
    "        print(f\"  Reading [{i}/{len(chunk_files)}] {cf}\")\n",
    "        with gzip.open(cpath, \"rb\") as fin:\n",
    "            chunk = pickle.load(fin)\n",
    "            for sample in chunk['data']:\n",
    "                all_samples.append(sample)\n",
    "                # Check sequence length\n",
    "                if 'sft_teacher_final_layer_logprobs' in sample:\n",
    "                    seq_len = sample['sft_teacher_final_layer_logprobs'].shape[1]\n",
    "                    max_seq_len = max(max_seq_len, seq_len)\n",
    "    \n",
    "    total_samples = len(all_samples)\n",
    "    vocab_size = 151936  # From your data\n",
    "    print(f\"\\nTotal samples: {total_samples}\")\n",
    "    print(f\"Max sequence length: {max_seq_len}\")\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Create unified data structure\n",
    "    print(\"\\nCreating unified data structure...\")\n",
    "    unified_data = {\n",
    "        'metadata': metadata,\n",
    "        'num_samples': total_samples,\n",
    "        'max_seq_len': max_seq_len,\n",
    "        'vocab_size': vocab_size,\n",
    "        # Store all logprobs as a list of sparse tensors with their metadata\n",
    "        'all_sparse_logprobs': [],\n",
    "        'logprobs_metadata': {\n",
    "            'sample_indices': [],      # Which sample each tensor belongs to\n",
    "            'sequence_lengths': [],    # Actual length of each sequence\n",
    "            'start_positions': [],     # Where each sample starts in a hypothetical stacked tensor\n",
    "        },\n",
    "        # Store other data in parallel arrays\n",
    "        'batch_indices': [],\n",
    "        'prompt_indices': [],\n",
    "        'full_user_prompts': [],\n",
    "        'sft_teacher_responses': [],\n",
    "        'sft_teacher_generated_tokens': [],\n",
    "        'kl_div1_per_layer': [],\n",
    "        'kl_div2_per_layer': [],\n",
    "        'exitable_layer_idxs': None,  # Same for all samples\n",
    "    }\n",
    "    \n",
    "    # Second pass: populate unified structure\n",
    "    print(\"Second pass: creating unified structure...\")\n",
    "    current_position = 0\n",
    "    \n",
    "    for idx, sample in enumerate(all_samples):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"  Processing sample {idx}/{total_samples}\")\n",
    "        \n",
    "        # Store logprobs with metadata\n",
    "        if 'sft_teacher_final_layer_logprobs' in sample:\n",
    "            sparse_logprobs = sample['sft_teacher_final_layer_logprobs']\n",
    "            seq_len = sparse_logprobs.shape[1]\n",
    "            \n",
    "            unified_data['all_sparse_logprobs'].append(sparse_logprobs)\n",
    "            unified_data['logprobs_metadata']['sample_indices'].append(idx)\n",
    "            unified_data['logprobs_metadata']['sequence_lengths'].append(seq_len)\n",
    "            unified_data['logprobs_metadata']['start_positions'].append(current_position)\n",
    "            current_position += seq_len\n",
    "        \n",
    "        # Store other data\n",
    "        unified_data['batch_indices'].append(sample.get('batch_idx'))\n",
    "        unified_data['prompt_indices'].append(sample.get('prompt_idx'))\n",
    "        unified_data['full_user_prompts'].append(sample.get('full_user_prompt'))\n",
    "        unified_data['sft_teacher_responses'].append(sample.get('sft_teacher_response'))\n",
    "        unified_data['sft_teacher_generated_tokens'].append(sample.get('sft_teacher_generated_tokens'))\n",
    "        unified_data['kl_div1_per_layer'].append(sample.get('kl_div1_per_layer'))\n",
    "        unified_data['kl_div2_per_layer'].append(sample.get('kl_div2_per_layer'))\n",
    "        \n",
    "        # Set exitable_layer_idxs (same for all samples)\n",
    "        if unified_data['exitable_layer_idxs'] is None and 'exitable_layer_idxs' in sample:\n",
    "            unified_data['exitable_layer_idxs'] = sample['exitable_layer_idxs']\n",
    "    \n",
    "    # Convert metadata lists to tensors for efficiency\n",
    "    unified_data['logprobs_metadata']['sample_indices'] = torch.tensor(\n",
    "        unified_data['logprobs_metadata']['sample_indices'], dtype=torch.long\n",
    "    )\n",
    "    unified_data['logprobs_metadata']['sequence_lengths'] = torch.tensor(\n",
    "        unified_data['logprobs_metadata']['sequence_lengths'], dtype=torch.long\n",
    "    )\n",
    "    unified_data['logprobs_metadata']['start_positions'] = torch.tensor(\n",
    "        unified_data['logprobs_metadata']['start_positions'], dtype=torch.long\n",
    "    )\n",
    "    \n",
    "    # Save unified structure\n",
    "    merged_path = os.path.join(output_dir, merged_filename)\n",
    "    print(f\"\\nSaving unified structure to {merged_path}...\")\n",
    "    with gzip.open(merged_path, \"wb\", compresslevel=compresslevel) as fout:\n",
    "        pickle.dump(unified_data, fout, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    # Clean up\n",
    "    del all_samples\n",
    "    gc.collect()\n",
    "    \n",
    "    # Delete chunks if requested\n",
    "    if delete_chunks:\n",
    "        for cf in chunk_files:\n",
    "            cpath = os.path.join(output_dir, cf)\n",
    "            os.remove(cpath)\n",
    "            print(f\"  Deleted {cf}\")\n",
    "    \n",
    "    size_mb = os.path.getsize(merged_path) / (1024 * 1024)\n",
    "    print(f\"\\nTotal samples: {total_samples}\")\n",
    "    print(f\"Merged file size: {size_mb:.2f} MB\")\n",
    "    print(f\"Saved to: {merged_path}\")\n",
    "\n",
    "def load_merged_teacher_data(merged_path: str):\n",
    "    \"\"\"Load the entire unified dataset\"\"\"\n",
    "    with gzip.open(merged_path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_sample_from_unified(unified_data: dict, sample_idx: int) -> dict:\n",
    "    \"\"\"\n",
    "    Reconstruct a single sample from the unified structure.\n",
    "    This shows how to access data during training.\n",
    "    \"\"\"\n",
    "    # Find the logprobs for this sample\n",
    "    logprobs_idx = (unified_data['logprobs_metadata']['sample_indices'] == sample_idx).nonzero()[0].item()\n",
    "    sparse_logprobs = unified_data['all_sparse_logprobs'][logprobs_idx]\n",
    "    \n",
    "    # Reconstruct the sample\n",
    "    sample = {\n",
    "        'batch_idx': unified_data['batch_indices'][sample_idx],\n",
    "        'prompt_idx': unified_data['prompt_indices'][sample_idx],\n",
    "        'full_user_prompt': unified_data['full_user_prompts'][sample_idx],\n",
    "        'sft_teacher_response': unified_data['sft_teacher_responses'][sample_idx],\n",
    "        'sft_teacher_generated_tokens': unified_data['sft_teacher_generated_tokens'][sample_idx],\n",
    "        'sft_teacher_final_layer_logprobs': sparse_logprobs,\n",
    "        'kl_div1_per_layer': unified_data['kl_div1_per_layer'][sample_idx],\n",
    "        'kl_div2_per_layer': unified_data['kl_div2_per_layer'][sample_idx],\n",
    "        'exitable_layer_idxs': unified_data['exitable_layer_idxs'],\n",
    "    }\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92698a3c-4a02-4398-be52-075d77cac62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging chunks from /workspace/data/teacher_generated_data_gzip...\n",
      "Found 7 chunk files\n",
      "  [1/7] chunk_0000.pkl.gz\n",
      "    deleted chunk_0000.pkl.gz\n",
      "  [2/7] chunk_0001.pkl.gz\n",
      "    deleted chunk_0001.pkl.gz\n",
      "  [3/7] chunk_0002.pkl.gz\n",
      "    deleted chunk_0002.pkl.gz\n",
      "  [4/7] chunk_0003.pkl.gz\n",
      "    deleted chunk_0003.pkl.gz\n",
      "  [5/7] chunk_0004.pkl.gz\n",
      "    deleted chunk_0004.pkl.gz\n",
      "  [6/7] chunk_0005.pkl.gz\n",
      "    deleted chunk_0005.pkl.gz\n",
      "  [7/7] chunk_0006.pkl.gz\n",
      "    deleted chunk_0006.pkl.gz\n",
      "\n",
      "Total samples: 619\n",
      "Merged file size: 472.56 MB\n",
      "Saved to: /workspace/data/teacher_generated_data_gzip/merged_teacher_data_sparse.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "merge_teacher_data_chunks(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51e1e274-010b-4fa4-9b4b-3a5f1f283c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.803590774536133"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.getsize(os.path.join(output_dir, \"merged_teacher_data_sparse_combtensor.pkl.gz\")) / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "723a1cb3-a2f0-4a3a-9b46-7ea8b87cac5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.832829475402832"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.getsize(os.path.join(output_dir, \"merged_teacher_data_sparse.pkl.gz\")) / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f82e0240-7539-4ec8-bf7f-8fee237c08fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2120.7067728042603"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.getsize(os.path.join(output_dir, \"merged_teacher_data_original.pkl.gz\")) / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d21392-f75a-421d-8ed5-fcc11b00a383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
