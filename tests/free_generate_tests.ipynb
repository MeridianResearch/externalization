{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8fce9d6-3148-42d3-a218-9faedeb116a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from shared_utils.load import get_tokenizer, configs_from_yaml\n",
    "from shared_utils.generate import generate_text\n",
    "from early_exit.util import get_model, load_model\n",
    "from early_exit.patching import replace_attention_layers, set_transformer_early_exit_mode\n",
    "\n",
    "model_path = \"../models/early_exit_20250811_layers_5_big/step_2000\"\n",
    "base_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "config = \"../config_deepseek.yaml\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ba1869-c14a-4ea5-8d4e-d4478c0fb164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "address this hack!\n",
      "trainable params: 2,179,072 || all params: 1,802,890,757 || trainable%: 0.1209\n",
      "Model loaded w exitable layers: tensor([ 5., 10., 15., 20., 25., inf])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(base_model)\n",
    "config = configs_from_yaml(config, tokenizer.eos_token_id)\n",
    "\n",
    "base_model = get_model(base_model, config['model'], device)\n",
    "model = replace_attention_layers(base_model, config['lora'], device)\n",
    "model = load_model(model, model_path)\n",
    "\n",
    "print(f\"Model loaded w exitable layers: {model.exitable_layer_idxs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c549b0c-f527-42ff-a5d6-9a20eb1a6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_transformer_early_exit_mode(model, 'free_generate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fcda0c4-a8bf-41ad-8aa7-458d7b53d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain the concept of recursion in programming.\"\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b09af26e-02fa-4718-b581-7e570466a168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 20])\n",
      "Free Generate Response: <｜begin▁of▁sentence｜><｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> Explain the concept of recursion in programming.\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to explain how recursion works in programming. Hmm, I remember hearing about recursion before, but I'm not entirely understanding how it's different from other programming concepts. Let me try to break it down.\n",
      "\n",
      "First, recursion is a method where a function calls itself. That means the function has a base case and/or recursive solution involving breaking into pieces. I think the base case is when you stop because you can't do any more work. But wait, how does that work in practice? Let's take example code.\n",
      "\n",
      "I remember seeing something like this:\n",
      "\n",
      "def factorial(n):\n",
      "    if ...)\n",
      "    return ...\n",
      "\n",
      "But I'm unclear on the base case. Maybe it's when n is 0 or 1? Because  factorial(0) is trivially equal� 1, and factorial itself itself is 1. So, the base case is when n is 0 or 1, and then it returns 1. Otherwise, it calls itself with n-1.\n",
      "\n",
      "Wait, but how does that work for larger numbers? Like factorial(5 would be 1 multiplied/divided/divided by 5, 4 etc. So, each step reduces/removes one from n until it reaches the base case.\n",
      "\n",
      "I'm a bit confused about the parameters. In the function definition, the parameter is n, but in the recursive calls, does it also pass n-1? Or is it a different variable? Maybe it's the same variable, but it's just being passed along as it's called.\n",
      "\n",
      "Also, I'm not understanding how the function calls are structured. Like, how does the function know when to stop and return the value? It must be that when it reaches the desired base case, it returns 1 or something, and then each previous call can build upon that, multiplying or adding as needed.\n",
      "\n",
      "Let me think of a simple example. Suppose I want to compute the factorial of 3. So, factorial(3) would call itself with\n",
      "Exit info: (tensor([[151646, 151645,   1446,    525,    264,  10950,  15473,  25302,    624,\n",
      "         151644,  81917,    279,   7286,    315,  50262,    304,  15473,    624,\n",
      "         151645,    715,  32313,     11,    773,    358,   1184,    311,  10339,\n",
      "           1246,  50262,   4278,    304,  15473,     13,  88190,     11,    358,\n",
      "           6099,  10778,    911,  50262,   1573,     11,    714,    358,   2776,\n",
      "            537,  11368,   8660,   1246,    432,    594,   2155,    504,   1008,\n",
      "          15473,  18940,     13,   6771,    752,   1430,    311,   1438,    432,\n",
      "           1495,    382,   5338,     11,  50262,    374,    264,   1714,   1380,\n",
      "            264,    729,   6738,   5086,     13,   2938,   3363,    279,    729,\n",
      "            702,    264,   2331,   1142,    323,   5144,  30819,   6291,  15860,\n",
      "          14719,   1119,   9666,     13,    358,   1744,    279,   2331,   1142,\n",
      "            374,    979,    498,   2936,   1576,    498,    646,    944,    653,\n",
      "            894,    803,    975,     13,   1988,   3783,     11,   1246,   1558,\n",
      "            429,    975,    304,   6588,     30,   6771,    594,   1896,   3110,\n",
      "           2038,    382,     40,   6099,   9120,   2494,   1075,    419,   1447,\n",
      "            750,  52962,   1445,    982,    262,    421,  52268,    262,    470,\n",
      "           5468,   3983,    358,   2776,  24416,    389,    279,   2331,   1142,\n",
      "             13,  10696,    432,    594,    979,    308,    374,    220,     15,\n",
      "            476,    220,     16,     30,   9211,    220,  52962,      7,     15,\n",
      "              8,    374,  35647,    398,   6144,    242,    220,     16,     11,\n",
      "            323,  52962,   5086,   5086,    374,    220,     16,     13,   2055,\n",
      "             11,    279,   2331,   1142,    374,    979,    308,    374,    220,\n",
      "             15,    476,    220,     16,     11,    323,   1221,    432,   4675,\n",
      "            220,     16,     13,  18214,     11,    432,   6738,   5086,    448,\n",
      "            308,     12,     16,    382,  14190,     11,    714,   1246,   1558,\n",
      "            429,    975,    369,   8131,   5109,     30,   8909,  52962,      7,\n",
      "             20,   1035,    387,    220,     16,  54916,  22013,   4490,  22013,\n",
      "           4490,    553,    220,     20,     11,    220,     19,   4992,     13,\n",
      "           2055,     11,   1817,   3019,  25271,  81771,  10088,    825,    504,\n",
      "            308,   3080,    432,  24491,    279,   2331,   1142,    382,     40,\n",
      "           2776,    264,   2699,  21815,    911,    279,   5029,     13,    758,\n",
      "            279,    729,   7271,     11,    279,   5733,    374,    308,     11,\n",
      "            714,    304,    279,  30819,   6738,     11,   1558,    432,   1083,\n",
      "           1494,    308,     12,     16,     30,   2521,    374,    432,    264,\n",
      "           2155,   3890,     30,  10696,    432,    594,    279,   1852,   3890,\n",
      "             11,    714,    432,    594,   1101,   1660,   5823,   3156,    438,\n",
      "            432,    594,   2598,    382,  13394,     11,    358,   2776,    537,\n",
      "           8660,   1246,    279,    729,   6738,    525,  32930,     13,   8909,\n",
      "             11,   1246,   1558,    279,    729,   1414,    979,    311,   2936,\n",
      "            323,    470,    279,    897,     30,   1084,   1969,    387,    429,\n",
      "            979,    432,  24491,    279,  12685,   2331,   1142,     11,    432,\n",
      "           4675,    220,     16,    476,   2494,     11,    323,   1221,   1817,\n",
      "           3681,   1618,    646,   1936,   5193,    429,     11,  84192,    476,\n",
      "           7842,    438,   4362,    382,  10061,    752,   1744,    315,    264,\n",
      "           4285,   3110,     13,  82610,    358,   1366,    311,  12564,    279,\n",
      "          52962,    315,    220,     18,     13,   2055,     11,  52962,      7,\n",
      "             18,      8,   1035,   1618,   5086,    448]], device='cuda:0'), tensor([[inf, inf, inf, inf, inf, 25., inf, 20., 25., inf, 25., inf, inf, inf,\n",
      "         25., inf, inf, inf, inf, inf, inf, inf, 25., inf, inf, inf, inf, 20.,\n",
      "         25., 25., inf, inf, 25., inf, inf, 25., inf, inf, inf, inf, inf, inf,\n",
      "         inf, 25., inf, inf, inf, inf, inf, inf, inf, 25., 25., inf, inf, 25.,\n",
      "         20., inf, inf, inf, inf, inf, 20., inf, inf, inf, 20., 25., 10., 20.,\n",
      "         inf, 10., 15., inf, inf, inf, inf, inf, inf, inf, inf, 20., inf, 20.,\n",
      "         25., inf, inf, inf, 15., inf, inf, inf, inf, inf, 25., inf, inf, inf,\n",
      "         inf, inf, inf, 20., inf, 25., inf, 25., inf, inf, inf, inf, inf, inf,\n",
      "         inf, 25., 25., inf, inf, inf, inf, inf, inf, 20., inf, inf, inf, inf,\n",
      "         inf, inf, 20., inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, inf, inf, inf, inf, inf, inf, 25., inf, inf, inf, inf, 20.,\n",
      "         20., 20., 20., inf, inf, inf, inf, inf, 20., 10., inf, inf, inf, inf,\n",
      "         inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 25.,\n",
      "         20., inf, inf, inf, inf, inf, inf, inf, inf, 20., inf, inf, 15., 20.,\n",
      "         inf, inf, inf, 20., inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, inf, inf, inf, 20., 25., inf, 20., 25., 15., inf, 10., inf,\n",
      "         inf, inf, inf, inf, inf, inf, 20., inf, inf, inf, inf, 25., inf, 15.,\n",
      "         inf, 20., inf, inf, inf, 25., inf, inf, 25., 25., inf, inf, inf, inf,\n",
      "         inf, 20., 20., inf, inf, inf, inf, inf, inf, inf, 25., inf, inf, inf,\n",
      "         inf, inf, inf, inf, inf, inf, 25., 20., 25., inf, inf, inf, inf, inf,\n",
      "         25., inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, inf, inf, inf, inf, inf, inf, inf, 20., inf, inf, inf, inf,\n",
      "         inf, inf, inf, inf, inf, 20., inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         25., inf, inf, inf, inf, inf, 25., inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, inf, inf, 25., inf, inf, 20., 20., inf, inf, 20., inf, inf,\n",
      "         inf, inf, 20., 25., inf, 25., 25., inf, inf, 25., inf, inf, 25., inf,\n",
      "         10., inf, 15., inf, inf, 25., 20., inf, inf, inf, inf, inf, inf, inf,\n",
      "         25., inf, inf, inf, 20., 25., inf, inf, 25., inf, inf, inf, inf, inf,\n",
      "         25., 25., 25., inf, 20., inf, 25., 20.]]))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    try:\n",
    "        free_generate_response, exit_info = generate_text(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            prefiller=prefiller,\n",
    "            tokenizer=tokenizer,\n",
    "            generation_config=config['generation'],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(f\"Free Generate Response: {free_generate_response}\")\n",
    "        print(f\"Exit info: {exit_info}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Free generate mode failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
