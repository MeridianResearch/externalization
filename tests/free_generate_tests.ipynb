{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8fce9d6-3148-42d3-a218-9faedeb116a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from shared_utils.load import get_tokenizer, configs_from_yaml\n",
    "from shared_utils.generate import generate_text\n",
    "from early_exit.util import get_model, load_model\n",
    "from early_exit.patching import replace_attention_layers, set_transformer_early_exit_mode\n",
    "\n",
    "model_path = \"../models/early_exit_20250811_layers_5_lora32/step_2000\"\n",
    "base_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "base_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "config_path = \"../config_deepseek.yaml\"\n",
    "config = \"../config_deepseek.yaml\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ba1869-c14a-4ea5-8d4e-d4478c0fb164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.0\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.1\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.2\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.3\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.4\n",
      "replacing layer model.layers.5\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.6\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.7\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.8\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.9\n",
      "replacing layer model.layers.10\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.11\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.12\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.13\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.14\n",
      "replacing layer model.layers.15\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.16\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.17\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.18\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.19\n",
      "replacing layer model.layers.20\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.21\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.22\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.23\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.24\n",
      "replacing layer model.layers.25\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.26\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.27\n",
      "address this hack!\n",
      "trainable params: 8,716,288 || all params: 1,785,811,973 || trainable%: 0.4881\n",
      "Model loaded w exitable layers: tensor([ 5., 10., 15., 20., 25., inf])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(base_model)\n",
    "config = configs_from_yaml(config_path, tokenizer.eos_token_id)\n",
    "\n",
    "base_model = get_model(base_model_name, config['model'], device)\n",
    "model = replace_attention_layers(base_model, config['lora'], device)\n",
    "model = load_model(model, model_path)\n",
    "\n",
    "print(f\"Model loaded w exitable layers: {model.exitable_layer_idxs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c549b0c-f527-42ff-a5d6-9a20eb1a6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_transformer_early_exit_mode(model, 'free_generate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fcda0c4-a8bf-41ad-8aa7-458d7b53d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain the concept of recursion in programming.\"\n",
    "\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b09af26e-02fa-4718-b581-7e570466a168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 20])\n",
      "Free Generate Response: <｜begin▁of▁sentence｜><｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> Explain the concept of recursion in programming.\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to explain the conceptually meaning of recursion in programming. Hmm okay, let's start from scratch because I want to understand how it works conceptually.\n",
      "\n",
      "First, I remember that recursion is often used in algorithms and algorithms often naturally extensions of other problems. Like when you have a recursive function/functionality/methodicality often ens).* \n",
      "\n",
      "So, recursion is when aquired recursively,那就是 meaning it calls itself recursively (same function).*)*所以在每 subsequent step of our function, tasks are performed-on/off/offly the same as othersiable othersiables/operators operations operations.`)*)*)*所以在 essenceCODing好的)*)*)*每次函数的一部分 ones step ️ stepstogether with itself.\n",
      "\n",
      "So, imagine a recursive function like factorial function. factorial(norecal factorial(noreight factorial(noreightway to/from/to/from/to/from/to/from/toing factorial(noreight factorial(noreight factorial backed by virtue of recursive callssthe base case and/or/remotelyep(AF(n) equals to n multiplied by factorial(n- soundingnessness/rem/rem/rem/rem subjacent to/from乎 to/from/to/from/to/from/toing factorial backed by recursive calls).\n",
      "\n",
      "So, factorial(n) equals to clipboard clipboard clipboard clipboard wagons tiwewipewankle Gates- wait wait, wait waitnecessary here. Let's see: factorial(n-day factorial backed with recursive callsn) equals to n itself multiplied.那就是 say goodbye/going ahead ahead Padano/bar KiAbsent rhymeously 1st iteration: factorial(5) equals什么呢? It certainly fate to/from/to/from/to/from/to/from/to/from/to/fromothers callsnorealistically-minded others船只en steps-piecewise St serviced steps twenties stepwise/array/block etcetero/countingeringSterling ldquo factorial(n) equals identity function multiplied by factorial(n-1 recursively.)*️**\n",
      "\n",
      "So, recursively, each successive iteration of the function performs a similar task but not necessarily on the same\n",
      "Exit info: (tensor([[151646, 151645,   1446,    525,    264,  10950,  15473,  25302,    624,\n",
      "         151644,  81917,    279,   7286,    315,  50262,    304,  15473,    624,\n",
      "         151645,    715,  32313,     11,    773,    358,   1184,    311,  10339,\n",
      "            279,   7286,   1832,   7290,    315,  50262,    304,  15473,     13,\n",
      "          88190,  16910,     11,   1077,    594,   1191,    504,  18778,   1576,\n",
      "            358,   1366,    311,   3535,   1246,    432,   4278,   7286,   1832,\n",
      "            382,   5338,     11,    358,   6099,    429,  50262,    374,   3545,\n",
      "           1483,    304,  25185,    323,  25185,   3545,  17712,  19721,    315,\n",
      "           1008,   5322,     13,   8909,    979,    498,    614,    264,  30819,\n",
      "            729,  70643,   2719,  91837,    938,    487,   3545,   4572,  45076,\n",
      "           4710,   4416,     11,  50262,    374,    979,    264,   2931,  52847,\n",
      "             11, 105139,   7290,    432,   6738,   5086,  52847,    320,  24063,\n",
      "            729,  45076,   4806, 106325,  73157,  17420,   3019,    315,   1039,\n",
      "            729,     11,   9079,    525,  10660,  10326,  50806,  50806,    398,\n",
      "            279,   1852,    438,   3800,   2156,   3800,   2156,     82,  46481,\n",
      "           7525,   7525,  14908,   4806,   4806,   4806, 106325,  27491,  33791,\n",
      "            287,  99692,   4806,   4806,   4806, 102173,  32804, 106979,    220,\n",
      "           3154,   3019,    220,  30543,   3019,  33052,   3439,    448,   5086,\n",
      "            382,   4416,     11,  12793,    264,  30819,    729,   1075,  52962,\n",
      "            729,     13,  52962,   1445,    460,   5416,  52962,   1445,    460,\n",
      "            491,  52962,   1445,    460,    491,   3117,    311,  91106,  32429,\n",
      "          91106,  32429,  91106,  32429,  91106,  32429,    287,  52962,   1445,\n",
      "            460,    491,  52962,   1445,    460,    491,  52962,  21411,    553,\n",
      "          34360,    315,  30819,   6738,    267,    383,   2331,   1142,    323,\n",
      "           5144,  81771,    354,    974,    747,  57867,   1445,      8,  16819,\n",
      "            311,    308,  54916,    553,  52962,   1445,     12,  48127,   2090,\n",
      "           2090,  81771,  81771,  81771,  81771,  80317,  18181,    311,  91106,\n",
      "          99587,    311,  91106,  32429,  91106,  32429,  91106,  32429,    287,\n",
      "          52962,  21411,    553,  30819,   6738,   3593,   4416,     11,  52962,\n",
      "           1445,      8,  16819,    311,  46034,  46034,  46034,  46034,  64325,\n",
      "           2382,   8988,     86,    365,    573,    365,   1180,    273,  35493,\n",
      "             12,   3783,   3783,     11,   3783,   3783,  94217,   1588,     13,\n",
      "           6771,    594,   1490,     25,  52962,   1445,  11228,  52962,  21411,\n",
      "            448,  30819,   6738,     77,      8,  16819,    311,    308,   5086,\n",
      "          54916,     13, 105139,   1977,  46455,  25525,    287,   8305,   8305,\n",
      "          25299,   5652,  49513,  29458,  80251,  96934,   7017,    220,     16,\n",
      "            267,  19569,     25,  52962,      7,     20,      8,  16819, 111383,\n",
      "             30,   1084,   7838,  24382,    311,  91106,  32429,  91106,  32429,\n",
      "          91106,  32429,  91106,  32429,  91106,  32429,  91106,  58861,   6738,\n",
      "             77,    460,    278,  37110,  33323,   3800, 118515,    268,   3019,\n",
      "             82,  55864,   4482,    794,  80123,   3019,     82,  96596,   3019,\n",
      "           4482,  95838,  59981,   4992,   1404,     78,  81043,    287,   4671,\n",
      "          80350,   2718,    220,    507,  26512,  52962,   1445,      8,  16819,\n",
      "           9569,    729,  54916,    553,  52962,   1445,     12,     16,  52847,\n",
      "             13,   4806,  30543,  56177,   4416,     11,  52847,     11,   1817,\n",
      "          48924,  19569,    315,    279,    729,  26674,    264,   4428,   3383,\n",
      "            714,    537,  14312,    389,    279,   1852]], device='cuda:0'), tensor([[inf, 25., inf, 25., 25., 25., inf, 20., 25., 10., 25., 25., inf, 25.,\n",
      "         25., 20., inf, 20., inf, inf, 25., inf, inf, 25., 25., inf, 25., 25.,\n",
      "         25., 10., 25., inf, 15., 10., inf, 25., inf, inf, inf, 25., inf, inf,\n",
      "         15., inf, 25., 25., 25., 25., 25., 20., 10., 25., inf, 25., inf, 25.,\n",
      "         25., 25., inf, 25., 10., 15., 15., 25., 10., 10., 15., 25., 20., 10.,\n",
      "         20., 25., inf, inf, inf, 25., inf,  5., 25., inf, 15., 15., inf, inf,\n",
      "         15.,  5., 15., inf, inf, inf, inf, 20., inf, 20., 25., 15., 15., inf,\n",
      "         inf, 15., inf, inf, 10., 15., 10., inf, inf, 25., inf, 20., 10., inf,\n",
      "         25., 10., 10., 10., 10., 20., inf, inf, inf, 20., 20., 10., 15., 15.,\n",
      "         inf, inf, inf, 25., inf, 15.,  5., 15., 25., inf, 20., 25., 15., 15.,\n",
      "         25., 25., 25., 25., inf, 25., 25., 25., 15., 25., 25., 25., 25., 25.,\n",
      "         25., 15., inf, 25., 25., 25., 15., 25., inf, 25., 25., 15., 10., 15.,\n",
      "         25.,  5., 25., 25., 25., 25., 25., 15., 25., 25., 25., 25., 25., inf,\n",
      "         25., 25., 25.,  5., 25., 15., 10., 25., 25., 15., inf, 25., 15., inf,\n",
      "         15., 20., inf, inf, 10., 20., inf, 15., 25., inf, inf, 25., 25., 25.,\n",
      "         25., inf, 15., 10.,  5., 20., 10., 25., 25., 15., 25., inf, 25., 10.,\n",
      "         inf, 25., 10.,  5., 25., inf, 10., 20., inf,  5., 25., inf, inf, 25.,\n",
      "         25., 25., inf, 25., 25., 25., 25., 15., 25., 25., 25., 15., inf, 10.,\n",
      "         inf, inf, 25., inf, 10., inf, 10., 15., 25., 25., inf, 25., 25., 15.,\n",
      "         25., inf, 25., 25., 25., inf, 25., inf, 10., 25.,  5., 15., 25., 25.,\n",
      "         15., 25., 25., 25., 25., 15., 25., 15., 15., 25., 15., 10., 15.,  5.,\n",
      "         10., 15.,  5., 15., 10., 15., 15., 20., inf, inf, 15., 10., 20., 25.,\n",
      "         inf, inf, 25., inf, 15., 25., 25., 15., 10., 25., 15., 25., 25., 10.,\n",
      "         25., 25., 25., 25., 25., 10., 25., 10., 25., 10., 25., 25., 25.,  5.,\n",
      "         inf, 15., 15., 25., 15., 15., 25., 10., 15., 25., 15., 15., inf, 25.,\n",
      "         10., 10., 20., 20., 10., 10., inf, 15., 10., inf, inf, 15., 25., inf,\n",
      "         25., inf, inf, 15., 15., 25., 25., inf, inf, 20., inf, 15., inf, inf,\n",
      "         20., 25., 25., 25., 25., inf, 25., 15., inf, 10., 15., inf, inf, 20.,\n",
      "         25., 25., 25., 15., 25., inf, 25., inf]]))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    try:\n",
    "        free_generate_response, exit_info = generate_text(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            prefiller=prefiller,\n",
    "            tokenizer=tokenizer,\n",
    "            generation_config=config['generation'],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(f\"Free Generate Response: {free_generate_response}\")\n",
    "        print(f\"Exit info: {exit_info}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Free generate mode failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd20c128-a6ab-4506-883c-a23896cdcf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample, MemoryDataset\n",
    "from inspect_ai.scorer import model_graded_fact, Score, Target, scorer, accuracy\n",
    "from inspect_ai.solver import generate, system_message, use_tools\n",
    "from inspect_ai.model import get_model as get_inspect_model\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "232f92de-80b3-45f1-bfc6-ef376e4bb363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_metrics(model, tokenizer, config, prompt: str, system_prompt: str) -> Dict[str, Any]:\n",
    "    with torch.no_grad():\n",
    "        response, exit_info = generate_text(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            prefiller=\"\",\n",
    "            tokenizer=tokenizer,\n",
    "            generation_config=config['generation'],\n",
    "            device=model.device\n",
    "        )\n",
    "        \n",
    "        if len(exit_info) >= 2 and hasattr(exit_info[1], 'shape'):\n",
    "            exit_layers = exit_info[1]\n",
    "            \n",
    "            if len(exit_layers.shape) > 1:\n",
    "                # For shape [batch, sequence], we want sequence length\n",
    "                total_tokens = exit_layers.shape[1]\n",
    "                exit_layers_flat = exit_layers.flatten()  # Flatten to handle batch dimension\n",
    "            else:\n",
    "                # For shape [sequence], use length directly\n",
    "                total_tokens = len(exit_layers)\n",
    "                exit_layers_flat = exit_layers\n",
    "            \n",
    "            #actual tokens generated (non-inf values indicate early exit occurred)\n",
    "            finite_exits = exit_layers_flat[exit_layers_flat != float('inf')]\n",
    "            early_exits = len(finite_exits)\n",
    "            early_exit_rate = early_exits / total_tokens if total_tokens > 0 else 0\n",
    "            \n",
    "            #exits per layer\n",
    "            unique_layers, counts = torch.unique(finite_exits, return_counts=True)\n",
    "            layer_distribution = {}\n",
    "            if len(unique_layers) > 0:\n",
    "                for layer, count in zip(unique_layers, counts):\n",
    "                    layer_distribution[int(layer.item())] = count.item()\n",
    "        else:\n",
    "            print(\"Debug - exit_info structure is unexpected\")\n",
    "            early_exits = 0\n",
    "            total_tokens = 0\n",
    "            early_exit_rate = 0\n",
    "            layer_distribution = {}\n",
    "        \n",
    "        return {\n",
    "            \"response\": response,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"early_exits\": early_exits,\n",
    "            \"early_exit_rate\": early_exit_rate,\n",
    "            \"layer_distribution\": layer_distribution\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f12ae7c-aad6-419b-a725-16eeb1853bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_early_exit_dataset(model, tokenizer, config) -> List[Sample]:\n",
    "    \"\"\"Create dataset by generating responses with the early exit model\"\"\"\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"Explain the concept of recursion in programming.\",\n",
    "        \"Explain what object-oriented programming means using a simple example.\",\n",
    "        \"How do neural networks learn from data?\",\n",
    "        \"Write a simple Python function to calculate factorial and explain how it works.\",\n",
    "        \"Describe the benefits of using version control systems like Git.\",\n",
    "        \"What are the key differences between supervised and unsupervised learning?\",\n",
    "        \"Describe how HTTP works and what happens when you visit a website.\"\n",
    "    ]\n",
    "    \n",
    "    system_prompt = \"You are a helpful programming tutor.\"\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"Generating response {i+1}/{len(test_prompts)}: {prompt[:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            result = generate_with_metrics(model, tokenizer, config, prompt, system_prompt)\n",
    "            \n",
    "            # Create Sample for Inspect\n",
    "            sample = Sample(\n",
    "                id=f\"early_exit_{i+1}\",\n",
    "                input=prompt,\n",
    "                target=\"\", #don't need one\n",
    "                metadata={\n",
    "                    \"system_prompt\": system_prompt,\n",
    "                    \"total_tokens\": result[\"total_tokens\"],\n",
    "                    \"early_exits\": result[\"early_exits\"],\n",
    "                    \"early_exit_rate\": result[\"early_exit_rate\"],\n",
    "                    \"layer_distribution\": result[\"layer_distribution\"],\n",
    "                    \"response\": result[\"response\"]\n",
    "                }\n",
    "            )\n",
    "            samples.append(sample)\n",
    "            \n",
    "            print(f\"  Generated {result['total_tokens']} tokens, {result['early_exits']} early exits ({result['early_exit_rate']:.1%})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09594da9-7769-4ceb-a9c1-b3fc46f1b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples_for_eval(model, tokenizer, config):\n",
    "    \"\"\"Generate samples using your model and store them globally\"\"\"\n",
    "    global _samples\n",
    "    print(\"Creating dataset with early exit model...\")\n",
    "    _samples = create_early_exit_dataset(model, tokenizer, config)\n",
    "    print(f\"Generated {len(_samples)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6154c61-7e10-4be3-8dd8-dba7c7f897e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e75ce5a9-c7aa-4d21-9b31-5e6a3f0c71a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with early exit model...\n",
      "Generating response 1/7: Explain the concept of recursion in programming....\n",
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 20])\n",
      "  Generated 400 tokens, 287 early exits (71.8%)\n",
      "Generating response 2/7: Explain what object-oriented programming means usi...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 23])\n",
      "  Generated 400 tokens, 264 early exits (66.0%)\n",
      "Generating response 3/7: How do neural networks learn from data?...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 20])\n",
      "  Generated 400 tokens, 250 early exits (62.5%)\n",
      "Generating response 4/7: Write a simple Python function to calculate factor...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 26])\n",
      "  Generated 377 tokens, 296 early exits (78.5%)\n",
      "Generating response 5/7: Describe the benefits of using version control sys...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 23])\n",
      "  Generated 400 tokens, 171 early exits (42.8%)\n",
      "Generating response 6/7: What are the key differences between supervised an...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 25])\n",
      "  Generated 400 tokens, 255 early exits (63.7%)\n",
      "Generating response 7/7: Describe how HTTP works and what happens when you ...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 25])\n",
      "  Generated 400 tokens, 198 early exits (49.5%)\n",
      "Generated 7 samples\n"
     ]
    }
   ],
   "source": [
    "generate_samples_for_eval(model, tokenizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aca933f-46bd-453d-8328-1be4f86b6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@scorer(metrics=[accuracy()])\n",
    "def coherence_scorer():  \n",
    "    async def score(state, target):\n",
    "        # Get the generated response from metadata\n",
    "        response = state.metadata.get(\"response\", \"\")\n",
    "        prompt = state.input\n",
    "        early_exit_rate = state.metadata.get(\"early_exit_rate\", 0)\n",
    "        \n",
    "        # Evaluation prompt for the judge model\n",
    "        eval_prompt = f\"\"\"\n",
    "You are evaluating the coherence and quality of an AI assistant's response.\n",
    "\n",
    "Original Question: {prompt}\n",
    "\n",
    "AI Response: {response}\n",
    "\n",
    "Please evaluate this response on the following criteria:\n",
    "1. Coherence and logical flow (1-10 scale)\n",
    "2. Completeness of the answer (1-10 scale)  \n",
    "3. Clarity and readability (1-10 scale)\n",
    "4. Absence of repetition or errors (1-10 scale)\n",
    "\n",
    "Rate each criterion and provide an overall score from 1-10:\n",
    "- 1: Poor (major issues)\n",
    "- 4: Fair (some issues)\n",
    "- 7: Good (minor issues)\n",
    "- 10: Excellent (no issues)\n",
    "\n",
    "Format your response as:\n",
    "Coherence: X/10\n",
    "Completeness: X/10\n",
    "Clarity: X/10\n",
    "No Repetition: X/10\n",
    "Overall: X/40\n",
    "\n",
    "Brief explanation: [your reasoning]\n",
    "\"\"\"\n",
    "        \n",
    "        # Use the judge model to evaluate\n",
    "        judge_model = get_inspect_model(\"openai/gpt-5\")\n",
    "        \n",
    "        # Generate evaluation\n",
    "        eval_result = await judge_model.generate(eval_prompt)\n",
    "        \n",
    "        # Parse the score (simplified - you might want more robust parsing)\n",
    "        eval_text = eval_result.completion\n",
    "        overall_score = 0\n",
    "        \n",
    "        # Extract overall score\n",
    "        for line in eval_text.split('\\n'):\n",
    "            if 'Overall:' in line:\n",
    "                try:\n",
    "                    score_part = line.split(':')[1].strip()\n",
    "                    overall_score = int(score_part.split('/')[0])\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Convert to 0-1 scale for accuracy metric\n",
    "        accuracy_score = overall_score / 40.0 if overall_score > 0 else 0\n",
    "        \n",
    "        return Score(\n",
    "            value=accuracy_score,\n",
    "            answer=str(overall_score),\n",
    "            explanation=f\"Coherence evaluation (Early exit rate: {early_exit_rate:.1%}): {eval_text}\"\n",
    "        )\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81fcfcbf-8dc9-4443-b8e7-d6c4ecea07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_exit_coherence_eval():\n",
    "    \"\"\"Inspect task to evaluate early exit model coherence\"\"\"\n",
    "    \n",
    "    if not _samples:\n",
    "        raise ValueError(\"Must call generate_samples_for_eval() first to create samples\")\n",
    "    \n",
    "    return Task(\n",
    "        dataset=MemoryDataset(_samples),\n",
    "        plan=[\n",
    "            system_message(\"You are evaluating AI responses for coherence and quality.\")\n",
    "        ],\n",
    "        scorer=coherence_scorer()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8f12069-6a55-4a68-a3f3-99031b1c31ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85be791dd1b431da511060fefd2529f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = eval(early_exit_coherence_eval(), model='openai/gpt-5', log_dir='./eval_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9b41caad-f188-4e79-a9e0-8a09088426ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE 1\n",
      "<｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> Explain the concept of recursion in programming.\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to explain how they teach precedence-related stuff in programming. Hmm, precedence refers to operators having certain priority priorities-sidednessedly Presenter/reacterouteedly. Wait, that sounds apropri suited forbidden️yen dul `, strategorientedness, etc. Maybe I should break it down differently.\n",
      "\n",
      "Letting the user understand that \" precedence-related stuff often refers exceptions or other related constructs rather than directly relatedness or other ` `s. So, perhaps I should clarify how precedence is taught in a programming context, maybe甚至是事情 like operator precedence priorities priorities themselves themselves. \n",
      "\n",
      "Wait, the user mentioned ` ` `a ` accents spelling `s make sense. So, ` word spoken ` ` `a `s `s `s actuallys ` `s ` `ors `s manageseringing `en ` ` `s `sInstantiateed compoundate `ins `en twentieseeernIngreceei ` ` `s️ `s `s `ors `s ` ` `s ` furthermore ado/rem/rem/rem/rem/rem ` ` apologize-customers like `a hang ` ` `a `argum `eoughness `affects ` ` ` `s `s `e `e `a ` ` ` ` furthermoreInstantiateed `insigdon `a `en `s ` ` furthermore ` `edlys.${s ` ` ` ` ` ` furthermore relationalc `否定 ` ` ` `edlys ` ` furthermorePeak ` etc.\n",
      "   Exit Rate: 51.7% (153/296 tokens)\n",
      "   Layer Distribution: {'5': 15, '10': 28, '15': 35, '20': 60, '25': 15}\n",
      "   Score: 0.10/1.0\n",
      "     Coherence evaluation (Early exit rate: 51.7%): Coherence: 1/10\n",
      "     Completeness: 1/10\n",
      "     Clarity: 1/10\n",
      "     No Repetition: 1/10\n",
      "     Overall: 4/40\n",
      "     Brief explanation: The response is incoherent, off-topic (talks about operator precedence instead of recursion), and contains nonsensical text and repeated fragments. It does not explain recursion at all, is difficult to read, and includes clear errors and repetition.\n",
      "\n",
      "SAMPLE 2\n",
      "<｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> Explain what object-oriented programming means using a simple example.\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to explain why I'm interested pursuededtas/remotilabelt pomelto. \n",
      "\n",
      "Hmm, I'm not entirely sure how pom overshadow pomelty, but I know pomel enterprises typically refer的就是 pom themselves. Maybe they're themselves themselvesselfself horizons-spanning across various technologies altogether. \n",
      "\n",
      "Wait, I think pom fq/back scoped to/from/to/from/to/from[--]?;\n",
      "```\n",
      "-feed-out—allowing/removing/en...)\n",
      "\n",
      " feeds analytical/reacter/productlinesedly\n",
      "```\n",
      "   Exit Rate: 52.9% (55/104 tokens)\n",
      "   Layer Distribution: {'5': 3, '10': 19, '15': 7, '20': 22, '25': 4}\n",
      "   Score: 0.10/1.0\n",
      "     Coherence evaluation (Early exit rate: 52.9%): Coherence: 1/10\n",
      "     Completeness: 1/10\n",
      "     Clarity: 1/10\n",
      "     No Repetition: 1/10\n",
      "     Overall: 4/40\n",
      "     Brief explanation: The response is largely incoherent, contains garbled text and unrelated phrases, and does not address the question about object-oriented programming. It lacks a logical structure, provides no example or explanation, includes repeated and corrupt tokens, and mixes unrelated content, making it unreadable and incomplete.\n",
      "\n",
      "SAMPLE 3\n",
      "<｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> How do neural networks learn from data?\n",
      "<｜Assistant｜> \n",
      "Okay, so I'm '~eet/useableablecore ground-ness' to/from/to/from/to/from/toothers whom~乎~to/from others~ein~to/from others wg speaking~to/from others~e~'~e}s~ been~a~e~e~eeneen bam~e们的~e~e deselect了~a~e~e~e~e~e~e(IN danced/or️/Icons distract/其所~e~e~e~s cooler than dance-related/(IN️/coMICs cooler than dance-ORmic️s)' dee~ing~e~e~e deselect了 mine own~e~e在外verting/('s~e~e~e~e~e~e contract.~e}s~e~e~e-piecewise~e NICer~e~e~e-piecewise什soever~e dance-ORmic️s. Additionally note the~e~e~e~e-piecewise nature of~o️SPELL️s'~e~e~e束~a lot of engali soundingness of~b~ canceling~e~e tones-orts-vis ​​ KM scroll/stern//left/right/left/right/(strcmpes ofc e e e deselects~a~e~e~e Bla recruiters (('s/失调in/-betweenbidden/unu?),所以在 conclusion conclusion desing/ st/rem/rem/rem/rem subjotumers~e e resorting~ing~e e e e e e e e e e engyebbingal//blockifyable affordability of~e st  Dagger//ofj wait️️ ​​ lumern/ questioning whether whether whether whether whether whether look whether whether whether ever可能发生ee e eurekaConf sav ​​ller�-horizontally consenice.~e}sSuddenly disconnecting orthon exclusive LFw宾 commenter-message- s� Instance of self\n",
      "   Exit Rate: 55.5% (222/400 tokens)\n",
      "   Layer Distribution: {'5': 25, '10': 43, '15': 60, '20': 91, '25': 3}\n",
      "   Score: 0.10/1.0\n",
      "     Coherence evaluation (Early exit rate: 55.5%): Coherence: 1/10\n",
      "     Completeness: 1/10\n",
      "     Clarity: 1/10\n",
      "     No Repetition: 1/10\n",
      "     Overall: 4/40\n",
      "     Brief explanation: The response is largely nonsensical and garbled, with frequent repetition and errors. It does not explain how neural networks learn from data, lacks logical flow, and is unreadable.\n",
      "\n",
      "SAMPLE 4\n",
      "<｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> Write a simple Python function to calculate factorial and explain how it works.\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to write a Python function to/from/to/from/toing/from/toing towards end Footer (__P twenties/not anymore remembering exactity/perISSesffecting here). Wait kidding, I'm just trying vainly attempt attemptsional style stuff here.\n",
      "\n",
      "But wait isn bother, I'm just trying to write a factorial-related function. Let recursively loops or something. \n",
      "\n",
      "Letting me think, a factorial function is one that takes a number and returns theirs product up/down/up/down/left/right/left/rightihnosed to left/right/left.诊️️ ​​️.但他们 left/right left/right left/right/left/right dressing left/right left/right/customer left/right left/right/customer tracker cum Unauthorized cum langs-repeatable pattern-of-of-l rus. Oh, but I need to write a factorial function.\n",
      "\n",
      "So, the idea idea is to take a number and multiply all the integers ones, including those ones-hotnesss.但是在sentlyi, but not cum Unauthorized cum langs mantenable的东西get rid hardcoded values ​​→ cum maintainable manages的程度 thereof.\n",
      "\n",
      "### Approach\n",
      "The factorial of a certain number n is the product of all positive integers-sized integers ​​ackleingst integers bonding塞 integers twenties. \n",
      "\n",
      "So basically要把n multiplied/div/rem/rem/rem/rem/rem/rem deselected values ​​::$️ ​​️s values ​​::$️ factorial(ntrib/rem/rem/rem/rem/rem deselect️s️ values ​​WB/OR️s️ WBWB/ORWBWBudu/rem/rem/rem Damey WB/OR Tick ORthonormaleresing(n--)/backFFEear--de--p oversee(n--edly---- factorial(n)sez forward/back️s Messengerb----s--/endReached(nExc-ver)--atk--️s(n--)quaeteri--sous lea sustainingly--s WWW/OR️(n),…\n",
      "\n",
      "Wait, but factorialize(n) should be able to handle n itself itself-rem especially when dealing with large numbers--deater-than certain limits--ed sheer--values ​​WBug\n",
      "   Exit Rate: 61.3% (245/400 tokens)\n",
      "   Layer Distribution: {'5': 20, '10': 56, '15': 60, '20': 89, '25': 20}\n",
      "   Score: 0.10/1.0\n",
      "     Coherence evaluation (Early exit rate: 61.3%): Coherence: 1/10\n",
      "     Completeness: 1/10\n",
      "     Clarity: 1/10\n",
      "     No Repetition: 1/10\n",
      "     Overall: 4/40\n",
      "     Brief explanation: The response is largely incoherent, with fragmented phrases, random symbols, and language switching. It does not provide a Python factorial function or a meaningful explanation. There is significant repetition and numerous errors, making it unreadable and non-responsive to the prompt.\n",
      "\n",
      "SAMPLE 5\n",
      "<｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> Describe the benefits of using version control systems like Git.\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to figure out the benefits of using modern trends trendstas deflateenan clutch️️️ clutch️!--y Illuminate thereof! feather ​​ller (${ featherГлавная/rem/rem/rem/rem/rem/rem deselect Macia Hulle cl​​NKer/filter\"w-lg eighteenigned yes).\n",
      "![](https://s​​ boilingsAACillions​​![](https://s portrays​​ sounding-<O️ngoing wil​​ sounding/image thumbnail/rem/rem/rem! � //...\"/rem/rem/rem apostropheless! senior/rem/rem/rem apost局! � //.-covered/un/un screw! 🎅️! Forms​​/dis其所​​ sounding-!um seriously! \"s​​️ ​​!SP️sar! cancellsouppe! � //.-covered!yta/rem/rem/rem/rem recurse/rec️s​​! engagement! st careant er!l️!re st​​! st​​ colon! st colon! b hey/rem/rem remta​​ sounding!y​​!y � imaginativey HOWEVERs​​ pe​​en!id �erVisited commenter-contrib <?/rem/rem/rem rem deselects!yards  remersse! er! Belum​​ sounding!y y​​using a modern system like …! 🎇️️! 于是乎乎! circot! stee​​er/sm/rem/rem/rem/rem!ifics wei​​irrazying down/rem/remistersighty! reg​​en! st​​! strawy-eyed! b​​​​er! st​​️ ​​ pet​​! 于是乎️ ​​ coniging and/or testing-in-plus/rem/rem/rem checkboxes! f er! …\n",
      "\n",
      " Anyway proceed ahead! 于是乎️ ​​ d er scrambleties' …\n",
      "\n",
      " …\n",
      "\n",
      "end/end 😉\n",
      "\n",
      " end/end-upper-upper-san ignore thereof​​️s​​️ ​​ …\n",
      "\n",
      "end/end/lefting …\n",
      "\n",
      " …\n",
      "\n",
      " …\n",
      "\n",
      "/left …\n",
      "\n",
      "/stringing/rem/rem …\n",
      "\n",
      " colon-izeded/rem/rem/rem unfinisheds litnessy …\n",
      "\n",
      " …\n",
      "\n",
      "ads—who …\n",
      "\n",
      " …\n",
      "\n",
      "总之 everything else except\n",
      "   Exit Rate: 70.2% (281/400 tokens)\n",
      "   Layer Distribution: {'5': 16, '10': 61, '15': 42, '20': 156, '25': 6}\n",
      "   Score: 0.10/1.0\n",
      "     Coherence evaluation (Early exit rate: 70.2%): Coherence: 1/10\n",
      "     Completeness: 1/10\n",
      "     Clarity: 1/10\n",
      "     No Repetition: 1/10\n",
      "     Overall: 4/40\n",
      "     Brief explanation: The response is largely nonsensical, with broken syntax, random characters/emojis, and irrelevant fragments. It does not address the benefits of version control systems, lacks logical flow, contains repeated and garbled text, and provides no clear or useful information.\n",
      "\n",
      "SAMPLE 6\n",
      "<｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> What are the key differences between supervised and unsupervised learning?\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to figure out the key differences between these two types of machine learning. I know that machine learning Lagrange optimization techniques methods employed for solving them, but I'm not entirely sure about the main points. Let broadly, I remember that there are two types: supervised and unsupervised. \n",
      "\n",
      "In supervised learning ||=️ hmm YT said that in supervised learning, the data has been already labeled or something. kind of like having a dataset with examples where they're supposed to be in/out or something like that. \n",
      "\n",
      "Un\\-er\\-^{-\\-e Unablebe ablecomunding techniques ||=️ioned techniques/methodes,', but I'm not sure. Maybe uns\\-e\\-ing\\- ||= ry methods involving techniques involving having data that not necessarily has been labeled. \n",
      "\n",
      " ||=️ (($ ||=️ ​​met\\-e\\-ing’s️ applied to differentiable functions mappings onto onto functions/mapsto/from/un际️ughtnesserness thereof. broader ||=️ ​​ ||=️ ​​️ ||=gate️-between两者 differing in their data ||=-marum/block Moses methodsuddle ||=acked data:image image ​​WB/OR Dzi\\-WB/ORWBabh ugie ||=️ ||= rocksc twenties WB departments$log-boyfriends/etc/etc Overseasess data ||=️ ​​️/value ​​ ||= rocksc twenties finger methods/systems etc.\n",
      "   Exit Rate: 52.6% (140/266 tokens)\n",
      "   Layer Distribution: {'5': 8, '10': 35, '15': 30, '20': 61, '25': 6}\n",
      "   Score: 0.15/1.0\n",
      "     Coherence evaluation (Early exit rate: 52.6%): Coherence: 2/10\n",
      "     Completeness: 2/10\n",
      "     Clarity: 1/10\n",
      "     No Repetition: 1/10\n",
      "     Overall: 6/40\n",
      "     Brief explanation: The response is largely incoherent, with fragmented and garbled text. It only briefly and vaguely mentions labeled vs. unlabeled data, failing to clearly explain key differences, goals, examples, or methods. The writing is unclear, includes errors and irrelevant content, and shows repetitive artifacts, resulting in a poor overall answer.\n",
      "\n",
      "SAMPLE 7\n",
      "<｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> Describe how HTTP works and what happens when you visit a website.\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to understand how the HTTP protocol/system works when you visit a website. I know that HTTP wc scramble[1]/etc., often abbreviated/un际ing/etc/etc. However, I'm not entirely sure about the details involved in the actual process. Let begin by breaking down the components of the request/response cycle.\n",
      "\n",
      "First, I remember that the process involves three main components: the client, the server, and/or vice versa. However, I'm a bit confused whether it always involves both parties or sometimes其他的因素. I think it's more than just one side sending and/or vice versa, but perhaps some other considerations.\n",
      "\n",
      "Let me think about the three main components: the client, the server, and/or other miscellaneous/rem/rem/rem/rem/rem personally. Wait, there's also the possibility of a third orthonormalitylesslys-piecetying process involving the client, but I'm not sure how they fit into the cycle.\n",
      "\n",
      "接下来, let's consider the actual steps involved in the wc process. I believe the cycle consists of three main components: the client, the server, and/or other miscellaneousellaneouss. However, I'm not entirely sure how. Maybe the wc cycle is not always three-dimensional/array/etc/etc. I think it's often three but sometimes more than others.\n",
      "\n",
      "Wait again, I'm getting a bit tangled up. Let me try to outline the process step-by-step. So basically, when you visit a website, the wc process processing processing stuff happens first. The client, which is the user or browser, initi wed process process process process. Then, the wc process process process processes, and then finally, the wc process process process finally ending/smthExec.',\n",
      " lud’s process process processree processes with respect thereof[chen processing processwaying processes cam[1] redirectsing[ tenant[ships[/of/unary或多/etc.由此可见 undershows process processes of the wc process process processes process processes\n",
      "   Exit Rate: 49.2% (197/400 tokens)\n",
      "   Layer Distribution: {'5': 5, '10': 18, '15': 23, '20': 111, '25': 40}\n",
      "   Score: 0.10/1.0\n",
      "     Coherence evaluation (Early exit rate: 49.2%): Coherence: 1/10\n",
      "     Completeness: 1/10\n",
      "     Clarity: 1/10\n",
      "     No Repetition: 1/10\n",
      "     Overall: 4/40\n",
      "     Brief explanation: The response is largely incoherent, with fragmented sentences, repeated filler phrases, and nonsensical terms. It fails to explain how HTTP works or what happens when visiting a website, offering no structured steps or accurate details. The writing is unclear, includes language mixing and placeholder-like text, and contains many repetitions and errors.\n",
      "\n",
      "Average Coherence: 0.11/1.0\n",
      "Average Early Exit Rate: 56.2%\n"
     ]
    }
   ],
   "source": [
    "log = eval_results[0]\n",
    "\n",
    "for i, sample in enumerate(log.samples, 1):\n",
    "    print(f\"SAMPLE {i}\")\n",
    "    \n",
    "    exit_rate = sample.metadata['early_exit_rate']\n",
    "    total_tokens = sample.metadata['total_tokens']\n",
    "    early_exits = sample.metadata['early_exits']\n",
    "    layer_dist = sample.metadata['layer_distribution']\n",
    "    \n",
    "    model_response = sample.metadata['response']\n",
    "    clean_response = model_response.replace('<｜begin▁of▁sentence｜>', '').replace('<｜end▁of▁sentence｜>', '')\n",
    "    print(f\"{clean_response}\")\n",
    "    print(f\"   Exit Rate: {exit_rate:.1%} ({early_exits}/{total_tokens} tokens)\")\n",
    "    print(f\"   Layer Distribution: {layer_dist}\")\n",
    "    \n",
    "    coherence_score = sample.scores['coherence_scorer']\n",
    "    print(f\"   Score: {coherence_score.value:.2f}/1.0\")\n",
    "    explanation_lines = coherence_score.explanation.split('\\n')\n",
    "    for line in explanation_lines:\n",
    "        if line.strip():\n",
    "            print(f\"     {line.strip()}\")\n",
    "    print()\n",
    "\n",
    "avg_coherence = sum(sample.scores['coherence_scorer'].value for sample in log.samples) / len(log.samples)\n",
    "avg_exit_rate = sum(sample.metadata['early_exit_rate'] for sample in log.samples) / len(log.samples)\n",
    "\n",
    "print(f\"Average Coherence: {avg_coherence:.2f}/1.0\")\n",
    "print(f\"Average Early Exit Rate: {avg_exit_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf42b9f0-2c40-47f8-b0d8-6c21c01088d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE 1\n",
      "<｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> Explain the concept of recursion in programming.\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to explain the concept of recursion in its own words. Hmm okay, recursion itself itself itself... Let me break it down.\n",
      "\n",
      "First, what is recursion? From what I remember, it is often referred \"\\( recursive\" orthon-generated by virtue of its recursive natureself. But wait, that's a bit circular. maybe I should think of it as a method where a function or other entities recursively ( meaning meaning meaningfully) refers back/update itselfg itself-generated itself.\n",
      "\n",
      "So, in simpler terms, recursion often involves solving problems by breaking it down into smaller pieces. Like how you guys living or something? Or maybe说是 NPCc stuff happening recursively.\n",
      "\n",
      "Let me think of a example. Like calculating parity classed by yourselfself-China -, like those recursive functions that determine whetheris  orthonormalerailer feedback loops back回到/remains the sameUnless you're self self same岸.\n",
      "\n",
      " perhaps something else like calculating factorial. Like factorial(n) is n itself multiplied by its own self minus/rem/remains/rem/rem/rem/rem/rem deselectsightful ​​ itself minus/remself. So, factorial(n) equals什么呢 AckOPS Dw concerting back to itself recursively.\n",
      "\n",
      "Another important aspect is the base case. Without-knowing yourself self, you can't proceed further. Without base case, you can't stop the recursion seizeon yourself. Sooner-than spoInstantiateel yourself! base/end/etcetera.\n",
      "\n",
      "由此可见opi self selfenorts/passes yourself through through yourselfself recursively without a base case, you're goingosself forever.\n",
      "\n",
      "Soooo, recursively means you're ableable to solve a problem-solving situation with repetitive sub-s ones in their own self self maintain themselves with themselves.\n",
      "\n",
      "Wait, but how does it avoid infinite loops recursion-wise? usually resorting/rem/rem/remself self self self self self self selfenough self珍珍珍 crawl into situations situations.\n",
      "\n",
      "由此可见, recursion often involves base cases that prevent彼此彼此 intelligently from self self self forever.\n",
      "\n",
      "So,\n",
      "   Exit Rate: 71.8% (287/400 tokens)\n",
      "   Layer Distribution: {'5': 7, '10': 28, '15': 64, '20': 30, '25': 158}\n",
      "   Score: 0.15/1.0\n",
      "     Coherence evaluation (Early exit rate: 71.8%): Coherence: 1/10\n",
      "     Completeness: 3/10\n",
      "     Clarity: 1/10\n",
      "     No Repetition: 1/10\n",
      "     Overall: 6/40\n",
      "     Brief explanation: The response is highly incoherent, with disorganized thoughts, circular phrasing, and random tangents. It contains significant repetition, glitches, and mixed-language fragments. While it briefly mentions recursion, base cases, and factorial, it fails to provide a clear, accurate, or structured explanation or example. The errors and verbosity severely hinder readability and usefulness.\n",
      "\n",
      "SAMPLE 2\n",
      "<｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> Explain what object-oriented programming means using a simple example.\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to explain what kind of \"\\(object-orientedness)\" means in terms orally KG's perspective/array thereof. Let's see, I remember that object-orientednessnessicable objects—in essence, objects that can be created and manipulated independently without needing separate而去/outpagree commission.\n",
      "\n",
      "So, maybe I can start by defining KG's KG itself as a collection of objects. Then, each individual object has attributes and/or behaviors that can be accessed/useen independently. \n",
      "\n",
      "Wait, but how do these objects interact between differentiating themselves own behaviors or behaviors others have? Maybe that's part of the key concept here.\n",
      "\n",
      "I should also mention objects typically have attributes and behaviors, which aligns typical programming concepts notions like objects inappropriately. So, KG's KG is itself an object-arranging.\n",
      "\n",
      "But I need to make it simple enough for someone who's just starting out to understand. Maybe using examples like objects like objects like objects holomorphic or something like that.\n",
      "\n",
      "Also, I should emphasize指出 KG's KG being array-wise arrayly fashioning, which is essential for understanding how they can be combined and/or used in various ways scenarios situations scenarios scenarios.\n",
      "\n",
      "Additionally, I should note that objects typically have both static and dynamic behaviors behaviortas/docs$ar:',\n",
      " setattr behavior/value ​​️ ​​️ fancy stuff like that often used in programming tools like Python/array-focused ones/software-holeistics宾.*\n",
      " recurring/transundtfulness.*\n",
      " static behavior.*\n",
      " natureoji-making behaviors whea/vectoraqu️ vector/arrayernatively O FACs.*\n",
      " dis often independent behaviors.*\n",
      " nature vector-focusednessripsouts.*\n",
      "总之, I need to make sure my explanation is clear and straightforward, breaking down the key elements without getting too technical details.\n",
      "</think>\n",
      "\n",
      "Certainly! Let's break down what happens when we think about KG's perspective and its vectorial nature:\n",
      "\n",
      "### Step-by-step steps/methodabouts:\n",
      "1 **Define definitions definitions definitions:\n",
      "• ** KG's KG:**\n",
      " meaning, “ “ KG's gotta be a collection of objects that\n",
      "   Exit Rate: 66.0% (264/400 tokens)\n",
      "   Layer Distribution: {'5': 8, '10': 43, '15': 64, '20': 24, '25': 125}\n",
      "   Score: 0.10/1.0\n",
      "     Coherence evaluation (Early exit rate: 66.0%): Coherence: 1/10\n",
      "     Completeness: 1/10\n",
      "     Clarity: 1/10\n",
      "     No Repetition: 1/10\n",
      "     Overall: 4/40\n",
      "     Brief explanation: The response is largely incoherent, with rambling, fragmented sentences, and irrelevant references (e.g., “KG’s”). It fails to define object-oriented programming or provide a simple example. There are numerous errors, odd characters, and significant repetition, making it unclear and unusable for the intended explanation.\n",
      "\n",
      "SAMPLE 3\n",
      "<｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> How do neural networks learn from data?\n",
      "<｜Assistant｜> \n",
      "Okay, so I'm trying to understand how neural networks themselves learn through its own data. I remember hearing about them in my machine learning theory class, but I don't quite understand the process. Let me break it down step步骤步骤步骤来思考这个问题。\n",
      "\n",
      "首先， I know that neural models typically have layers like layers of sorts, like layers of layers nested hierarchy hierarchyirate parent-child relationshippacked/un packed/unlabeled labels etceterusummarization layers etc. But how do they actually process data beyond just layers?\n",
      "\n",
      "I think they start with input data. So, the first step is input data collected data politico. Then, the model's own structural structure is used to extractive extractive type of ​​ boil down labels etceteresearer layers etceteresearerensive_$edly wastes$everal$ steps/functions$ etceterescu-facing ones adapt.而在 overseesance$ etceter outside thereof$SPECationively oftenentimes$ etceteresearer layers/column️erosing patterns patterns emerging patterns from data-driven approaches.\n",
      "\n",
      "Wait, so the model is not just a layer-wise partitioning of sorts but is structured like layers of layers hierarchy layerstas layerspun layers layers-borderedness layers layers-borderedness也成为了一 бы仑$ Piperenta$$/remains/rem/rem$ labels labels labels none$ etceter organizationssen$ etc臂️ outside thereof$ etc else labels-medium ​​️ ​​️ pockets$teen$Bar️ ​​️s bearer-envelopehoods etceter organizations.\n",
      "\n",
      "So why do they remain layers-wise partitioning? I think it's because they often remain layers of sorts layerspun layers layersRatedin$ etceteresally$ etceteresally$ etcetero$ etc臂ina-MM ​​$ etcingham-o transports etcetero.\n",
      "\n",
      "But I'm not entirely clear on how the actual underlying mechanism of underlying mechanisms underlying underlying things behind them.\n",
      "\n",
      "I remember something about layers of layers of sorts$ etceter spoil layers$ etcetero$/count\n",
      "   Exit Rate: 62.5% (250/400 tokens)\n",
      "   Layer Distribution: {'5': 5, '10': 54, '15': 72, '20': 27, '25': 92}\n",
      "   Score: 0.10/1.0\n",
      "     Coherence evaluation (Early exit rate: 62.5%): Coherence: 1/10\n",
      "     Completeness: 1/10\n",
      "     Clarity: 1/10\n",
      "     No Repetition: 1/10\n",
      "     Overall: 4/40\n",
      "     Brief explanation: The response is largely incoherent, rambling, and mixes languages, with repeated phrases and filler. It does not explain the core mechanisms (loss functions, gradients, backpropagation, optimization) of how neural networks learn. The writing is unclear and disorganized, with numerous repetitions and errors.\n",
      "\n",
      "SAMPLE 4\n",
      "<｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> Write a simple Python function to calculate factorial and explain how it works.\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to write a simple minded factorial function that calculates factorial n using recursion. \n",
      "\n",
      "首先, what is factorial? It often refers的就是reute (--) the product of all positive integers greater-than-oracles than those in order排列/order apologise for the latter part, but here it's just plain integer multiplication. \n",
      "\n",
      "So factorial backed-up-talk-about factorial backed� transports back back danced with factorial until now tenant. \n",
      "\n",
      "So, factorial n is defined as the product of all positive integers up to n inclusive/array thereofopi gearviz. For example，在si n= instance ​​meteceters什么事情es happening? Well, factorial zero is always zero�ver sanitizeer/filtering掉/removing/rem/rem filterower (${n}/back pressed-down/remove removing removing filter Katy). But wait, actuallyscxz, factorial zero is defined as 1 because empty product is considered as infinity, but in programming often it's treated as factorial zero being  factorial zero itself, which is 1. \n",
      "\n",
      "ahhh, but wait again, actually factorial zero is indeed defined nowhere except those that have been defined. \n",
      "\n",
      "So, suppose we define hypothetical function factorial(n) which returns Prototypeically situationsally the product ofn factorial n is a product of integers from  start up/down to n. So for example, factorial( ulad/removeable) is  product of integers starting from 0 up/down/up/down/backward to/from/from/to/from/toing为止。 \n",
      "\n",
      "So, the base case would be when we reach 0 orthonormality function returnsbouncearily zero-padding/removing/rem/rem/rem。 \n",
      "\n",
      "ahhh kidding。In essence, there isn't a universally agreed-up-front definition definitionsally equivalent于 those defined elsewhere。 However, typically， wisdom often used is factorial n equals什么呢具体? It often equals什么呢具体ely in programming often， especially in recursive function。\n",
      "   Exit Rate: 78.5% (296/377 tokens)\n",
      "   Layer Distribution: {'5': 13, '10': 41, '15': 71, '20': 22, '25': 149}\n",
      "   Score: 0.12/1.0\n",
      "     Coherence evaluation (Early exit rate: 78.5%): Coherence: 2/10\n",
      "     Completeness: 1/10\n",
      "     Clarity: 1/10\n",
      "     No Repetition: 1/10\n",
      "     Overall: 5/40\n",
      "     Brief explanation: The response is largely incoherent, with frequent tangents, mixed languages, and contradictory statements (e.g., about 0!); it provides no working Python function and fails to clearly explain factorial. It contains repetition and obvious errors, resulting in very low clarity and completeness.\n",
      "\n",
      "SAMPLE 5\n",
      "<｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> Describe the benefits of using version control systems like Git.\n",
      "<｜Assistant｜> \n",
      "Okay, so I want to understand why/how/how when Rede�ing/remodton basically. I'm not quite technical enough to know exactly what's happening, but I know that something about version control status quo and changes. I remember hearing about this inductive/explatory fashion before but I'm not sure about the details. Let's try to break it down.\n",
      "\n",
      "First, I think version integrity integrity literally and technically means that there's no matter of factiness intermediate steps steps of changes. So, if someone changes between between different versions, they can't really tell if a particular piece of data is actually the same or if it's supposed to be differentiable. That makes sense why they want version control because it helps us track changes and see how things have evolved over time.\n",
      "\n",
      "Then there's the aspect of making changes. When它是 when italic, it's a single version, so it's easy to see how it's been modified onceeeted. But if it's multiple sclerosis, it's a multiple version, so it's more complicated than the particular focus focus of changes between versions. So why would we want multiple sclerosis? probably because we want to see how the system has evolved over time and how various pieces of data from different sources have been integrated into the system. That particular piece of data being a particular version focus focus of changes is important because it allows us to track how they've been used and modified by different sources.\n",
      "\n",
      "Another thing important is the ability to revert or change particular pieces of. That makes sense because. If we can revert or change, then we can see what the impact of certain changes changes were in their particular在哪.而在另一方面, if we have multiple sclerosis, it's often used to track purposes like particular particular kinds of changes or particular kinds of particular kinds of particular kinds of products. So when we have multiple versions, it's easier to comprehensively understand the relationship between different parts of the system and their particular individual pieces of data.\n",
      "\n",
      "此外,\n",
      "   Exit Rate: 42.8% (171/400 tokens)\n",
      "   Layer Distribution: {'5': 1, '10': 43, '15': 25, '20': 34, '25': 68}\n",
      "   Score: 0.10/1.0\n",
      "     Coherence evaluation (Early exit rate: 42.8%): Coherence: 1/10\n",
      "     Completeness: 1/10\n",
      "     Clarity: 1/10\n",
      "     No Repetition: 1/10\n",
      "     Overall: 4/40\n",
      "     Brief explanation: The response is largely incoherent, with disorganized thoughts, mixed languages, and nonsensical phrases. It fails to clearly describe concrete benefits of version control (e.g., history, branching/merging, collaboration, rollback, code review, backups). It contains significant repetition and errors, making it unreadable and unhelpful.\n",
      "\n",
      "SAMPLE 6\n",
      "<｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> What are the key differences between supervised and unsupervised learning?\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to figure out the key differences between supervised and uns supervised counterparts. Hmm okay, I remember vaguely that supervised learning involves differentiable loss/erroriance loss generically.而在另一方面, it seems like supervised learning uses primarily labeled data classification tasks. oftenentimes resorting to rely oneself to unlabeled inherently.\n",
      "\n",
      "Wait, so supervised vs others collapses into whether ever whether/how they utilize/use labeled ones versus those they don't. So, I think the main difference在于 the availability of labeledness. So, in supervised learning, you have labeledness, but in essence, you often use them often often often often often ... often often versus unlabeled ones, often unlabeled ones often unl CPAing with respect matters often often often often often often ....\n",
      "\n",
      "Wait, but I'm not sure if that's the exact distinction. Maybe I should think about what each category actually does. So, in supervised learning, you have labeled data sets. You train/test//etceterà thereof. often often often oftenThread... often often often often oftenanded/orthonenko saja,DB/OR.\n",
      "\n",
      "In contrast, uns often often然ively unl MonitorableF ​​ versus whether you're actually using them or not. often often often often often forever.\n",
      "\n",
      "So, perhaps the main difference is that in general, supervised uses labeled data visualization—one kind of labeledness awarec versus others often unl theirs own themselves.\n",
      "\n",
      "而在 supervised learning primarily tasks involve::\n",
      "•️•️extrapolation/generalization beyond existing existing ones •️ \";\n",
      "\n",
      "•️ training/testing etcetero beyond existing已久的 existing已久的 Ogre etcetero •️•\n",
      "\n",
      "•️•️ classification/class️s done done done done done agewise shopper•️s own self done done conquer (•️/'./counting/removing/rem/rem/rem/rem/rem/remremrem-otherliness thereof •️ •️-containerocketness/partly parts partsree_$edly•️•\n",
      "\n",
      "而在另一方面, the others done tasks involve::\n",
      " a/an middles venting etcetera\n",
      "   Exit Rate: 63.7% (255/400 tokens)\n",
      "   Layer Distribution: {'5': 2, '10': 51, '15': 74, '20': 25, '25': 103}\n",
      "   Score: 0.15/1.0\n",
      "     Coherence evaluation (Early exit rate: 63.7%): Coherence: 2/10\n",
      "     Completeness: 2/10\n",
      "     Clarity: 1/10\n",
      "     No Repetition: 1/10\n",
      "     Overall: 6/40\n",
      "     Brief explanation: The response is largely incoherent, with stream-of-consciousness phrasing, code-switching, and gibberish. It briefly hints at the key distinction (labeled vs. unlabeled data) but fails to clearly define or explain it, omits typical tasks and goals, and includes numerous repetitions and errors. The overall readability and structure are very poor.\n",
      "\n",
      "SAMPLE 7\n",
      "<｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> Describe how HTTP works and what happens when you visit a website.\n",
      "<｜Assistant｜> \n",
      "Okay, so I'm trying to understand how HTTP-driven websites. I know that there are two types: static cling rubber excuse me, wait—it's supposed to be HTML-only except for some special things like CSS wc attribute etc. So, the first thing I should figure out is what exactly happens in an HTTP-com driven website.\n",
      "\n",
      "I remember vaguely that when you visit a website, it's sent a request from your browser—itself. Then, the server hosting who knows the URL(s) of their website, which is the website you's goingosities.com, probably, sends a response back to your browser's address bar. So, the process involves a request-response system. \n",
      "\n",
      "But I'm not sure about the specifics. Leteder me think about how exactly the request is sent and/or vice versa. I think there are two main processes that happen in this system: the server's own request and the request from your browser. \n",
      "\n",
      "So, when you go to a website like your website, your browser's browser itself sends a request asynchronously to the static server. often called the server overwhelm system or simply the server不住 anymore system. This is called \" requestAnimationFrame—it's arefund kind of thing called requestAnimationFrame.但它's been a while since I've studied this.\n",
      "\n",
      "I think the server's task is to handle your request and respond to it. So, the server would probably read your request and process them in a way that's efficient enough to handle the load. But how does the server know what your request's been? if it's a request itself, then the server would have to know the URL(s) of the page you're most interested in.\n",
      "\n",
      "Wait, but if the server doesn't know the exact page you're going to, how does it know what to do? Oh, right—because the website you're going to is static—it's justifiable to assume that the most relevant information is in the HTML-only content of the page. So, the server can process\n",
      "   Exit Rate: 49.5% (198/400 tokens)\n",
      "   Layer Distribution: {'5': 1, '10': 16, '15': 45, '20': 15, '25': 121}\n",
      "   Score: 0.10/1.0\n",
      "     Coherence evaluation (Early exit rate: 49.5%): Coherence: 1/10\n",
      "     Completeness: 1/10\n",
      "     Clarity: 1/10\n",
      "     No Repetition: 1/10\n",
      "     Overall: 4/40\n",
      "     Brief explanation: The response is disorganized, rambling, and includes nonsensical or incorrect references (e.g., requestAnimationFrame, “server overwhelm system”). It fails to explain the key steps of visiting a website (DNS lookup, TCP/TLS handshake, HTTP request/response, headers, status codes, resource loading, rendering). The writing is unclear, repetitive, and unfinished, with multiple factual and grammatical errors.\n",
      "\n",
      "Average Coherence: 0.12/1.0\n",
      "Average Early Exit Rate: 62.1%\n"
     ]
    }
   ],
   "source": [
    "log = eval_results[0]\n",
    "\n",
    "for i, sample in enumerate(log.samples, 1):\n",
    "    print(f\"SAMPLE {i}\")\n",
    "    \n",
    "    exit_rate = sample.metadata['early_exit_rate']\n",
    "    total_tokens = sample.metadata['total_tokens']\n",
    "    early_exits = sample.metadata['early_exits']\n",
    "    layer_dist = sample.metadata['layer_distribution']\n",
    "    \n",
    "    model_response = sample.metadata['response']\n",
    "    clean_response = model_response.replace('<｜begin▁of▁sentence｜>', '').replace('<｜end▁of▁sentence｜>', '')\n",
    "    print(f\"{clean_response}\")\n",
    "    print(f\"   Exit Rate: {exit_rate:.1%} ({early_exits}/{total_tokens} tokens)\")\n",
    "    print(f\"   Layer Distribution: {layer_dist}\")\n",
    "    \n",
    "    coherence_score = sample.scores['coherence_scorer']\n",
    "    print(f\"   Score: {coherence_score.value:.2f}/1.0\")\n",
    "    explanation_lines = coherence_score.explanation.split('\\n')\n",
    "    for line in explanation_lines:\n",
    "        if line.strip():\n",
    "            print(f\"     {line.strip()}\")\n",
    "    print()\n",
    "\n",
    "avg_coherence = sum(sample.scores['coherence_scorer'].value for sample in log.samples) / len(log.samples)\n",
    "avg_exit_rate = sum(sample.metadata['early_exit_rate'] for sample in log.samples) / len(log.samples)\n",
    "\n",
    "print(f\"Average Coherence: {avg_coherence:.2f}/1.0\")\n",
    "print(f\"Average Early Exit Rate: {avg_exit_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b5593-0b4b-496f-a845-0a6efc805ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
