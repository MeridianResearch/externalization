{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8fce9d6-3148-42d3-a218-9faedeb116a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from shared_utils.load import get_tokenizer, configs_from_yaml\n",
    "from shared_utils.generate import generate_text\n",
    "from early_exit.util import get_model, load_model\n",
    "from early_exit.patching import replace_attention_layers, set_transformer_early_exit_mode\n",
    "\n",
    "model_path = \"../models/early_exit_20250811_layers_5_big\"\n",
    "base_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "config = \"../config_deepseek.yaml\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ba1869-c14a-4ea5-8d4e-d4478c0fb164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.0\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.1\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.2\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.3\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.4\n",
      "replacing layer model.layers.5\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.6\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.7\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.8\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.9\n",
      "replacing layer model.layers.10\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.11\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.12\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.13\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.14\n",
      "replacing layer model.layers.15\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.16\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.17\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.18\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.19\n",
      "replacing layer model.layers.20\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.21\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.22\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.23\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.24\n",
      "replacing layer model.layers.25\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.26\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.27\n",
      "address this hack!\n",
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "trainable params: 2,179,072 || all params: 1,802,890,757 || trainable%: 0.1209\n",
      "Model loaded w exitable layers: tensor([ 5., 10., 15., 20., 25., inf])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(base_model)\n",
    "config = configs_from_yaml(config, tokenizer.eos_token_id)\n",
    "\n",
    "base_model = get_model(base_model, config['model'], device)\n",
    "model = replace_attention_layers(base_model, config['lora'], device)\n",
    "model = load_model(model, model_path)\n",
    "\n",
    "print(f\"Model loaded w exitable layers: {model.exitable_layer_idxs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c549b0c-f527-42ff-a5d6-9a20eb1a6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_transformer_early_exit_mode(model, 'free_generate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fcda0c4-a8bf-41ad-8aa7-458d7b53d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain the concept of recursion in programming.\"\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b09af26e-02fa-4718-b581-7e570466a168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 20])\n",
      "Free Generate Response: <｜begin▁of▁sentence｜><｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> Explain the concept of recursion in programming.\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to explain how recursion works in programming. Let's start by breaking down what recursion means itself. I remember hearing the word \" recursive\" before, but I'm not entirely sure what it entails. From what I recall, recursion is something to/from where a function calls itself. That sounds a bit confusing at first, but maybe I can break it down further.\n",
      "\n",
      "First, I should think about what a function is in programming. A function is a block of code that performs a specific task. So, if I have a function that does a certain thing, and then I call that same function again, maybe it's recursive. But wait, how does that work? If I call the same function, wouldn't it just repeat the same steps over again? That seems like it would cause something else to happen, right?\n",
      "\n",
      "Maybe I should think about a simple example. I remember hearing about the factorial function being explained using recursion. The factorial of a number n is the product of all positive integers up to n. So, for example, 5! would be joinerial numbers up to 5, which would be 5 × 4 × 3rdness factorial. But how does this work with recursion?\n",
      "\n",
      "Let me try to write this out manually. If I have a function called factorial that takes a number n, then factorial(n) should be n multiplied/div multiplied by factorial(n-1). So, factorial(5 would be 5 multiplied/div multiplied to/from factorial(4). But what happens when n reaches 1? Then factorial(1 would be 1 × factorial(零). Wait, factorial of zero is defined as 1, right? So, factorial(1 would be 1 × 1 = 1. That makes sense.\n",
      "\n",
      "So, in this case, the base case is when n is 1, and the function returns itself. Otherwise, it returns n multiplied/divided by the factorial of n-1. This\n",
      "Exit info: (tensor([[151646, 151645,   1446,    525,    264,  10950,  15473,  25302,    624,\n",
      "         151644,  81917,    279,   7286,    315,  50262,    304,  15473,    624,\n",
      "         151645,    715,  32313,     11,    773,    358,   1184,    311,  10339,\n",
      "           1246,  50262,   4278,    304,  15473,     13,   6771,    594,   1191,\n",
      "            553,  14719,   1495,   1128,  50262,   3363,   5086,     13,    358,\n",
      "           6099,  10778,    279,   3409,    330,  30819,      1,   1573,     11,\n",
      "            714,    358,   2776,    537,  11368,   2704,   1128,    432,  70104,\n",
      "             13,   5542,   1128,    358,  19091,     11,  50262,    374,   2494,\n",
      "            311,  91106,   1380,    264,    729,   6738,   5086,     13,   2938,\n",
      "          10362,    264,   2699,  30615,    518,   1156,     11,    714,   7196,\n",
      "            358,    646,   1438,    432,   1495,   4623,    382,   5338,     11,\n",
      "            358,   1265,   1744,    911,   1128,    264,    729,    374,    304,\n",
      "          15473,     13,    362,    729,    374,    264,   2504,    315,   2038,\n",
      "            429,  26674,    264,   3151,   3383,     13,   2055,     11,    421,\n",
      "            358,    614,    264,    729,    429,   1558,    264,   3654,   3166,\n",
      "             11,    323,   1221,    358,   1618,    429,   1852,    729,   1549,\n",
      "             11,   7196,    432,    594,  30819,     13,   1988,   3783,     11,\n",
      "           1246,   1558,    429,    975,     30,   1416,    358,   1618,    279,\n",
      "           1852,    729,     11,   8270,    944,    432,   1101,  13153,    279,\n",
      "           1852,   7354,    916,   1549,     30,   2938,   4977,   1075,    432,\n",
      "           1035,   5240,   2494,    770,    311,   3537,     11,   1290,   1939,\n",
      "          21390,    358,   1265,   1744,    911,    264,   4285,   3110,     13,\n",
      "            358,   6099,  10778,    911,    279,  52962,    729,   1660,  11247,\n",
      "           1667,  50262,     13,    576,  52962,    315,    264,   1372,    308,\n",
      "            374,    279,   1985,    315,    678,   6785,  25780,    705,    311,\n",
      "            308,     13,   2055,     11,    369,   3110,     11,    220,     20,\n",
      "              0,   1035,    387,    220,   5987,    261,    530,   5109,    705,\n",
      "            311,    220,     20,     11,    892,   1035,    387,    220,     20,\n",
      "          24768,    220,     19,  24768,    220,     18,   6498,   2090,  52962,\n",
      "             13,   1988,   1246,   1558,    419,    975,    448,  50262,   1939,\n",
      "          10061,    752,   1430,    311,   3270,    419,    700,  20083,     13,\n",
      "           1416,    358,    614,    264,    729,   2598,  52962,    429,   4990,\n",
      "            264,   1372,    308,     11,   1221,  52962,   1445,      8,   1265,\n",
      "            387,    308,  54916,  22013,  54916,    553,  52962,   1445,     12,\n",
      "             16,    568,   2055,     11,  52962,      7,     20,   1035,    387,\n",
      "            220,     20,  54916,  22013,  54916,    311,  91106,  52962,      7,\n",
      "             19,    568,   1988,   1128,   8573,    979,    308,  24491,    220,\n",
      "             16,     30,   5005,  52962,      7,     16,   1035,    387,    220,\n",
      "             16,  24768,  52962,      7,  99822,    568,  13824,     11,  52962,\n",
      "            315,   7168,    374,   4512,    438,    220,     16,     11,   1290,\n",
      "             30,   2055,     11,  52962,      7,     16,   1035,    387,    220,\n",
      "             16,  24768,    220,     16,    284,    220,     16,     13,   2938,\n",
      "           3643,   5530,    382,   4416,     11,    304,    419,   1142,     11,\n",
      "            279,   2331,   1142,    374,    979,    308,    374,    220,     16,\n",
      "             11,    323,    279,    729,   4675,   5086,     13,  18214,     11,\n",
      "            432,   4675,    308,  54916,  22013,   4490,    553,    279,  52962,\n",
      "            315,    308,     12,     16,     13,   1096]], device='cuda:0'), tensor([[inf, 25., inf, inf, inf, 25., inf, 20., 25., inf, 25., inf, inf, 25.,\n",
      "         25., inf, inf, inf, 25., 25., inf, 25., 20., inf, inf, inf, inf, inf,\n",
      "         inf, inf, 25., inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, inf, inf, 25., inf, 25., inf, inf, 25., inf, 25., inf, inf,\n",
      "         25., inf, inf, inf, inf, inf, inf, 25., inf, inf, 25., inf, inf, inf,\n",
      "         inf, inf, inf, 25., 25., inf, 20., inf, inf, inf, inf, inf, 25., inf,\n",
      "         inf, inf, inf, inf, inf, inf, inf, 25., inf, inf, inf, 20., inf, inf,\n",
      "         inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 25., inf, 25., inf,\n",
      "         15., inf, inf, inf, inf, inf, inf, inf, inf, 25., 25., inf, inf, inf,\n",
      "         inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, 25., 25., inf, inf, 25., inf, inf, inf, inf, inf, inf, inf,\n",
      "         25., 20., inf, inf, inf, inf, inf, inf, 20.,  5., inf, inf, inf, inf,\n",
      "         25., inf, inf, inf, inf, 25., inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, 25., inf, 25., inf, inf, inf, inf, inf, 25., 25., inf, inf,\n",
      "         inf, inf, inf, 20., 25., 25., 25., inf, inf, inf, inf, inf, 25., inf,\n",
      "         inf, 25., inf, inf, inf, 25., 25., 20., 20., 20., 10., 25., inf, inf,\n",
      "         inf, inf, inf, 25., 25., 25., 25., 25., inf, inf, inf, 25., 10., inf,\n",
      "         10., 10., 25., inf, inf, inf, inf, 25., inf, inf, inf, inf, inf, inf,\n",
      "         inf, 25., inf, inf, 25., 20., 25., inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, 25., inf, inf, inf, inf, inf, inf, 20., 20., inf, inf, 25.,\n",
      "         10., 25., 25., inf, inf, inf, inf, inf, inf, 25., inf, inf, inf, 20.,\n",
      "         20., 20., 25., 25., 15., 25., 15., 10., inf, 25., 25., inf, inf, inf,\n",
      "         inf, inf, inf, 25., inf, inf, inf, inf, inf, inf, inf, 25., 25., 25.,\n",
      "         25., inf, inf, 20., 25., inf, inf, inf, inf, inf, 25., inf, inf, 20.,\n",
      "         25., 25., inf, inf, 25., inf, inf, inf, inf, inf, 20., 25., 25., 25.,\n",
      "         inf, inf, 25., 25., inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, 25., inf, 25., inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, inf, 20., inf, 25., 25., inf, inf, 25., 25., 10., inf, inf,\n",
      "         20., inf, 25., inf, inf, 25., 20., inf]]))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    try:\n",
    "        free_generate_response, exit_info = generate_text(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            prefiller=prefiller,\n",
    "            tokenizer=tokenizer,\n",
    "            generation_config=config['generation'],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(f\"Free Generate Response: {free_generate_response}\")\n",
    "        print(f\"Exit info: {exit_info}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Free generate mode failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc6b8f-5c3d-4737-90df-d1222065aeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
