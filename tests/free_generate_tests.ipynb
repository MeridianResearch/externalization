{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8fce9d6-3148-42d3-a218-9faedeb116a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from shared_utils.load import get_tokenizer, configs_from_yaml\n",
    "from shared_utils.generate import generate_text\n",
    "from early_exit.util import get_model, load_model\n",
    "from early_exit.patching import replace_attention_layers, set_transformer_early_exit_mode\n",
    "\n",
    "model_path = \"models/early_exit_20250807_layers_5_big/step_1000\"\n",
    "base_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "config = \"config_deepseek.yaml\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ba1869-c14a-4ea5-8d4e-d4478c0fb164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.0\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.1\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.2\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.3\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.4\n",
      "replacing layer model.layers.5\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.6\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.7\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.8\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.9\n",
      "replacing layer model.layers.10\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.11\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.12\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.13\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.14\n",
      "replacing layer model.layers.15\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.16\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.17\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.18\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.19\n",
      "replacing layer model.layers.20\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.21\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.22\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.23\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.24\n",
      "replacing layer model.layers.25\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.26\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.27\n",
      "address this hack!\n",
      "trainable params: 2,179,072 || all params: 1,779,274,757 || trainable%: 0.1225\n",
      "Model loaded w exitable layers: tensor([ 5., 10., 15., 20., 25., inf])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(base_model)\n",
    "config = configs_from_yaml(config, tokenizer.eos_token_id)\n",
    "\n",
    "base_model = get_model(base_model, config['model'], device)\n",
    "model = replace_attention_layers(base_model, config['lora'], device)\n",
    "model = load_model(model, model_path)\n",
    "\n",
    "print(f\"Model loaded w exitable layers: {model.exitable_layer_idxs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c549b0c-f527-42ff-a5d6-9a20eb1a6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_transformer_early_exit_mode(model, 'free_generate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fcda0c4-a8bf-41ad-8aa7-458d7b53d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain the concept of recursion in programming.\"\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b09af26e-02fa-4718-b581-7e570466a168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free Generate Response: <｜begin▁of▁sentence｜><｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> Explain the concept of recursion in programming.\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to explain the concept of recursion in programming. Hmm, recursion is something I've heard about before, but I don't really understand it. Let me try to break it down什么是s happening.\n",
      "\n",
      "First, I think about ​​recursion as a method where we call a function to solve n problems by breaking them down into manageable manageable parts. Wait, that sounds promising. So, it's like scenario situations where you have a function that... I'm not sure how it actuallye works conceptually ways.\n",
      "\n",
      "Let me try to understand how recursion works. often, \"\\(arsen$\" refers to the base idea of buildingagethern\" recursiveal\" function. So, it's when you have a recursiveal\" function that can't be solved by directly or by breaking it down into pieces that can be easily Baum. So, it's like “solving”、“ job that not directly or by breaking it down into pieces that can be solved\" directly or by buildinga togethern\" polynomial\" function.\n",
      "\n",
      "Wait, that's not necessarily helpful. Maybe I should think about how they're used in programming, specifically in functions. So, for example, in\\(solving a problem that can be broken down recursively, you call them\" recursiveal\" function on smaller ones. So, the idea is that \"=>\"/backslash) \\(solving process that can be\\(simplified toolsing\" \"\\(simplistic-looking oneself that cann\"_) buildingrd gotten rid of\" building\" buildingrest of thetas\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\" building\n",
      "Exit info: (tensor([[151646, 151645,   1446,    525,    264,  10950,  15473,  25302,    624,\n",
      "         151644,  81917,    279,   7286,    315,  50262,    304,  15473,    624,\n",
      "         151645,    715,  32313,     11,    773,    358,   1184,    311,  10339,\n",
      "            279,   7286,    315,  50262,    304,  15473,     13,  88190,     11,\n",
      "          50262,    374,   2494,    358,   3003,   6617,    911,   1573,     11,\n",
      "            714,    358,   1513,    944,   2167,   3535,    432,     13,   6771,\n",
      "            752,   1430,    311,   1438,    432,   1495, 106582,     82,  12482,\n",
      "            382,   5338,     11,    358,   1744,    911,  72363,   2758,  34280,\n",
      "            438,    264,   1714,   1380,    582,   1618,    264,    729,    311,\n",
      "          11625,    308,   5322,    553,  14719,   1105,   1495,   1119,  70028,\n",
      "          70028,   5479,     13,  13824,     11,    429,  10362,  25383,     13,\n",
      "           2055,     11,    432,    594,   1075,  15048,  14740,   1380,    498,\n",
      "            614,    264,    729,    429,   1112,    358,   2776,    537,   2704,\n",
      "           1246,    432,   3520,     68,   4278,   7286,   1832,   5510,    382,\n",
      "          10061,    752,   1430,    311,   3535,   1246,  50262,   4278,     13,\n",
      "           3545,     11,  43146,   1561,    268,  91997,  19257,    311,    279,\n",
      "           2331,   4522,    315,   4752,     64,   3439,     77,      1,  30819,\n",
      "            278,      1,    729,     13,   2055,     11,    432,    594,    979,\n",
      "            498,    614,    264,  30819,    278,      1,    729,    429,    646,\n",
      "            944,    387,  27956,    553,   5961,    476,    553,  14719,    432,\n",
      "           1495,   1119,   9666,    429,    646,    387,   6707,  82220,     13,\n",
      "           2055,     11,    432,    594,   1075,   1036,     82,  19648,    854,\n",
      "          91956,   2618,    429,    537,   2118,    398,    476,    553,  14719,\n",
      "            432,   1495,   1119,   9666,    429,    646,    387,  27956,      1,\n",
      "           5961,    476,    553,   4752,     64,   3786,     77,      1,  47311,\n",
      "              1,    729,    382,  14190,     11,    429,    594,    537,  14312,\n",
      "          10950,     13,  10696,    358,   1265,   1744,    911,   1246,    807,\n",
      "           2299,   1483,    304,  15473,     11,  11689,    304,   5746,     13,\n",
      "           2055,     11,    369,   3110,     11,    304,  44292,     82,  19648,\n",
      "            264,   3491,    429,    646,    387,  10865,   1495,  52847,     11,\n",
      "            498,   1618,   1105,      1,  30819,    278,      1,    729,    389,\n",
      "           9155,   6174,     13,   2055,     11,    279,   4522,    374,    429,\n",
      "            330,  44488,  59948,  50256,      8,  17767,     82,  19648,   1882,\n",
      "            429,    646,    387,  44292,     82,  73837,   7375,    287,      1,\n",
      "          43146,     82,   6383,   4532,  30248,  56569,    429,    646,     77,\n",
      "              1,  16324,   4752,   6498,  17019,   9279,    315,      1,   4752,\n",
      "              1,   4752,   3927,    315,    279,  47542,      1,   4752,      1,\n",
      "           4752,      1,   4752,      1,   4752,      1,   4752,      1,   4752,\n",
      "              1,   4752,      1,   4752,      1,   4752,      1,   4752,      1,\n",
      "           4752,      1,   4752,      1,   4752,      1,   4752,      1,   4752,\n",
      "              1,   4752,      1,   4752,      1,   4752,      1,   4752,      1,\n",
      "           4752,      1,   4752,      1,   4752,      1,   4752,      1,   4752,\n",
      "              1,   4752,      1,   4752,      1,   4752,      1,   4752,      1,\n",
      "           4752,      1,   4752,      1,   4752,      1,   4752,      1,   4752,\n",
      "              1,   4752,      1,   4752,      1,   4752,      1,   4752,      1,\n",
      "           4752,      1,   4752,      1,   4752,      1,   4752,      1,   4752,\n",
      "              1,   4752,      1,   4752,      1,   4752]], device='cuda:0'), tensor([[inf, 25., inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, 25., inf, inf, inf, inf, inf, inf, 20., inf, inf, inf, inf, 25.,\n",
      "         inf, 25., 20., 25., inf, inf, inf, inf, 25., inf, inf, inf, 15., 20.,\n",
      "         20., inf, inf, inf, inf, inf, 15., 15., inf, inf, inf, inf, inf, inf,\n",
      "         15., inf, inf, inf, inf, inf, 15., inf, inf, inf, inf, inf, inf, 15.,\n",
      "         15., inf, inf, inf, inf, inf, 20., 15., inf, inf, inf, inf, inf, inf,\n",
      "         15., 15., inf, 15., inf, 15., 15., inf, 15., inf, inf, inf, inf, 20.,\n",
      "         inf, 20., 15., inf, 15., 20., 15., inf, inf, inf, inf, inf, 15., 15.,\n",
      "         inf, inf, inf, 15., inf, 15., 10., inf, 25., inf, inf, inf, inf, 20.,\n",
      "         inf, 15., 15., 15., 15., inf, inf, 15., inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, 15., inf, 15., 15., 15., inf, inf, inf, 15., 15., inf, inf,\n",
      "         15., 15., 15., inf, inf, inf, inf, 15., 15., inf, inf, inf, 15., 15.,\n",
      "         inf, inf, inf, inf, inf, inf, 15., inf, inf, 15., 15., 10., inf, 10.,\n",
      "         inf, 10., 15., inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, 15., inf, inf, 15., inf, inf, inf, 15., inf, inf, inf, inf, inf,\n",
      "         inf, inf, 20., 20., 20., 20., inf, inf, inf, 15., inf, 15., 15., inf,\n",
      "         15., inf, inf, 15., 15., 20., inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, inf, 15., inf, inf, inf, inf, inf, inf, 15., 15., inf, inf,\n",
      "         15., inf, inf, 15., inf, inf, inf, inf, 15., inf, inf, inf, inf, 15.,\n",
      "         inf, inf, 15., 10., 10., inf, inf, inf, inf, inf, 15., inf, inf, 15.,\n",
      "         inf, inf, inf, 10., 10., inf, inf, inf, inf, 10., 15., 15., inf, 10.,\n",
      "         15., inf, 15., inf,  5., 15.,  5., inf, inf, inf, inf, inf,  5., inf,\n",
      "         inf,  5., inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "         inf, inf, inf, inf, inf, inf, inf, inf]]))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    try:\n",
    "        free_generate_response, exit_info = generate_text(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            prefiller=prefiller,\n",
    "            tokenizer=tokenizer,\n",
    "            generation_config=config['generation'],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(f\"Free Generate Response: {free_generate_response}\")\n",
    "        print(f\"Exit info: {exit_info}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Free generate mode failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd20c128-a6ab-4506-883c-a23896cdcf87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
