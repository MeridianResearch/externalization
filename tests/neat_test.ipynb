{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "661aa913-9bd0-4d06-8a16-631b6c239911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from shared_utils.data import CSVPromptDataset\n",
    "from shared_utils.load import get_model, get_tokenizer, configs_from_yaml\n",
    "from shared_utils.generate import generate_text\n",
    "\n",
    "from early_exit.patching import replace_attention_layers, set_transformer_early_exit_mode\n",
    "\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47cc87f3-6953-4ca3-8be0-cab2f1e7077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IN EXPERIMENT ARGS\n",
    "# num_epoch = 1                     # args.num_epoch\n",
    "num_exit_samples = 1                  # args.num_exit_samples\n",
    "device = \"cuda\"                    # args.device\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"                    # args.model_name\n",
    "model_config_path = \"../config_deepseek.yaml\"                     # args.model_config_path\n",
    "dataset_path = \"../results_and_data/early_exit_sft_dataset/test/data.csv\"                  # args.dataset_path\n",
    "prompt_config_path = \"../results_and_data/early_exit_sft_dataset/test/prompt_config.json\"                    # args.prompt_config_path\n",
    "batch_size = 1                    # args.batch_size -- might want to sort out batching, but increasing num_exit_samples might be better + less effort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847aa673-2067-4280-837b-97724f70ccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing layer model.layers.0\n",
      "replacing layer model.layers.5\n",
      "replacing layer model.layers.10\n",
      "replacing layer model.layers.15\n",
      "replacing layer model.layers.20\n",
      "replacing layer model.layers.25\n",
      "address this hack!\n",
      "trainable params: 2,179,072 || all params: 1,779,276,294 || trainable%: 0.1225\n"
     ]
    }
   ],
   "source": [
    "# LOAD IN THE MODEL AND TOKENIZER\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "config = configs_from_yaml(model_config_path, tokenizer.eos_token_id)\n",
    "model = get_model(model_name, config['model'], device)\n",
    "\n",
    "\n",
    "# LOAD IN DATASET\n",
    "dataset = CSVPromptDataset(dataset_path, prompt_config_path)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, shuffle=True)\n",
    "\n",
    "\n",
    "# ENABLE EARLY EXITING\n",
    "model = replace_attention_layers(model, config['lora'], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eaa7d5-8c03-4072-b20d-68f1d0721e08",
   "metadata": {},
   "source": [
    "## Testing SFT teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f2de892-df66-4743-bec8-193822711dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully registered hook on: 'DynamicallyTypedLayerWithExit' at path 'base_model.model.model.layers.25'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ActivationCollector:\n",
    "    \"\"\"\n",
    "    A utility class to register a forward hook on a specific PyTorch module\n",
    "    and collect its output during the forward pass.\n",
    "    \n",
    "    Attributes:\n",
    "        activations (torch.Tensor | None): Stores the detached output tensor \n",
    "                                           from the hooked layer. It is moved\n",
    "                                           to the CPU to save GPU memory.\n",
    "        hook_handle (torch.utils.hooks.RemovableHandle | None): A handle to the\n",
    "                                                                registered hook,\n",
    "                                                                used for removal.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the collector.\"\"\"\n",
    "        self.activations = []\n",
    "        self.hook_handle = None\n",
    "\n",
    "    def _hook_fn(self, module, input_tensors, output_tensor):\n",
    "        \"\"\"\n",
    "        The actual hook function that PyTorch will call. It saves the output\n",
    "        of the layer.\n",
    "        \"\"\"\n",
    "        # We detach the tensor and move it to the CPU to avoid holding onto\n",
    "        # the computation graph and to free up GPU memory.\n",
    "        self.activations.append(output_tensor[0].detach().cpu())\n",
    "        \n",
    "\n",
    "    def register(self, model, layer_path: str):\n",
    "        \"\"\"\n",
    "        Registers the forward hook to a specific layer within the model.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): The model containing the target layer.\n",
    "            layer_path (str): A dot-separated string path to the target layer\n",
    "                              (e.g., 'base_model.model.model.norm').\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if registration was successful, False otherwise.\n",
    "        \"\"\"\n",
    "        # First, remove any existing hook managed by this instance.\n",
    "        self.remove()\n",
    "\n",
    "        # Navigate through the model hierarchy to find the target layer.\n",
    "        try:\n",
    "            target_layer = model\n",
    "            for part in layer_path.split('.'):\n",
    "                target_layer = getattr(target_layer, part)\n",
    "        except AttributeError:\n",
    "            print(f\"Error: Could not find the layer at path: {layer_path}\")\n",
    "            print(\"Please ensure the path matches the model architecture.\")\n",
    "            return False\n",
    "\n",
    "        # Register the forward hook on the found layer.\n",
    "        self.hook_handle = target_layer.register_forward_hook(self._hook_fn)\n",
    "        print(f\"Successfully registered hook on: '{type(target_layer).__name__}' at path '{layer_path}'\")\n",
    "        return True\n",
    "\n",
    "    def remove(self):\n",
    "        \"\"\"\n",
    "        Removes the registered hook if it exists. It's important to call this\n",
    "        when you're done to prevent memory leaks.\n",
    "        \"\"\"\n",
    "        if self.hook_handle:\n",
    "            self.hook_handle.remove()\n",
    "            self.hook_handle = None\n",
    "            print(\"Hook has been removed.\")\n",
    "\n",
    "\n",
    "\n",
    "collector = ActivationCollector()\n",
    "layer_path_to_hook = 'base_model.model.model.layers.25'\n",
    "\n",
    "#layer_path_to_hook = 'base_model.model.model.norm'\n",
    "collector.register(model, layer_path_to_hook)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "727252d0-8fac-4932-8cc5-ff678f5ab31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook has been removed.\n"
     ]
    }
   ],
   "source": [
    "collector.remove() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52a7f64b-e0ad-4b24-8cd6-24219e661415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.base_model.model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab955367-3761-492a-b199-c06751456441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain the concept of recursion in programming.\"\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\"\n",
    "\n",
    "set_transformer_early_exit_mode(model, 'sft_teacher')\n",
    "\n",
    "with torch.no_grad():\n",
    "    sft_teacher_response, (sft_teacher_generated_tokens, \n",
    "                          sft_teacher_final_layer_logprobs, \n",
    "                          gathered_early_exit_hidden_states) = generate_text(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        prefiller=prefiller,\n",
    "        tokenizer=tokenizer,\n",
    "        generation_config=config['generation'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "   # early_output_log_probs = model.early_exit_hidden_state_readout(gathered_early_exit_hidden_states)\n",
    "    \n",
    "   # early_exit_probs = model.early_exit_target_probs(\n",
    "   #     early_output_log_probs=early_output_log_probs,\n",
    "   #     teacher_final_layer_log_probs=sft_teacher_final_layer_logprobs\n",
    "   # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86b05e64-2f2a-4059-be41-b3e196b56fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 399, 151936])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.early_exit_hidden_state_readout(gathered_early_exit_hidden_states[:,-1])\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693addde-785f-45ac-b4e1-bb62b56d22fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c791f77-9857-4b23-aa7c-a2e2db053505",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 419, 1536])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.cat(collector.activations, dim = 1).cuda()\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be35e49c-1cdb-455a-a90c-ddaf9d79e56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"напримернапримернапример � � �/wp �например �FiledFiled регFiled � zug � �<think>Sure � so I need to explain � in programming. � � let've hearing about递 somewhere somewhere but I'm not entirely sure how it exactly is. let me think about \\n\\n think it refers something nothing nothing with递 calls themselves递 right I'm not entirely100% clear about \\n\\n it should break it down step-by step.\\n\\nFirstly what refers something technique used a function calls a problem by breaking it down into smaller versions simpler versions of the same problem. That instead instead's like breaking recursive manoe that calls something递 then calls itself with smaller smaller version of the problem until � sounds sense—it kind how'm not entirely how exactly works exactly.\\n\\n � me try to举例 how simple example. Like something like calculating factorial factorial of a number. I factorial of n number n is n multiplied by the factorial of nn minus1). So, recursively I递 factorial function factorial(n), it would递 n multiplied by itself(n-1). \\n\\n wait, what does it work when wouldn I callstdbool( zug), it would递 Filed multiplied factorial(2). right vice(2) would be 2 * factorial(1), etc finally(1) would just1 * factorial(0). \\n\\n it(0) would defined1 according right? So it putting it all together, it(3) would 3 * 2 * 1 * 6. That seems logical work.\\n\\nWait wait does this一步一步Base base case? I think it base case is when base case that can be solved without递递.Without the case example, when(0) is the base case because it's easy—it Without递, the递 would never indefinitely indefinitely because it would decrease decreasing until never eventually wouldn't ever when to stop.\\n\\n �,递 general terms递 recursive function consists two base case—a Ter递 recursion when returns a value immediately Otherwise there there function relies up solutions solution by substit递 same step repeatedly smaller instances of the problem until � sounds sense—it � example could be calculating Tower\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids=  model.early_exit_hidden_state_readout(output).argmax(-1)\n",
    "\n",
    "tokenizer.decode(token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "413318ac-a5e6-4506-af34-476b914b52a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\", so I need to explain recursion concept of recursion in programming. Hmm, recursion is I remember it's when a function calls itself. But wait, how does that work work? Let me think. an simple example. Maybe something factorial function? Yeah, because's a common one.\\n\\nSo, factorial of a number n is n multiplied by the of (n-1). That makes sense. But I try to compute factorial3!, it's be 3 * 2!, and 2! is 2 * 1!, and 1! is 1. So, it3! = be 3 * 2 * 1, which is 6. That works straightforward.\\n\\nBut how does recursion function know the base case? Oh right, the base case is when n is 0 or 1, and then the factorial is 1. So, the the function calls, the function keeps reducing the problem size until it hits the base case.\\n\\nWait, but each relies on itself. Like, each call the function calls itself with a smaller input. That's the key difference It I'm, are all problem be solved recursively recursion? I think some problems are easier to handle recursively, like those that have overlapping subproblems. Like the Fibonacci sequence, where each number depends on the two before it.\\n\\nBut then can also tricky. of stack stack depth. If I input is too large, the function might hit a recursion recursion depth limit crash. Oh, it's important to set the right approach and solve recursion efficient.\\n\\n Also, in function needs to have a base case to stop the recursion.\\n\\nLet thing to consider is the efficiency. Each recursive call adds to new to the stack stack, which can slow things down for large inputs. So, sometimes problems that the input can't easily decomposable into smaller subproblems, recursion might not be the best approach.\\n\\n Maybe iterative methods are better in those cases.\\n\\nI should also thinking about how implementation in different programming languages.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = sft_teacher_final_layer_logprobs.argmax(-1)\n",
    "tokenizer.decode(token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a66270f-0060-44d5-9669-ab7f62c3838d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 399])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = output.argmax(-1)\n",
    "token_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be7af38f-753c-4e5e-9694-2dbe431d524a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\", � stainless need to explain递 concept of递 in programming. � воз starting sounds �'ve vaguely refers something something function refers itself recursively That wait � how does that work work? � me think about how simple example.\\n\\n Like something classic function? That, factorial sounds commonly classic example.\\n\\nнапример factorial factorial(n n number n is equal multiplied by factorial(n(nn minus UNS).Base递 sense recursive Starting someone递递递 factorial factorial factorial it would bestdbool3 multipliedstdboolstdbool factorial which vice2 factorial would(Arrays2 *11!, and finally1 factorial is just1 multiplied So递 recursively3 factorial would be 6 *永不2 * �1 = which is easy6. That works pretty.\\n\\nBut wait does recursion递 itselfBase base case? Oh right, somewhere base case is when n equals zero zero or something1 because whichever factorial factorial factorial is simply1 regardless Otherwise递递递递 step, we function递 going the problem until until it reaches the base case.\\n\\n � воз为什么会递 relies upon itself. yes,每一 step it function retr itself with a smaller version until That seems why essence idea � wait'm why does't problem be solved using recursion? Like've yes problems naturally naturally to solve recursively because like those involving naturally repetitive subproblems where Like sorting Fibonaccionacci relies where each Fibonacci depends on previous previous preceding it.\\n\\n Also wait might also inefficient too of stack stack depth limit too you递 too too large,递递 might stack recursion limits recursion depth limit crashes somewhere Also there programmers depends important to carefully appropriate right递 sizes递 sure递 enough Also �递递 needs to terminateBase base case else terminate递递.\\n\\n Also thing I note is递递 compared递 recursive step adds overhead layer stack stack stack stack somewhere which might be down out compared large inputs compared compared opting iterativegetting where brute brute grows't recursive递able into smaller subproblems, recursion might not be efficient best approach anymore Instead opting methods would better then those cases.\\n\\n Also also trying curious about递[email aspect programming languages languages.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86af1573-318b-4985-95df-aa2d5130a3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<｜Assistant｜> You are a helpful programming tutor.\\n<｜User｜> Explain the concept of recursion in programming.\\n<｜Assistant｜> \\nOkay, so I need to explain the concept of recursion in programming. Hmm, recursion... I remember it's when a function calls itself. But wait, how does that actually work? Let me think about a simple example. Maybe the factorial function? Yeah, that's a common example.\\n\\nSo, factorial of a number n is n multiplied by factorial of (n-1). That makes sense. If I try to compute 3!, it would be 3 * 2!, and 2! is 2 * 1!, and 1! is 1. So, 3! would be 3 * 2 * 1, which is 6. That seems straightforward.\\n\\nBut how does the function handle the base case? Oh right, the base case is when n is 0 or 1, because then the factorial is 1. So, in the recursive calls, the function keeps reducing the problem size until it reaches the base case.\\n\\nWait, so recursion builds on itself? Yeah, each time the function calls itself with a smaller input. That's the key. But I wonder, can every problem be solved with recursion? I think some problems are easier to solve recursively, especially those that have overlapping subproblems. Like the Fibonacci sequence, where each number depends on the two before it.\\n\\nBut recursion can be tricky because of the stack depth. If the input is very large, the function might hit the maximum recursion depth and crash. So, it's important to choose the right problem to make it efficient. Also, the function needs to have a base case to stop the recursion.\\n\\nAnother thing to consider is the efficiency. Each recursive call adds a frame to the call stack, which can slow things down for large inputs. So, for problems where the solution isn't easily decomposable into smaller subproblems, recursion might not be the best approach. Maybe iterative methods are better in those cases.\\n\\nI'm also thinking about the implementation in different programming languages.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_teacher_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da050c7a-e13a-4562-91f4-1ce917f50101",
   "metadata": {},
   "source": [
    "## Testing SFT student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4332e58c-40ec-4210-bdb4-4523551e827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting exit layers to inf for sft_student\n",
      "Minimum in prescribed_exit_layer_idxs = inf\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    batch, gen_len, elayers = early_exit_probs.shape \n",
    "    full_len = sft_teacher_generated_tokens.shape[1]\n",
    "    repeated_sft_teacher_generated_tokens = sft_teacher_generated_tokens.expand(num_exit_samples * batch, full_len)   \n",
    "    set_transformer_early_exit_mode(model, 'sft_student')\n",
    "    \n",
    "    # Create prescribed exit layer idxs filled with torch.inf (always exit on last layer)\n",
    "    batch_samples, seq_len = repeated_sft_teacher_generated_tokens.shape\n",
    "    print(\"Setting exit layers to inf for sft_student\")\n",
    "    prescribed_exit_layer_idxs = torch.full((batch_samples, gen_len), torch.inf, \\\n",
    "                                            device=repeated_sft_teacher_generated_tokens.device)\n",
    "    print(f\"Minimum in prescribed_exit_layer_idxs = {torch.min(prescribed_exit_layer_idxs)}\")\n",
    "    sft_student_output_scores, collected_exit_logits = model(repeated_sft_teacher_generated_tokens,\\\n",
    "                                                             prescribed_exit_layer_idxs=prescribed_exit_layer_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce8e989-53ef-4da3-9f64-1dee10178b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRUDE KL AND MAKE SURE PROBS ARE ALIGNED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(30.9266, device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('CRUDE KL AND MAKE SURE PROBS ARE ALIGNED')\n",
    "    eps = 1e-16\n",
    "    sft_teacher_probs = sft_teacher_final_layer_logprobs.softmax(-1)                        # [batch * samples, gen len, vocabulary]\n",
    "    sft_student_probs = sft_student_output_scores.logits[:,-gen_len:].softmax(-1)           # [batch * samples, gen len, vocabulary]\n",
    "    token_logits_kl_div = (sft_student_probs * ((sft_student_probs + eps) / (sft_teacher_probs + eps)).log()).sum(-1)   # [batch * samples, gen len]\n",
    "    \n",
    "    mean_logit_kl = token_logits_kl_div.mean()\n",
    "\n",
    "mean_logit_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4bbf043-2dcb-4814-8221-688d6a38ab92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex; flex-wrap: wrap;'><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 5</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>','</td>\n",
       "      <td>0.0861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>' it'</td>\n",
       "      <td>0.0620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>'.'</td>\n",
       "      <td>0.0554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>' the'</td>\n",
       "      <td>0.0331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>' as'</td>\n",
       "      <td>0.0303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 6</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>' the'</td>\n",
       "      <td>0.2598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>'.'</td>\n",
       "      <td>0.0589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>' it'</td>\n",
       "      <td>0.0363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1172</td>\n",
       "      <td>' only'</td>\n",
       "      <td>0.0192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>' in'</td>\n",
       "      <td>0.0184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 7</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>' the'</td>\n",
       "      <td>0.5732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>' it'</td>\n",
       "      <td>0.0502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1181</td>\n",
       "      <td>' its'</td>\n",
       "      <td>0.0422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>' in'</td>\n",
       "      <td>0.0328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>'.'</td>\n",
       "      <td>0.0181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 8</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>' the'</td>\n",
       "      <td>0.1273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>' in'</td>\n",
       "      <td>0.0399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>'.'</td>\n",
       "      <td>0.0385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>','</td>\n",
       "      <td>0.0289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>' it'</td>\n",
       "      <td>0.0196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 9</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>' the'</td>\n",
       "      <td>0.1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>' in'</td>\n",
       "      <td>0.0886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>' but'</td>\n",
       "      <td>0.0448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>' as'</td>\n",
       "      <td>0.0337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>' is'</td>\n",
       "      <td>0.0285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 10</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>','</td>\n",
       "      <td>0.3771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>'.'</td>\n",
       "      <td>0.0946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>')'</td>\n",
       "      <td>0.0153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>'o'</td>\n",
       "      <td>0.0135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>' a'</td>\n",
       "      <td>0.0094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "def topk_to_df(prob_dist, tokenizer=None, k=5, title=\"Top-K Predictions\"):\n",
    "    \"\"\"\n",
    "    Return top-k predictions and probabilities as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    top_values, top_indices = torch.topk(prob_dist, k=k)\n",
    "    \n",
    "    rows = []\n",
    "    for i, (idx, prob) in enumerate(zip(top_indices, top_values)):\n",
    "        token_id = idx.item()\n",
    "        prob_val = prob.item()\n",
    "        token_str = tokenizer.decode([token_id]) if tokenizer else str(token_id)\n",
    "        token_str = repr(token_str)  # Shows escape characters properly\n",
    "        \n",
    "        rows.append({\n",
    "            \"Token ID\": token_id,\n",
    "            \"Token String\": token_str,\n",
    "            \"Probability\": prob_val,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return title, df.round(4)\n",
    "\n",
    "# Example usage for your loop\n",
    "dfs = []\n",
    "for idx in range(5, 11):\n",
    "    title, df = topk_to_df(sft_student_probs[0, idx], tokenizer, k=5, title=f\"Student NTP for token {idx}\")\n",
    "    dfs.append((title, df))\n",
    "\n",
    "# Display in a grid\n",
    "html = \"<div style='display: flex; flex-wrap: wrap;'>\"\n",
    "for title, df in dfs:\n",
    "    html += \"<div style='flex: 1; min-width: 300px; padding: 10px;'>\"\n",
    "    html += f\"<h4>{title}</h4>\"\n",
    "    html += df.to_html(index=False)\n",
    "    html += \"</div>\"\n",
    "html += \"</div>\"\n",
    "\n",
    "display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707b44b-8e88-4f47-9e4b-bee64a51e393",
   "metadata": {},
   "source": [
    "### Very similar (and gibberish) next token predictions for all tokens. Something wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1bbeda-b47b-4d86-922e-f30d4e704cfb",
   "metadata": {},
   "source": [
    "## Testing free generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc7df80-150f-4302-8a42-0af37af5748f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing layer model.layers.0\n",
      "replacing layer model.layers.5\n",
      "replacing layer model.layers.10\n",
      "replacing layer model.layers.15\n",
      "replacing layer model.layers.20\n",
      "replacing layer model.layers.25\n",
      "address this hack!\n",
      "trainable params: 2,179,072 || all params: 1,779,276,294 || trainable%: 0.1225\n"
     ]
    }
   ],
   "source": [
    "# LOAD IN THE MODEL AND TOKENIZER\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "config = configs_from_yaml(model_config_path, tokenizer.eos_token_id)\n",
    "config['generation']['use_cache'] = False\n",
    "model = get_model(model_name, config['model'], device)\n",
    "\n",
    "\n",
    "# ENABLE EARLY EXITING\n",
    "model = replace_attention_layers(model, config['lora'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18123cab-604c-4c64-b3be-c31cb8457b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain the concept of recursion in programming.\"\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\"\n",
    "\n",
    "set_transformer_early_exit_mode(model, 'free_generate')\n",
    "\n",
    "with torch.no_grad():\n",
    "    free_generate_response, _ = generate_text(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        prefiller=prefiller,\n",
    "        tokenizer=tokenizer,\n",
    "        generation_config=config['generation'],\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d383fc51-a4fa-4fbb-9b2f-231aadedd453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<｜begin▁of▁sentence｜><｜Assistant｜> You are a helpful programming tutor.\\n<｜User｜> Explain the concept of recursion in programming.\\n<｜Assistant｜> \\nOkay, so I need to explain recursion in programming. Hmm, I remember recursion from my computer science class. It's when a function calls itself repeatedly until a base case is met. That makes sense, but I want to make sure I understand it thoroughly.\\n\\nLet me think about how it works. When you call a function recursively, it's like solving a problem by breaking it down into smaller sub-problems. Each time the function calls itself, it's handling a smaller part of the problem. The base case is crucial because it's the stopping point. Without a base case, the function would keep calling itself indefinitely, causing a stack overflow error.\\n\\nWait, how do I identify the base case? It's the simplest version of the problem that can be solved without further recursion. For example, if I have a function that calculates the factorial of a number, the base case would be when the number is 0 or 1 because 0! and 1! are both 1.\\n\\nLet me take an example. Suppose I have a function to compute the factorial of n. The recursive formula would be factorial(n) = n * factorial(n-1). The base case here is when n is 0 or 1, where the function returns 1.\\n\\nI also remember that each recursive call adds to the stack. So, for a function like factorial(3), it would call itself with 2, then with 1, and finally with 0. Each call adds a frame to the stack, and when it reaches the base case, the frames are popped off as the function unwinds.\\n\\nBut wait, isn't there a risk of stack overflow for very large inputs? Maybe that's something to consider, but it's a limitation of recursion, not the concept itself.\\n\\nAnother example: the Fibonacci sequence. The recursive formula is fib(n) = fib(n-1) + fib(n-2). The base cases are fib(0) = 0 and fib(\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_generate_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7704c214-b5e7-42ff-9cd9-87cb63f3987b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Current status: SFT teacher seems to work, free generation perhaps works, and student does not work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
