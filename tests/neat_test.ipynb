{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "661aa913-9bd0-4d06-8a16-631b6c239911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from shared_utils.data import CSVPromptDataset\n",
    "from shared_utils.load import get_model, get_tokenizer, configs_from_yaml\n",
    "from shared_utils.generate import generate_text\n",
    "\n",
    "from early_exit.patching import replace_attention_layers, set_transformer_early_exit_mode\n",
    "\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47cc87f3-6953-4ca3-8be0-cab2f1e7077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IN EXPERIMENT ARGS\n",
    "# num_epoch = 1                     # args.num_epoch\n",
    "num_exit_samples = 1                  # args.num_exit_samples\n",
    "device = \"cuda\"                    # args.device\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"                    # args.model_name\n",
    "model_config_path = \"../config_deepseek.yaml\"                     # args.model_config_path\n",
    "dataset_path = \"../results_and_data/early_exit_sft_dataset/test/data.csv\"                  # args.dataset_path\n",
    "prompt_config_path = \"../results_and_data/early_exit_sft_dataset/test/prompt_config.json\"                    # args.prompt_config_path\n",
    "batch_size = 1                    # args.batch_size -- might want to sort out batching, but increasing num_exit_samples might be better + less effort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847aa673-2067-4280-837b-97724f70ccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing layer model.layers.0\n",
      "replacing layer model.layers.5\n",
      "replacing layer model.layers.10\n",
      "replacing layer model.layers.15\n",
      "replacing layer model.layers.20\n",
      "replacing layer model.layers.25\n",
      "address this hack!\n",
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "trainable params: 2,179,072 || all params: 1,779,276,294 || trainable%: 0.1225\n"
     ]
    }
   ],
   "source": [
    "# LOAD IN THE MODEL AND TOKENIZER\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "config = configs_from_yaml(model_config_path, tokenizer.eos_token_id)\n",
    "model = get_model(model_name, config['model'], device)\n",
    "\n",
    "\n",
    "# LOAD IN DATASET\n",
    "dataset = CSVPromptDataset(dataset_path, prompt_config_path)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, shuffle=True)\n",
    "\n",
    "\n",
    "# ENABLE EARLY EXITING\n",
    "model = replace_attention_layers(model, config['lora'], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eaa7d5-8c03-4072-b20d-68f1d0721e08",
   "metadata": {},
   "source": [
    "## Testing SFT teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab955367-3761-492a-b199-c06751456441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 20])\n",
      "CRUDE KL\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain the concept of recursion in programming.\"\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\"\n",
    "\n",
    "set_transformer_early_exit_mode(model, 'sft_teacher')\n",
    "\n",
    "with torch.no_grad():\n",
    "    sft_teacher_response, (sft_teacher_generated_tokens, \n",
    "                          sft_teacher_final_layer_logprobs, \n",
    "                          gathered_early_exit_hidden_states) = generate_text(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        prefiller=prefiller,\n",
    "        tokenizer=tokenizer,\n",
    "        generation_config=config['generation'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    early_output_log_probs = model.early_exit_hidden_state_readout(gathered_early_exit_hidden_states)\n",
    "    \n",
    "    early_exit_probs = model.early_exit_target_probs(\n",
    "        early_output_log_probs=early_output_log_probs,\n",
    "        teacher_final_layer_log_probs=sft_teacher_final_layer_logprobs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86af1573-318b-4985-95df-aa2d5130a3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<｜begin▁of▁sentence｜><｜Assistant｜> You are a helpful programming tutor.\\n<｜User｜> Explain the concept of recursion in programming.\\n<｜Assistant｜> \\nAlright, so I need to explain recursion in programming. Hmm, recursion is a bit tricky because it's not something everyone is familiar with. Let me break it down step by step.\\n\\nFirst, I should start with the basics. I remember that recursion involves functions calling themselves. So, the same function is used in different parts of the code. That makes sense because it allows for solving complex problems by breaking them down into smaller, similar problems.\\n\\nWait, how does recursion work exactly? Like, if I have a function that does something and then calls itself with a modified version of the problem. So, each call to the function is a step towards the solution. That seems logical. Maybe I can use an example to make it clearer.\\n\\nLet me think of a simple example, like calculating the factorial of a number. The factorial of a number n is n multiplied by the factorial of (n-1), right? So, for example, 5! = 5 * 4 * 3 * 2 * 1. If I write a recursive function for this, it would call itself with n-1 until it reaches the base case, which is when n is 0 or 1, because 0! and 1! are both 1.\\n\\nBut wait, what's the base case? That's the stopping point for the recursion. Without it, the function would keep calling itself forever. So, in the factorial example, the base case would be when n is 0 or 1.\\n\\nI should also mention the advantages of recursion. It can make the code more readable and easier to understand because each step of the process is clear. Plus, it can be more concise and efficient in some cases, especially with divide-and-conquer algorithms.\\n\\nHowever, I need to be careful with recursion to avoid stack overflow errors. Each function call adds a frame to the call stack, and if it's too deep, the stack can get too large, causing the program to\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_teacher_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da050c7a-e13a-4562-91f4-1ce917f50101",
   "metadata": {},
   "source": [
    "## Testing SFT student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4332e58c-40ec-4210-bdb4-4523551e827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting exit layers to inf for sft_student\n",
      "Minimum in prescribed_exit_layer_idxs = inf\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    batch, gen_len, elayers = early_exit_probs.shape \n",
    "    full_len = sft_teacher_generated_tokens.shape[1]\n",
    "    repeated_sft_teacher_generated_tokens = sft_teacher_generated_tokens.expand(num_exit_samples * batch, full_len)   \n",
    "    set_transformer_early_exit_mode(model, 'sft_student')\n",
    "    \n",
    "    # Create prescribed exit layer idxs filled with torch.inf (always exit on last layer)\n",
    "    batch_samples, seq_len = repeated_sft_teacher_generated_tokens.shape\n",
    "    print(\"Setting exit layers to inf for sft_student\")\n",
    "    prescribed_exit_layer_idxs = torch.full((batch_samples, gen_len), torch.inf, \\\n",
    "                                            device=repeated_sft_teacher_generated_tokens.device)\n",
    "    print(f\"Minimum in prescribed_exit_layer_idxs = {torch.min(prescribed_exit_layer_idxs)}\")\n",
    "    sft_student_output_scores, collected_exit_logits = model(repeated_sft_teacher_generated_tokens,\\\n",
    "                                                             prescribed_exit_layer_idxs=prescribed_exit_layer_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce8e989-53ef-4da3-9f64-1dee10178b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRUDE KL AND MAKE SURE PROBS ARE ALIGNED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(30.9266, device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('CRUDE KL AND MAKE SURE PROBS ARE ALIGNED')\n",
    "    eps = 1e-16\n",
    "    sft_teacher_probs = sft_teacher_final_layer_logprobs.softmax(-1)                        # [batch * samples, gen len, vocabulary]\n",
    "    sft_student_probs = sft_student_output_scores.logits[:,-gen_len:].softmax(-1)           # [batch * samples, gen len, vocabulary]\n",
    "    token_logits_kl_div = (sft_student_probs * ((sft_student_probs + eps) / (sft_teacher_probs + eps)).log()).sum(-1)   # [batch * samples, gen len]\n",
    "    \n",
    "    mean_logit_kl = token_logits_kl_div.mean()\n",
    "\n",
    "mean_logit_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4bbf043-2dcb-4814-8221-688d6a38ab92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex; flex-wrap: wrap;'><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 5</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>','</td>\n",
       "      <td>0.0861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>' it'</td>\n",
       "      <td>0.0620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>'.'</td>\n",
       "      <td>0.0554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>' the'</td>\n",
       "      <td>0.0331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>' as'</td>\n",
       "      <td>0.0303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 6</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>' the'</td>\n",
       "      <td>0.2598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>'.'</td>\n",
       "      <td>0.0589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>' it'</td>\n",
       "      <td>0.0363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1172</td>\n",
       "      <td>' only'</td>\n",
       "      <td>0.0192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>' in'</td>\n",
       "      <td>0.0184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 7</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>' the'</td>\n",
       "      <td>0.5732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>' it'</td>\n",
       "      <td>0.0502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1181</td>\n",
       "      <td>' its'</td>\n",
       "      <td>0.0422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>' in'</td>\n",
       "      <td>0.0328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>'.'</td>\n",
       "      <td>0.0181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 8</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>' the'</td>\n",
       "      <td>0.1273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>' in'</td>\n",
       "      <td>0.0399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>'.'</td>\n",
       "      <td>0.0385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>','</td>\n",
       "      <td>0.0289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>' it'</td>\n",
       "      <td>0.0196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 9</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>' the'</td>\n",
       "      <td>0.1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>' in'</td>\n",
       "      <td>0.0886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>' but'</td>\n",
       "      <td>0.0448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>' as'</td>\n",
       "      <td>0.0337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>' is'</td>\n",
       "      <td>0.0285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 10</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>','</td>\n",
       "      <td>0.3771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>'.'</td>\n",
       "      <td>0.0946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>')'</td>\n",
       "      <td>0.0153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>'o'</td>\n",
       "      <td>0.0135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>' a'</td>\n",
       "      <td>0.0094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "def topk_to_df(prob_dist, tokenizer=None, k=5, title=\"Top-K Predictions\"):\n",
    "    \"\"\"\n",
    "    Return top-k predictions and probabilities as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    top_values, top_indices = torch.topk(prob_dist, k=k)\n",
    "    \n",
    "    rows = []\n",
    "    for i, (idx, prob) in enumerate(zip(top_indices, top_values)):\n",
    "        token_id = idx.item()\n",
    "        prob_val = prob.item()\n",
    "        token_str = tokenizer.decode([token_id]) if tokenizer else str(token_id)\n",
    "        token_str = repr(token_str)  # Shows escape characters properly\n",
    "        \n",
    "        rows.append({\n",
    "            \"Token ID\": token_id,\n",
    "            \"Token String\": token_str,\n",
    "            \"Probability\": prob_val,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return title, df.round(4)\n",
    "\n",
    "# Example usage for your loop\n",
    "dfs = []\n",
    "for idx in range(5, 11):\n",
    "    title, df = topk_to_df(sft_student_probs[0, idx], tokenizer, k=5, title=f\"Student NTP for token {idx}\")\n",
    "    dfs.append((title, df))\n",
    "\n",
    "# Display in a grid\n",
    "html = \"<div style='display: flex; flex-wrap: wrap;'>\"\n",
    "for title, df in dfs:\n",
    "    html += \"<div style='flex: 1; min-width: 300px; padding: 10px;'>\"\n",
    "    html += f\"<h4>{title}</h4>\"\n",
    "    html += df.to_html(index=False)\n",
    "    html += \"</div>\"\n",
    "html += \"</div>\"\n",
    "\n",
    "display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707b44b-8e88-4f47-9e4b-bee64a51e393",
   "metadata": {},
   "source": [
    "### Very similar (and gibberish) next token predictions for all tokens. Something wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1bbeda-b47b-4d86-922e-f30d4e704cfb",
   "metadata": {},
   "source": [
    "## Testing free generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc7df80-150f-4302-8a42-0af37af5748f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing layer model.layers.0\n",
      "replacing layer model.layers.5\n",
      "replacing layer model.layers.10\n",
      "replacing layer model.layers.15\n",
      "replacing layer model.layers.20\n",
      "replacing layer model.layers.25\n",
      "address this hack!\n",
      "trainable params: 2,179,072 || all params: 1,779,276,294 || trainable%: 0.1225\n"
     ]
    }
   ],
   "source": [
    "# LOAD IN THE MODEL AND TOKENIZER\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "config = configs_from_yaml(model_config_path, tokenizer.eos_token_id)\n",
    "config['generation']['use_cache'] = False\n",
    "model = get_model(model_name, config['model'], device)\n",
    "\n",
    "\n",
    "# ENABLE EARLY EXITING\n",
    "model = replace_attention_layers(model, config['lora'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18123cab-604c-4c64-b3be-c31cb8457b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain the concept of recursion in programming.\"\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\"\n",
    "\n",
    "set_transformer_early_exit_mode(model, 'free_generate')\n",
    "\n",
    "with torch.no_grad():\n",
    "    free_generate_response, _ = generate_text(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        prefiller=prefiller,\n",
    "        tokenizer=tokenizer,\n",
    "        generation_config=config['generation'],\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d383fc51-a4fa-4fbb-9b2f-231aadedd453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<｜begin▁of▁sentence｜><｜Assistant｜> You are a helpful programming tutor.\\n<｜User｜> Explain the concept of recursion in programming.\\n<｜Assistant｜> \\nOkay, so I need to explain recursion in programming. Hmm, I remember recursion from my computer science class. It's when a function calls itself repeatedly until a base case is met. That makes sense, but I want to make sure I understand it thoroughly.\\n\\nLet me think about how it works. When you call a function recursively, it's like solving a problem by breaking it down into smaller sub-problems. Each time the function calls itself, it's handling a smaller part of the problem. The base case is crucial because it's the stopping point. Without a base case, the function would keep calling itself indefinitely, causing a stack overflow error.\\n\\nWait, how do I identify the base case? It's the simplest version of the problem that can be solved without further recursion. For example, if I have a function that calculates the factorial of a number, the base case would be when the number is 0 or 1 because 0! and 1! are both 1.\\n\\nLet me take an example. Suppose I have a function to compute the factorial of n. The recursive formula would be factorial(n) = n * factorial(n-1). The base case here is when n is 0 or 1, where the function returns 1.\\n\\nI also remember that each recursive call adds to the stack. So, for a function like factorial(3), it would call itself with 2, then with 1, and finally with 0. Each call adds a frame to the stack, and when it reaches the base case, the frames are popped off as the function unwinds.\\n\\nBut wait, isn't there a risk of stack overflow for very large inputs? Maybe that's something to consider, but it's a limitation of recursion, not the concept itself.\\n\\nAnother example: the Fibonacci sequence. The recursive formula is fib(n) = fib(n-1) + fib(n-2). The base cases are fib(0) = 0 and fib(\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_generate_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7704c214-b5e7-42ff-9cd9-87cb63f3987b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Current status: SFT teacher seems to work, free generation perhaps works, and student does not work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
