{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "661aa913-9bd0-4d06-8a16-631b6c239911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from shared_utils.data import CSVPromptDataset\n",
    "from shared_utils.load import get_model, get_tokenizer, configs_from_yaml\n",
    "from shared_utils.generate import generate_text\n",
    "\n",
    "from early_exit.patching import replace_attention_layers, set_transformer_early_exit_mode\n",
    "\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47cc87f3-6953-4ca3-8be0-cab2f1e7077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IN EXPERIMENT ARGS\n",
    "# num_epoch = 1                     # args.num_epoch\n",
    "num_exit_samples = 1                  # args.num_exit_samples\n",
    "device = \"cuda\"                    # args.device\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"                    # args.model_name\n",
    "model_config_path = \"../config_deepseek.yaml\"                     # args.model_config_path\n",
    "dataset_path = \"../results_and_data/early_exit_sft_dataset/test/data.csv\"                  # args.dataset_path\n",
    "prompt_config_path = \"../results_and_data/early_exit_sft_dataset/test/prompt_config.json\"                    # args.prompt_config_path\n",
    "batch_size = 1                    # args.batch_size -- might want to sort out batching, but increasing num_exit_samples might be better + less effort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847aa673-2067-4280-837b-97724f70ccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing layer model.layers.0\n",
      "replacing layer model.layers.5\n",
      "replacing layer model.layers.10\n",
      "replacing layer model.layers.15\n",
      "replacing layer model.layers.20\n",
      "replacing layer model.layers.25\n",
      "address this hack!\n",
      "trainable params: 2,179,072 || all params: 1,779,276,294 || trainable%: 0.1225\n"
     ]
    }
   ],
   "source": [
    "# LOAD IN THE MODEL AND TOKENIZER\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "config = configs_from_yaml(model_config_path, tokenizer.eos_token_id)\n",
    "model = get_model(model_name, config['model'], device)\n",
    "\n",
    "\n",
    "# LOAD IN DATASET\n",
    "dataset = CSVPromptDataset(dataset_path, prompt_config_path)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, shuffle=True)\n",
    "\n",
    "\n",
    "# ENABLE EARLY EXITING\n",
    "model = replace_attention_layers(model, config['lora'], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eaa7d5-8c03-4072-b20d-68f1d0721e08",
   "metadata": {},
   "source": [
    "## Testing SFT teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f2de892-df66-4743-bec8-193822711dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully registered hook on: 'DynamicallyTypedLayerWithExit' at path 'base_model.model.model.layers.25'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ActivationCollector:\n",
    "    \"\"\"\n",
    "    A utility class to register a forward hook on a specific PyTorch module\n",
    "    and collect its output during the forward pass.\n",
    "    \n",
    "    Attributes:\n",
    "        activations (torch.Tensor | None): Stores the detached output tensor \n",
    "                                           from the hooked layer. It is moved\n",
    "                                           to the CPU to save GPU memory.\n",
    "        hook_handle (torch.utils.hooks.RemovableHandle | None): A handle to the\n",
    "                                                                registered hook,\n",
    "                                                                used for removal.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the collector.\"\"\"\n",
    "        self.activations = []\n",
    "        self.hook_handle = None\n",
    "\n",
    "    def _hook_fn(self, module, input_tensors, output_tensor):\n",
    "        \"\"\"\n",
    "        The actual hook function that PyTorch will call. It saves the output\n",
    "        of the layer.\n",
    "        \"\"\"\n",
    "        # We detach the tensor and move it to the CPU to avoid holding onto\n",
    "        # the computation graph and to free up GPU memory.\n",
    "        self.activations.append(output_tensor[0].detach().cpu())\n",
    "        \n",
    "\n",
    "    def register(self, model, layer_path: str):\n",
    "        \"\"\"\n",
    "        Registers the forward hook to a specific layer within the model.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): The model containing the target layer.\n",
    "            layer_path (str): A dot-separated string path to the target layer\n",
    "                              (e.g., 'base_model.model.model.norm').\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if registration was successful, False otherwise.\n",
    "        \"\"\"\n",
    "        # First, remove any existing hook managed by this instance.\n",
    "        self.remove()\n",
    "\n",
    "        # Navigate through the model hierarchy to find the target layer.\n",
    "        try:\n",
    "            target_layer = model\n",
    "            for part in layer_path.split('.'):\n",
    "                target_layer = getattr(target_layer, part)\n",
    "        except AttributeError:\n",
    "            print(f\"Error: Could not find the layer at path: {layer_path}\")\n",
    "            print(\"Please ensure the path matches the model architecture.\")\n",
    "            return False\n",
    "\n",
    "        # Register the forward hook on the found layer.\n",
    "        self.hook_handle = target_layer.register_forward_hook(self._hook_fn)\n",
    "        print(f\"Successfully registered hook on: '{type(target_layer).__name__}' at path '{layer_path}'\")\n",
    "        return True\n",
    "\n",
    "    def remove(self):\n",
    "        \"\"\"\n",
    "        Removes the registered hook if it exists. It's important to call this\n",
    "        when you're done to prevent memory leaks.\n",
    "        \"\"\"\n",
    "        if self.hook_handle:\n",
    "            self.hook_handle.remove()\n",
    "            self.hook_handle = None\n",
    "            print(\"Hook has been removed.\")\n",
    "\n",
    "\n",
    "\n",
    "collector = ActivationCollector()\n",
    "layer_path_to_hook = 'base_model.model.model.layers.25'\n",
    "\n",
    "#layer_path_to_hook = 'base_model.model.model.norm'\n",
    "collector.register(model, layer_path_to_hook)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "727252d0-8fac-4932-8cc5-ff678f5ab31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook has been removed.\n"
     ]
    }
   ],
   "source": [
    "collector.remove() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52a7f64b-e0ad-4b24-8cd6-24219e661415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.base_model.model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab955367-3761-492a-b199-c06751456441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain the concept of recursion in programming.\"\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\"\n",
    "\n",
    "set_transformer_early_exit_mode(model, 'sft_teacher')\n",
    "\n",
    "with torch.no_grad():\n",
    "    sft_teacher_response, (sft_teacher_generated_tokens, \n",
    "                          sft_teacher_final_layer_logprobs, \n",
    "                          gathered_early_exit_hidden_states) = generate_text(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        prefiller=prefiller,\n",
    "        tokenizer=tokenizer,\n",
    "        generation_config=config['generation'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "   # early_output_log_probs = model.early_exit_hidden_state_readout(gathered_early_exit_hidden_states)\n",
    "    \n",
    "   # early_exit_probs = model.early_exit_target_probs(\n",
    "   #     early_output_log_probs=early_output_log_probs,\n",
    "   #     teacher_final_layer_log_probs=sft_teacher_final_layer_logprobs\n",
    "   # )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da050c7a-e13a-4562-91f4-1ce917f50101",
   "metadata": {},
   "source": [
    "## Testing SFT student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4332e58c-40ec-4210-bdb4-4523551e827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting exit layers to inf for sft_student\n",
      "Minimum in prescribed_exit_layer_idxs = inf\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    batch, gen_len, elayers = early_exit_probs.shape \n",
    "    full_len = sft_teacher_generated_tokens.shape[1]\n",
    "    repeated_sft_teacher_generated_tokens = sft_teacher_generated_tokens.expand(num_exit_samples * batch, full_len)   \n",
    "    set_transformer_early_exit_mode(model, 'sft_student')\n",
    "    \n",
    "    # Create prescribed exit layer idxs filled with torch.inf (always exit on last layer)\n",
    "    batch_samples, seq_len = repeated_sft_teacher_generated_tokens.shape\n",
    "    print(\"Setting exit layers to inf for sft_student\")\n",
    "    prescribed_exit_layer_idxs = torch.full((batch_samples, gen_len), torch.inf, \\\n",
    "                                            device=repeated_sft_teacher_generated_tokens.device)\n",
    "    print(f\"Minimum in prescribed_exit_layer_idxs = {torch.min(prescribed_exit_layer_idxs)}\")\n",
    "    sft_student_output_scores, collected_exit_logits = model(repeated_sft_teacher_generated_tokens,\\\n",
    "                                                             prescribed_exit_layer_idxs=prescribed_exit_layer_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce8e989-53ef-4da3-9f64-1dee10178b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRUDE KL AND MAKE SURE PROBS ARE ALIGNED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(30.9266, device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('CRUDE KL AND MAKE SURE PROBS ARE ALIGNED')\n",
    "    eps = 1e-16\n",
    "    sft_teacher_probs = sft_teacher_final_layer_logprobs.softmax(-1)                        # [batch * samples, gen len, vocabulary]\n",
    "    sft_student_probs = sft_student_output_scores.logits[:,-gen_len:].softmax(-1)           # [batch * samples, gen len, vocabulary]\n",
    "    token_logits_kl_div = (sft_student_probs * ((sft_student_probs + eps) / (sft_teacher_probs + eps)).log()).sum(-1)   # [batch * samples, gen len]\n",
    "    \n",
    "    mean_logit_kl = token_logits_kl_div.mean()\n",
    "\n",
    "mean_logit_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4bbf043-2dcb-4814-8221-688d6a38ab92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex; flex-wrap: wrap;'><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 5</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>','</td>\n",
       "      <td>0.0861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>' it'</td>\n",
       "      <td>0.0620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>'.'</td>\n",
       "      <td>0.0554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>' the'</td>\n",
       "      <td>0.0331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>' as'</td>\n",
       "      <td>0.0303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 6</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>' the'</td>\n",
       "      <td>0.2598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>'.'</td>\n",
       "      <td>0.0589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>' it'</td>\n",
       "      <td>0.0363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1172</td>\n",
       "      <td>' only'</td>\n",
       "      <td>0.0192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>' in'</td>\n",
       "      <td>0.0184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 7</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>' the'</td>\n",
       "      <td>0.5732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>' it'</td>\n",
       "      <td>0.0502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1181</td>\n",
       "      <td>' its'</td>\n",
       "      <td>0.0422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>' in'</td>\n",
       "      <td>0.0328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>'.'</td>\n",
       "      <td>0.0181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 8</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>' the'</td>\n",
       "      <td>0.1273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>' in'</td>\n",
       "      <td>0.0399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>'.'</td>\n",
       "      <td>0.0385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>','</td>\n",
       "      <td>0.0289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>' it'</td>\n",
       "      <td>0.0196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 9</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>' the'</td>\n",
       "      <td>0.1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>' in'</td>\n",
       "      <td>0.0886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>' but'</td>\n",
       "      <td>0.0448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>' as'</td>\n",
       "      <td>0.0337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>' is'</td>\n",
       "      <td>0.0285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style='flex: 1; min-width: 300px; padding: 10px;'><h4>Student NTP for token 10</h4><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token ID</th>\n",
       "      <th>Token String</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>','</td>\n",
       "      <td>0.3771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>'.'</td>\n",
       "      <td>0.0946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>')'</td>\n",
       "      <td>0.0153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>'o'</td>\n",
       "      <td>0.0135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>' a'</td>\n",
       "      <td>0.0094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "def topk_to_df(prob_dist, tokenizer=None, k=5, title=\"Top-K Predictions\"):\n",
    "    \"\"\"\n",
    "    Return top-k predictions and probabilities as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    top_values, top_indices = torch.topk(prob_dist, k=k)\n",
    "    \n",
    "    rows = []\n",
    "    for i, (idx, prob) in enumerate(zip(top_indices, top_values)):\n",
    "        token_id = idx.item()\n",
    "        prob_val = prob.item()\n",
    "        token_str = tokenizer.decode([token_id]) if tokenizer else str(token_id)\n",
    "        token_str = repr(token_str)  # Shows escape characters properly\n",
    "        \n",
    "        rows.append({\n",
    "            \"Token ID\": token_id,\n",
    "            \"Token String\": token_str,\n",
    "            \"Probability\": prob_val,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return title, df.round(4)\n",
    "\n",
    "# Example usage for your loop\n",
    "dfs = []\n",
    "for idx in range(5, 11):\n",
    "    title, df = topk_to_df(sft_student_probs[0, idx], tokenizer, k=5, title=f\"Student NTP for token {idx}\")\n",
    "    dfs.append((title, df))\n",
    "\n",
    "# Display in a grid\n",
    "html = \"<div style='display: flex; flex-wrap: wrap;'>\"\n",
    "for title, df in dfs:\n",
    "    html += \"<div style='flex: 1; min-width: 300px; padding: 10px;'>\"\n",
    "    html += f\"<h4>{title}</h4>\"\n",
    "    html += df.to_html(index=False)\n",
    "    html += \"</div>\"\n",
    "html += \"</div>\"\n",
    "\n",
    "display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707b44b-8e88-4f47-9e4b-bee64a51e393",
   "metadata": {},
   "source": [
    "### Very similar (and gibberish) next token predictions for all tokens. Something wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1bbeda-b47b-4d86-922e-f30d4e704cfb",
   "metadata": {},
   "source": [
    "## Testing free generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc7df80-150f-4302-8a42-0af37af5748f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing layer model.layers.0\n",
      "replacing layer model.layers.5\n",
      "replacing layer model.layers.10\n",
      "replacing layer model.layers.15\n",
      "replacing layer model.layers.20\n",
      "replacing layer model.layers.25\n",
      "address this hack!\n",
      "trainable params: 2,179,072 || all params: 1,779,276,294 || trainable%: 0.1225\n"
     ]
    }
   ],
   "source": [
    "# LOAD IN THE MODEL AND TOKENIZER\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "config = configs_from_yaml(model_config_path, tokenizer.eos_token_id)\n",
    "config['generation']['use_cache'] = False\n",
    "model = get_model(model_name, config['model'], device)\n",
    "\n",
    "\n",
    "# ENABLE EARLY EXITING\n",
    "model = replace_attention_layers(model, config['lora'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18123cab-604c-4c64-b3be-c31cb8457b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain the concept of recursion in programming.\"\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\"\n",
    "\n",
    "set_transformer_early_exit_mode(model, 'free_generate')\n",
    "\n",
    "with torch.no_grad():\n",
    "    free_generate_response, _ = generate_text(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        prefiller=prefiller,\n",
    "        tokenizer=tokenizer,\n",
    "        generation_config=config['generation'],\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d383fc51-a4fa-4fbb-9b2f-231aadedd453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<｜begin▁of▁sentence｜><｜Assistant｜> You are a helpful programming tutor.\\n<｜User｜> Explain the concept of recursion in programming.\\n<｜Assistant｜> \\nOkay, so I need to explain recursion in programming. Hmm, I remember recursion from my computer science class. It's when a function calls itself repeatedly until a base case is met. That makes sense, but I want to make sure I understand it thoroughly.\\n\\nLet me think about how it works. When you call a function recursively, it's like solving a problem by breaking it down into smaller sub-problems. Each time the function calls itself, it's handling a smaller part of the problem. The base case is crucial because it's the stopping point. Without a base case, the function would keep calling itself indefinitely, causing a stack overflow error.\\n\\nWait, how do I identify the base case? It's the simplest version of the problem that can be solved without further recursion. For example, if I have a function that calculates the factorial of a number, the base case would be when the number is 0 or 1 because 0! and 1! are both 1.\\n\\nLet me take an example. Suppose I have a function to compute the factorial of n. The recursive formula would be factorial(n) = n * factorial(n-1). The base case here is when n is 0 or 1, where the function returns 1.\\n\\nI also remember that each recursive call adds to the stack. So, for a function like factorial(3), it would call itself with 2, then with 1, and finally with 0. Each call adds a frame to the stack, and when it reaches the base case, the frames are popped off as the function unwinds.\\n\\nBut wait, isn't there a risk of stack overflow for very large inputs? Maybe that's something to consider, but it's a limitation of recursion, not the concept itself.\\n\\nAnother example: the Fibonacci sequence. The recursive formula is fib(n) = fib(n-1) + fib(n-2). The base cases are fib(0) = 0 and fib(\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_generate_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7704c214-b5e7-42ff-9cd9-87cb63f3987b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Current status: SFT teacher seems to work, free generation perhaps works, and student does not work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
