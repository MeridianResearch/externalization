{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "269cd867-509c-40c3-8a84-10ed7b740388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Device: cuda\n",
      "Tokenizer loaded. Vocab size: 151643\n",
      "EOS token: <｜end▁of▁sentence｜> (ID: 151643)\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Early exit layer indices: tensor([ 0,  5, 10, 15, 20, 25], dtype=torch.int32)\n",
      "Total exitable layers: 6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from shared_utils.generate import format_conversation, transform_conversations\n",
    "from early_exit.util import module_name_is_layer_base\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store data\n",
    "token_data = []\n",
    "\n",
    "from shared_utils.load import get_model, get_tokenizer, configs_from_yaml\n",
    "import random\n",
    "# Model configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # torch_dtype=torch.float16,  # Use half precision for efficiency\n",
    "    device_map=\"auto\" if device == 'cuda' else None,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "prompt = \"Explain the concept of recursion in programming.\"\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\"\n",
    "\n",
    "pre_transformed_conversation = format_conversation(user_prompts = [prompt], system_prompt=system_prompt)\n",
    "formatted_prompt = transform_conversations(pre_transformed_conversation, prefiller)[0]\n",
    "from early_exit.util import module_name_is_layer_base\n",
    "early_exit_layer_idxs = []\n",
    "for name, module in model.named_modules():\n",
    "    if module_name_is_layer_base(name):\n",
    "        # Extract layer index from module name (e.g., \"model.layers.0\" -> 0)\n",
    "        layer_idx = int(name.split('.')[-1])\n",
    "        early_exit_layer_idxs.append(layer_idx)\n",
    "\n",
    "early_exit_layer_idxs = torch.tensor(early_exit_layer_idxs, dtype = torch.int32)  # Add inf for final layer\n",
    "print(f\"Early exit layer indices: {early_exit_layer_idxs}\")\n",
    "print(f\"Total exitable layers: {len(early_exit_layer_idxs)}\")  # Subtract 1 for the inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d285d60b-8505-488f-8e2c-4e750a0ce6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     step      token                     exit_layer  prob_exit_early  \\\n",
      "0       0  Certainly                             -1         0.000052   \n",
      "1       1          ,                             -1         0.217027   \n",
      "2       2        the                             -1         0.000034   \n",
      "3       3       need                             -1         0.029931   \n",
      "4       4         to  tensor(25, dtype=torch.int32)         0.999718   \n",
      "..    ...        ...                            ...              ...   \n",
      "295    95       runs                             -1         0.000099   \n",
      "296    96          .                             -1         0.715062   \n",
      "297    97      Using                             -1         0.000036   \n",
      "298    98     ursion                             -1         0.000409   \n",
      "299    99      helps                             -1         0.000444   \n",
      "\n",
      "     kl_layer_25 difficulty did_exit_early  \n",
      "0      18.426769       hard            NaN  \n",
      "1       2.106080       hard            NaN  \n",
      "2      13.172898       hard            NaN  \n",
      "3       5.351672       hard            NaN  \n",
      "4       0.000564       easy            NaN  \n",
      "..           ...        ...            ...  \n",
      "295    32.391983       hard          False  \n",
      "296     1.368949       hard          False  \n",
      "297    27.435762       hard          False  \n",
      "298    21.333799       hard          False  \n",
      "299    15.922745       hard          False  \n",
      "\n",
      "[300 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "model_config_path = \"../config_deepseek.yaml\"                     # args.model_config_path\n",
    "\n",
    "config = configs_from_yaml(model_config_path, tokenizer.eos_token_id)\n",
    "config['generation']['max_new_tokens'] = 100\n",
    "\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs.input_ids\n",
    "prompt_length = input_ids.shape[1]\n",
    "\n",
    "KL_FACTOR = 1\n",
    "current_input = input_ids.clone()\n",
    "generated_tokens_manual = []\n",
    "chosen_exit_layers = []\n",
    "\n",
    "\n",
    "for step in range(config['generation']['max_new_tokens']):\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(current_input, use_cache=True, output_hidden_states=True)\n",
    "        # print(outputs.logits.shape)\n",
    "        logits = outputs.logits[:, -1, :]  # Get logits for last token\n",
    "        hidden_states = torch.stack(outputs.hidden_states)\n",
    "        exit_hidden_states = hidden_states[early_exit_layer_idxs, :, -1, :].transpose(0,1)\n",
    "        exit_predictions = model.lm_head(exit_hidden_states)\n",
    "        # 1. Get KL divergence between early exit and final layers\n",
    "        final_predictions = torch.softmax(logits, dim=-1)\n",
    "        teacher_expanded = final_predictions.unsqueeze(1)  \n",
    "        early_output_probs = torch.softmax(exit_predictions, dim=-1)\n",
    "        # Sum over vocab -> [batch, exitable layers, sequence]\n",
    "        # print(teacher_expanded.shape, early_output_probs.shape)\n",
    "        eps = 1e-16\n",
    "        # kl_div = (teacher_expanded * ((teacher_expanded + eps) / (early_output_probs + eps)).log()).sum(-1)\n",
    "        kl_div = - (teacher_expanded * (early_output_probs + eps).log()).sum(-1)\n",
    "        \n",
    "        # 2. Scale KL divergencees by KL_FACTOR and pass through sigmoid (0-1)\n",
    "        sigmoid_kls = torch.sigmoid(KL_FACTOR * kl_div)  # [batch, exitable layers, sequence]\n",
    "        sigmoid_kls = 2.0 * sigmoid_kls - 1.0\n",
    "        sigmoid_kls = 1.0 - sigmoid_kls\n",
    "        \n",
    "        predictions = final_predictions\n",
    "        chosen_exit_layer = -1\n",
    "        for qdx, exit_layer in enumerate(early_exit_layer_idxs):\n",
    "            rand_val = random.random()\n",
    "            if rand_val < sigmoid_kls[0, qdx]:\n",
    "                predictions = early_output_probs[:, qdx]\n",
    "                chosen_exit_layer = exit_layer\n",
    "                break\n",
    "        chosen_exit_layers.append(int(chosen_exit_layer))\n",
    "        \n",
    "        # Sample next token\n",
    "        next_token = torch.multinomial(predictions, 1)\n",
    "        \n",
    "        # Decode token BEFORE printing KL stats\n",
    "        token_text = tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Calculate overall probability of exiting early\n",
    "        prob_reach_final = 1.0\n",
    "        for qdx in range(len(early_exit_layer_idxs)):\n",
    "            prob_reach_final *= (1 - sigmoid_kls[0, qdx].item())\n",
    "        prob_exit_early = 1.0 - prob_reach_final\n",
    "        token_text = tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
    "    \n",
    "        # Calculate overall probability of exiting early\n",
    "        prob_reach_final = 1.0\n",
    "        for qdx in range(len(early_exit_layer_idxs)):\n",
    "            prob_reach_final *= (1 - sigmoid_kls[0, qdx].item())\n",
    "        prob_exit_early = 1.0 - prob_reach_final\n",
    "        \n",
    "        # Store data\n",
    "        token_data.append({\n",
    "            'step': step,\n",
    "            'token': token_text,\n",
    "            'exit_layer': chosen_exit_layer,  # -1 means final layer, otherwise the actual layer number\n",
    "            'did_exit_early': chosen_exit_layer != -1,\n",
    "            'prob_exit_early': prob_exit_early,\n",
    "            'kl_layer_25': kl_div[0, -1].item(),\n",
    "            'difficulty': 'easy' if chosen_exit_layer >= 20 else 'hard' if chosen_exit_layer == -1 else 'medium'\n",
    "        })\n",
    "        for idx, layer_num in enumerate(early_exit_layer_idxs):\n",
    "            predictions = final_predictions\n",
    "            chosen_exit_layer = -1\n",
    "            for qdx, exit_layer in enumerate(early_exit_layer_idxs):\n",
    "                rand_val = random.random()\n",
    "                if rand_val < sigmoid_kls[0, qdx]:\n",
    "                    predictions = early_output_probs[:, qdx]\n",
    "                    chosen_exit_layer = exit_layer\n",
    "                    break\n",
    "            chosen_exit_layers.append(int(chosen_exit_layer))\n",
    "            # Sample next token\n",
    "            next_token = torch.multinomial(predictions, 1)\n",
    "            \n",
    "        # Check for EOS\n",
    "        if next_token.item() == config['generation']['eos_token_id']:\n",
    "            print(f\"EOS token encountered at step {step}\")\n",
    "            break\n",
    "            \n",
    "        # Add token to sequence\n",
    "        current_input = torch.cat([current_input, next_token], dim=1)\n",
    "        generated_tokens_manual.append(next_token.item())\n",
    "        \n",
    "        # Decode and print current token\n",
    "        token_text = tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
    "        # print(f\"Step {step}: Token {next_token.item()} -> '{token_text}'\")\n",
    "df = pd.DataFrame(token_data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6325e734-3d0e-45d3-9493-ab61f1edc574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>token</th>\n",
       "      <th>exit_layer</th>\n",
       "      <th>prob_exit_early</th>\n",
       "      <th>kl_layer_25</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>did_exit_early</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Certainly</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>18.426769</td>\n",
       "      <td>hard</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.217027</td>\n",
       "      <td>2.106080</td>\n",
       "      <td>hard</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>13.172898</td>\n",
       "      <td>hard</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>need</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.029931</td>\n",
       "      <td>5.351672</td>\n",
       "      <td>hard</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>to</td>\n",
       "      <td>tensor(25, dtype=torch.int32)</td>\n",
       "      <td>0.999718</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>easy</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step      token                     exit_layer  prob_exit_early  \\\n",
       "0     0  Certainly                             -1         0.000052   \n",
       "1     1          ,                             -1         0.217027   \n",
       "2     2        the                             -1         0.000034   \n",
       "3     3       need                             -1         0.029931   \n",
       "4     4         to  tensor(25, dtype=torch.int32)         0.999718   \n",
       "\n",
       "   kl_layer_25 difficulty did_exit_early  \n",
       "0    18.426769       hard            NaN  \n",
       "1     2.106080       hard            NaN  \n",
       "2    13.172898       hard            NaN  \n",
       "3     5.351672       hard            NaN  \n",
       "4     0.000564       easy            NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dec536-75d5-4271-860c-c1ff9e6f2b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
