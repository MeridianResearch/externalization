{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "269cd867-509c-40c3-8a84-10ed7b740388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Device: cuda\n",
      "Early exit layer indices: tensor([ 0,  5, 10, 15, 20, 25], dtype=torch.int32)\n",
      "\n",
      "Processing prompt 1/30: Explain why the Monty Hall problem solution is cou...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 2/30: What would happen if gravity suddenly became twice...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 3/30: Design an algorithm to detect cycles in a linked l...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 4/30: Why does hot water sometimes freeze faster than co...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 5/30: Explain the grandfather paradox in time travel....\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 6/30: How would you implement a LRU cache with O(1) oper...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 7/30: What are the implications of Gödel's incompletenes...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 8/30: Derive the formula for the area of a circle from f...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 9/30: Explain why correlation does not imply causation w...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 10/30: How does quantum entanglement challenge classical ...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 11/30: Design a distributed system for real-time collabor...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 12/30: What is the halting problem and why is it undecida...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 13/30: Explain the prisoner's dilemma and its real-world ...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 14/30: How would you detect if a binary tree is balanced?...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 15/30: What causes the Dunning-Kruger effect psychologica...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 16/30: Derive Bayes' theorem and explain its significance...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 17/30: How does TCP ensure reliable data transmission?...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 18/30: Explain the concept of emergence in complex system...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 19/30: What is the traveling salesman problem and why is ...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 20/30: How does gradient descent find local minima in neu...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 21/30: Explain the twin paradox in special relativity....\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 22/30: Design a hash table that handles collisions effici...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 23/30: What is Russell's paradox and how does it affect s...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 24/30: How would you implement mutex locks in an operatin...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 25/30: Explain the concept of computational complexity wi...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 26/30: What is the Chinese room argument about artificial...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 29 tokens\n",
      "\n",
      "Processing prompt 27/30: How does dynamic programming differ from divide an...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 28/30: Explain why P vs NP is such an important problem....\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 29/30: What are the philosophical implications of the shi...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Processing prompt 30/30: How would you design a garbage collector for a pro...\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Generated 100 tokens\n",
      "\n",
      "Total tokens generated: 2929\n",
      "Average tokens per prompt: 97.6\n",
      "\n",
      "Data saved to: early_exit_analysis_30_prompts.csv\n",
      "\n",
      "Exit layer distribution:\n",
      "exit_layer\n",
      "-1     2285\n",
      " 5       11\n",
      " 10      29\n",
      " 15      52\n",
      " 20     142\n",
      " 25     410\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Early exit rate: 21.99%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from shared_utils.generate import format_conversation, transform_conversations\n",
    "from early_exit.util import module_name_is_layer_base\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shared_utils.load import get_model, get_tokenizer, configs_from_yaml\n",
    "import random\n",
    "\n",
    "# Generate 30 thinking-intensive prompts\n",
    "thinking_prompts = [\n",
    "    \"Explain why the Monty Hall problem solution is counterintuitive.\",\n",
    "    \"What would happen if gravity suddenly became twice as strong?\",\n",
    "    \"Design an algorithm to detect cycles in a linked list.\",\n",
    "    \"Why does hot water sometimes freeze faster than cold water?\",\n",
    "    \"Explain the grandfather paradox in time travel.\",\n",
    "    \"How would you implement a LRU cache with O(1) operations?\",\n",
    "    \"What are the implications of Gödel's incompleteness theorems?\",\n",
    "    \"Derive the formula for the area of a circle from first principles.\",\n",
    "    \"Explain why correlation does not imply causation with examples.\",\n",
    "    \"How does quantum entanglement challenge classical physics?\",\n",
    "    \"Design a distributed system for real-time collaborative editing.\",\n",
    "    \"What is the halting problem and why is it undecidable?\",\n",
    "    \"Explain the prisoner's dilemma and its real-world applications.\",\n",
    "    \"How would you detect if a binary tree is balanced?\",\n",
    "    \"What causes the Dunning-Kruger effect psychologically?\",\n",
    "    \"Derive Bayes' theorem and explain its significance.\",\n",
    "    \"How does TCP ensure reliable data transmission?\",\n",
    "    \"Explain the concept of emergence in complex systems.\",\n",
    "    \"What is the traveling salesman problem and why is it NP-hard?\",\n",
    "    \"How does gradient descent find local minima in neural networks?\",\n",
    "    \"Explain the twin paradox in special relativity.\",\n",
    "    \"Design a hash table that handles collisions efficiently.\",\n",
    "    \"What is Russell's paradox and how does it affect set theory?\",\n",
    "    \"How would you implement mutex locks in an operating system?\",\n",
    "    \"Explain the concept of computational complexity with examples.\",\n",
    "    \"What is the Chinese room argument about artificial intelligence?\",\n",
    "    \"How does dynamic programming differ from divide and conquer?\",\n",
    "    \"Explain why P vs NP is such an important problem.\",\n",
    "    \"What are the philosophical implications of the ship of Theseus?\",\n",
    "    \"How would you design a garbage collector for a programming language?\"\n",
    "]\n",
    "\n",
    "# Model configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\" if device == 'cuda' else None,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Get early exit layer indices\n",
    "early_exit_layer_idxs = []\n",
    "for name, module in model.named_modules():\n",
    "    if module_name_is_layer_base(name):\n",
    "        layer_idx = int(name.split('.')[-1])\n",
    "        early_exit_layer_idxs.append(layer_idx)\n",
    "early_exit_layer_idxs = torch.tensor(early_exit_layer_idxs, dtype=torch.int32)\n",
    "print(f\"Early exit layer indices: {early_exit_layer_idxs}\")\n",
    "\n",
    "# Configuration\n",
    "model_config_path = \"../config_deepseek.yaml\"\n",
    "config = configs_from_yaml(model_config_path, tokenizer.eos_token_id)\n",
    "config['generation']['max_new_tokens'] = 100\n",
    "KL_FACTOR = 1\n",
    "\n",
    "# System prompt\n",
    "system_prompt = \"You are a helpful assistant that thinks step by step.\"\n",
    "\n",
    "# Collect all data\n",
    "all_token_data = []\n",
    "\n",
    "# Process each prompt\n",
    "for prompt_idx, prompt in enumerate(thinking_prompts):\n",
    "    print(f\"\\nProcessing prompt {prompt_idx + 1}/30: {prompt[:50]}...\")\n",
    "    \n",
    "    # Format prompt\n",
    "    pre_transformed_conversation = format_conversation(user_prompts=[prompt], system_prompt=system_prompt)\n",
    "    formatted_prompt = transform_conversations(pre_transformed_conversation, \"\")[0]\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs.input_ids\n",
    "    prompt_length = input_ids.shape[1]\n",
    "    \n",
    "    # Generation variables\n",
    "    current_input = input_ids.clone()\n",
    "    generated_tokens_manual = []\n",
    "    chosen_exit_layers = []\n",
    "    \n",
    "    # Generate tokens\n",
    "    for step in range(config['generation']['max_new_tokens']):\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(current_input, use_cache=True, output_hidden_states=True)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            hidden_states = torch.stack(outputs.hidden_states)\n",
    "            exit_hidden_states = hidden_states[early_exit_layer_idxs, :, -1, :].transpose(0,1)\n",
    "            exit_predictions = model.lm_head(exit_hidden_states)\n",
    "            \n",
    "            # Get KL divergence\n",
    "            final_predictions = torch.softmax(logits, dim=-1)\n",
    "            teacher_expanded = final_predictions.unsqueeze(1)\n",
    "            early_output_probs = torch.softmax(exit_predictions, dim=-1)\n",
    "            \n",
    "            eps = 1e-16\n",
    "            kl_div = - (teacher_expanded * (early_output_probs + eps).log()).sum(-1)\n",
    "            \n",
    "            # Scale KL divergences\n",
    "            sigmoid_kls = torch.sigmoid(KL_FACTOR * kl_div)\n",
    "            sigmoid_kls = 2.0 * sigmoid_kls - 1.0\n",
    "            sigmoid_kls = 1.0 - sigmoid_kls\n",
    "            \n",
    "            # Choose exit layer\n",
    "            predictions = final_predictions\n",
    "            chosen_exit_layer = -1\n",
    "            for qdx, exit_layer in enumerate(early_exit_layer_idxs):\n",
    "                rand_val = random.random()\n",
    "                if rand_val < sigmoid_kls[0, qdx]:\n",
    "                    predictions = early_output_probs[:, qdx]\n",
    "                    chosen_exit_layer = int(exit_layer.item())\n",
    "                    break\n",
    "            \n",
    "            chosen_exit_layers.append(chosen_exit_layer)\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token = torch.multinomial(predictions, 1)\n",
    "            \n",
    "            # Decode token\n",
    "            token_text = tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Calculate probability of exiting early\n",
    "            prob_reach_final = 1.0\n",
    "            for qdx in range(len(early_exit_layer_idxs)):\n",
    "                prob_reach_final *= (1 - sigmoid_kls[0, qdx].item())\n",
    "            prob_exit_early = 1.0 - prob_reach_final\n",
    "            \n",
    "            # Store data\n",
    "            token_data_entry = {\n",
    "                'prompt_idx': prompt_idx,\n",
    "                'prompt': prompt[:50] + '...',  # Truncate for readability\n",
    "                'step': step,\n",
    "                'token': token_text,\n",
    "                'exit_layer': chosen_exit_layer,\n",
    "                'did_exit_early': chosen_exit_layer != -1,\n",
    "                'prob_exit_early': prob_exit_early,\n",
    "            }\n",
    "            \n",
    "            # Add KL divergence for each layer\n",
    "            for idx, layer_num in enumerate(early_exit_layer_idxs):\n",
    "                token_data_entry[f'kl_layer_{int(layer_num)}'] = float(kl_div[0, idx].item())\n",
    "            \n",
    "            all_token_data.append(token_data_entry)\n",
    "            \n",
    "            # Check for EOS\n",
    "            if next_token.item() == config['generation']['eos_token_id']:\n",
    "                break\n",
    "            \n",
    "            # Add token to sequence\n",
    "            current_input = torch.cat([current_input, next_token], dim=1)\n",
    "            generated_tokens_manual.append(next_token.item())\n",
    "    \n",
    "    print(f\"Generated {step + 1} tokens\")\n",
    "\n",
    "# Create DataFrame and save\n",
    "df = pd.DataFrame(all_token_data)\n",
    "df.to_csv('early_exit_analysis_30_prompts.csv', index=False)\n",
    "\n",
    "print(f\"\\nTotal tokens generated: {len(df)}\")\n",
    "print(f\"Average tokens per prompt: {len(df) / len(thinking_prompts):.1f}\")\n",
    "print(\"\\nData saved to: early_exit_analysis_30_prompts.csv\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(\"\\nExit layer distribution:\")\n",
    "print(df['exit_layer'].value_counts().sort_index())\n",
    "print(f\"\\nEarly exit rate: {df['did_exit_early'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d285d60b-8505-488f-8e2c-4e750a0ce6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6325e734-3d0e-45d3-9493-ab61f1edc574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dec536-75d5-4271-860c-c1ff9e6f2b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
