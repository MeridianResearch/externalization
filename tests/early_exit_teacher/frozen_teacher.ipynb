{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5fb0c0-499b-46d1-b1cb-b3b638d167d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import sys\n",
    "sys.path.append(\"/project/project_465001340/fair_stuff/externalization/\")\n",
    "from early_exit.patching.method_patching import replace_attention_layers, set_transformer_early_exit_mode\n",
    "from shared_utils.generate import format_conversation, transform_conversations\n",
    "from early_exit.util import module_name_is_layer_base\n",
    "import numpy as np\n",
    "from early_exit.util import get_model\n",
    "from shared_utils.load import get_tokenizer, configs_from_yaml\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fdb3e16-99e4-41f9-8065-d566a4b65d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Device: cuda\n",
      "Tokenizer loaded. Vocab size: 151643\n",
      "EOS token: <｜end▁of▁sentence｜> (ID: 151643)\n",
      "address this hack!\n",
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "trainable params: 2,179,072 || all params: 1,779,276,294 || trainable%: 0.1225\n",
      "transform_conversations currently only for Deepseek models!\n",
      "Early exit layer indices: tensor([ 0,  5, 10, 15, 20, 25], dtype=torch.int32)\n",
      "Total exitable layers: 6\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "model_config_path = \"/project/project_465001340/fair_stuff/externalization/config_deepseek.yaml\"                     # args.model_config_path\n",
    "config = configs_from_yaml(model_config_path, tokenizer.eos_token_id)\n",
    "config['generation']['max_new_tokens'] = 100\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # torch_dtype=torch.float16,  # Use half precision for efficiency\n",
    "    device_map=\"auto\" if device == 'cuda' else None,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "frozen_model = get_model(model_name, config['model'], device)\n",
    "frozen_model = replace_attention_layers(frozen_model, config['lora'], device)\n",
    "set_transformer_early_exit_mode(frozen_model, 'sft_teacher')\n",
    "set_transformer_early_exit_mode(frozen_model, 'sft_student')\n",
    "\n",
    "prompt = \"Explain the concept of recursion in programming.\"\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\"\n",
    "\n",
    "pre_transformed_conversation = format_conversation(user_prompts = [prompt], system_prompt=system_prompt)\n",
    "formatted_prompt = transform_conversations(pre_transformed_conversation, prefiller)[0]\n",
    "\n",
    "from early_exit.util import module_name_is_layer_base\n",
    "early_exit_layer_idxs = []\n",
    "for name, module in model.named_modules():\n",
    "    if module_name_is_layer_base(name):\n",
    "        # Extract layer index from module name (e.g., \"model.layers.0\" -> 0)\n",
    "        layer_idx = int(name.split('.')[-1])\n",
    "        early_exit_layer_idxs.append(layer_idx)\n",
    "\n",
    "early_exit_layer_idxs = torch.tensor(early_exit_layer_idxs, dtype = torch.int32)  # Add inf for final layer\n",
    "print(f\"Early exit layer indices: {early_exit_layer_idxs}\")\n",
    "print(f\"Total exitable layers: {len(early_exit_layer_idxs)}\")  # Subtract 1 for the inf\n",
    "\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs.input_ids\n",
    "prompt_length = input_ids.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "650bf60b-e947-4b56-81e4-373ba8c90cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Okay, so I need to explain the concept of recursion in its in owe-betaaggerAn BBBaggerABCDEFGagger registaggerAnAn polite-car degrade degrade attenuation degrade attenuation degrade将以agger attenuationagger degradeagger\n",
      "Chosen exit layers: [-1, -1, -1, -1, -1, 25, -1, -1, 20, 20, -1, 25, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "KL_FACTOR = 8\n",
    "current_input = input_ids.clone()\n",
    "generated_tokens_manual = []\n",
    "chosen_exit_layers = []\n",
    "config['generation']['max_new_tokens'] = 40\n",
    "for step in range(config['generation']['max_new_tokens']):\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(current_input, use_cache=True, output_hidden_states=True)\n",
    "        # print(outputs.logits.shape)\n",
    "        logits = outputs.logits[:, -1, :]  # Get logits for last token\n",
    "        hidden_states = torch.stack(outputs.hidden_states)\n",
    "        exit_hidden_states = hidden_states[early_exit_layer_idxs, :, -1, :].transpose(0,1)\n",
    "        exit_predictions = model.lm_head(exit_hidden_states)\n",
    "\n",
    "        # 1. Get KL divergence between early exit and final layers\n",
    "        final_predictions = torch.softmax(logits, dim=-1)\n",
    "        teacher_expanded = final_predictions.unsqueeze(1)  \n",
    "        early_output_probs = torch.softmax(exit_predictions, dim=-1)\n",
    "\n",
    "        # Sum over vocab -> [batch, exitable layers, sequence]\n",
    "        # print(teacher_expanded.shape, early_output_probs.shape)\n",
    "        eps = 1e-16\n",
    "        kl_div = (teacher_expanded * ((teacher_expanded + eps) / (early_output_probs + eps)).log()).sum(-1)\n",
    "        # kl_div = - (teacher_expanded * (early_output_probs + eps).log()).sum(-1)\n",
    "\n",
    "        # 2. Scale KL divergencees by KL_FACTOR and pass through sigmoid (0-1)\n",
    "        sigmoid_kls = torch.sigmoid(KL_FACTOR * kl_div)  # [batch, exitable layers, sequence]\n",
    "        sigmoid_kls = 2.0 * sigmoid_kls - 1.0\n",
    "        sigmoid_kls = 1.0 - sigmoid_kls\n",
    "        predictions = final_predictions\n",
    "        chosen_exit_layer = -1\n",
    "        for qdx, exit_layer in enumerate(early_exit_layer_idxs):\n",
    "            rand_val = random.random()\n",
    "            if rand_val < sigmoid_kls[0, qdx]:\n",
    "                predictions = early_output_probs[:, qdx]\n",
    "                chosen_exit_layer = exit_layer\n",
    "                break\n",
    "        # Sample next token\n",
    "        # import ipdb;  ipdb.set_trace();\n",
    "        if step > 1:\n",
    "            chosen_exit_layers_tensor = torch.tensor(chosen_exit_layers[:-1], device=device).unsqueeze(0).float()  # Add batch dimension\n",
    "            chosen_exit_layers_tensor = torch.where(\n",
    "                chosen_exit_layers_tensor == -1,\n",
    "                torch.full_like(chosen_exit_layers_tensor, float('inf')),\n",
    "                chosen_exit_layers_tensor\n",
    "            )\n",
    "            # print(current_input)\n",
    "            # print([tokenizer.decode(item, skip_special_tokens=True) for item in current_input.squeeze()])\n",
    "            output_scores, _ = frozen_model(current_input, prescribed_exit_layer_idxs = chosen_exit_layers_tensor) # [batch * samples, full length, vocabulary]\n",
    "            \n",
    "            # print(current_input.shape, chosen_exit_layers_tensor.shape)\n",
    "            next_token_teacher = torch.argmax(predictions, dim=-1).unsqueeze(-1)\n",
    "            next_token = torch.argmax(output_scores.logits[:,-1], dim=-1).unsqueeze(-1)\n",
    "            # print(next_token_teacher, tokenizer.decode(next_token_teacher[0], skip_special_tokens=True))\n",
    "            # print(next_token, tokenizer.decode(next_token[0], skip_special_tokens=True))\n",
    "        # import ipdb;  ipdb.set_trace();\n",
    "        # Check for EOS\n",
    "        else:\n",
    "            next_token = torch.argmax(predictions, dim=-1).unsqueeze(-1)\n",
    "        if next_token.item() == config['generation']['eos_token_id']:\n",
    "            print(f\"EOS token encountered at step {step}\")\n",
    "            break\n",
    "            \n",
    "        # Add token to sequence\n",
    "        current_input = torch.cat([current_input, next_token], dim=1)\n",
    "        generated_tokens_manual.append(next_token.item())\n",
    "        \n",
    "        # Decode and print current token\n",
    "        token_text = tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
    "        # print(f\"Step {step}: Token {next_token.item()} -> '{token_text}'\")\n",
    "        chosen_exit_layers.append(int(chosen_exit_layer))\n",
    "\n",
    "manual_generated_text = tokenizer.decode(generated_tokens_manual, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated text: {manual_generated_text}\")\n",
    "print(f\"Chosen exit layers: {chosen_exit_layers}\")\n",
    "# print(f\"Total tokens generated: {len(generated_tokens_manual)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1c07d71-c5e7-4cd3-8f49-60ff02e7b7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22, 151936])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_scores.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19518ca5-b6b5-46c9-bfc5-9cc99420d2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 23]), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_input.shape, chosen_exit_layers_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "494328a9-3a61-4569-97d6-3cda5d56b1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22, 151936])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_scores.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9518fce6-8f8d-4f33-9564-ba3df2a95b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12.0701,  1.8095,  0.1320,  ..., -2.6847, -2.6852, -2.6860],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_scores.logits[0, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a74becec-269e-421f-8edb-74b179b398ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1774, -0.9944, -3.3728,  ...,  0.6654,  0.6649,  0.6652],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_scores.logits[0, 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebfcfa-031d-42e0-989b-494941700ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
