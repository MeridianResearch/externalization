{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7679396f-a3d2-4804-882c-8a08c8972123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabled automatic differentiation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import sys\n",
    "# sys.path.append(\"../../\")\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "# sys.path.append(\"..\")\n",
    "from early_exit.patching.method_patching import replace_attention_layers, set_transformer_early_exit_mode\n",
    "from shared_utils.generate import format_conversation, transform_conversations\n",
    "from early_exit.util import module_name_is_layer_base\n",
    "import numpy as np\n",
    "from early_exit.util import get_model\n",
    "from shared_utils.load import get_tokenizer, configs_from_yaml\n",
    "from shared_utils.generate import generate_text\n",
    "import random\n",
    "from early_exit_teacher.visualization import visualize_tokens_by_exit_layer, create_html_visualization\n",
    "from IPython.display import HTML, display\n",
    "from early_exit.util import module_name_is_layer_base\n",
    "torch.set_grad_enabled(False)\n",
    "print(\"Disabled automatic differentiation\")\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers.models.qwen2.modeling_qwen2 import apply_rotary_pos_emb, repeat_kv, Qwen2Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed5f7fc2-cb8b-476d-8f80-e2ccf63ae255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Device: cuda\n",
      "address this hack!\n",
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "trainable params: 2,179,072 || all params: 1,779,276,294 || trainable%: 0.1225\n",
      "Tokenizer loaded. Vocab size: 151643\n",
      "EOS token: <｜end▁of▁sentence｜> (ID: 151643)\n",
      "transform_conversations currently only for Deepseek models!\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "model_config_path = \"/project/project_465001340/fair_stuff/externalization/config_deepseek.yaml\"   # args.model_config_path\n",
    "config = configs_from_yaml(model_config_path, tokenizer.eos_token_id)\n",
    "\n",
    "model = get_model(model_name, config['model'], device)\n",
    "model = replace_attention_layers(model, config['lora'], device)\n",
    "# set_transformer_early_exit_mode(model, 'off')\n",
    "\n",
    "# Load tokenizer\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "\n",
    "config['generation']['max_new_tokens'] = 10\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "\n",
    "\n",
    "\n",
    "prompt = \"Explain the concept of recursion in programming.\"\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\"\n",
    "\n",
    "pre_transformed_conversation = format_conversation(user_prompts = [prompt], system_prompt=system_prompt)\n",
    "formatted_prompt = transform_conversations(pre_transformed_conversation, prefiller)[0]\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b35fadb-718f-4c68-bf15-9c3733caf3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from early_exit.util import module_name_is_layer_base\n",
    "early_exit_layer_idxs = []\n",
    "for name, module in model.named_modules():\n",
    "    if module_name_is_layer_base(name):\n",
    "        layer_idx = int(name.split('.')[-1])\n",
    "        early_exit_layer_idxs.append(layer_idx)\n",
    "\n",
    "early_exit_layer_idxs = torch.tensor(early_exit_layer_idxs, dtype = torch.int32)  # Add inf for final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d319c49-7bd6-4784-b1c6-43105864dfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜Assistant｜> You are a helpful programming tutor.\n",
      "<｜User｜> Explain the concept of recursion in programming.\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to explain � \"__新型冠状onium__\" the concept of recursion in programming.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"font-family: Arial, sans-serif; margin: 20px; padding: 20px; \n",
       "                background-color: #f9f9f9; border-radius: 10px;\">\n",
       "        <h3 style=\"text-align: center; color: #333; margin-bottom: 20px;\">Committed Early Exit Token Generation</h3>\n",
       "        \n",
       "        <!-- Legend -->\n",
       "        <div style=\"display: flex; justify-content: center; gap: 15px; \n",
       "                    margin: 20px 0; padding: 15px; background-color: #fff; \n",
       "                    border-radius: 5px; flex-wrap: wrap; border: 1px solid #ddd;\">\n",
       "    \n",
       "                <div style=\"display: flex; align-items: center; gap: 8px;\">\n",
       "                    <div style=\"width: 25px; height: 15px; background-color: #6f91f2; \n",
       "                                border: 1px solid #333; border-radius: 3px;\"></div>\n",
       "                    <span style=\"font-size: 14px;\">Layer 25</span>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"display: flex; align-items: center; gap: 8px;\">\n",
       "                    <div style=\"width: 25px; height: 15px; background-color: #3a4cc0; \n",
       "                                border: 1px solid #333; border-radius: 3px;\"></div>\n",
       "                    <span style=\"font-size: 14px;\">Final Layer</span>\n",
       "                </div>\n",
       "            \n",
       "        </div>\n",
       "        \n",
       "        <!-- Tokens -->\n",
       "        <div style=\"line-height: 2.5; word-wrap: break-word; padding: 15px; \n",
       "                    background-color: #fff; border-radius: 5px; border: 1px solid #ddd;\">\n",
       "    <span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #3a4cc0; color: white; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\">Okay</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #3a4cc0; color: white; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\">,</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #3a4cc0; color: white; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\"> so</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #6f91f2; color: black; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\"> I</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #6f91f2; color: black; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\"> need</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #6f91f2; color: black; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\"> to</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #6f91f2; color: black; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\"> explain</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #6f91f2; color: black; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\"> �</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #6f91f2; color: black; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\"> \"__</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #6f91f2; color: black; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\">新型冠状</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #3a4cc0; color: white; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\">onium</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #3a4cc0; color: white; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\">__</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #3a4cc0; color: white; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\">\"</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #3a4cc0; color: white; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\"> the</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #3a4cc0; color: white; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\"> concept</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #3a4cc0; color: white; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\"> of</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #3a4cc0; color: white; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\"> recursion</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #3a4cc0; color: white; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\"> in</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #3a4cc0; color: white; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\"> programming</span><span style=\"display: inline-block; padding: 4px 8px; margin: 2px; \n",
       "                                      border-radius: 4px; border: 1px solid #666; \n",
       "                                      font-family: monospace; font-size: 13px; \n",
       "                                      background-color: #3a4cc0; color: white; \n",
       "                                      font-weight: bold; max-width: 200px; \n",
       "                                      overflow-wrap: break-word; vertical-align: middle;\">.</span>\n",
       "        </div>\n",
       "        \n",
       "        <!-- Statistics -->\n",
       "        <div style=\"margin-top: 15px; padding: 10px; background-color: #e8f4fd; \n",
       "                    border-radius: 5px; font-family: monospace; font-size: 13px;\">\n",
       "    Total tokens: 20 | Layer 25: 7 (35.0%) | Final: 13 (65.0%)\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def set_early_exit_layer(step):\n",
    "    if step > 2 and step < 10: return 25\n",
    "    return 27\n",
    "\n",
    "set_transformer_early_exit_mode(model, 'off')\n",
    "generated_tokens = []\n",
    "max_new_tokens = 20\n",
    "generated = inputs[\"input_ids\"]\n",
    "chosen_exit_layers = []\n",
    "all_student_logits = []\n",
    "all_teacher_logits = []\n",
    "for step in range(max_new_tokens):  # generate 10 tokens\n",
    "    if step == 0:\n",
    "        student_outputs = model(generated, use_cache=True)\n",
    "        student_logits = student_outputs.logits\n",
    "        student_cache = student_outputs.past_key_values\n",
    "        teacher_outputs = model(generated, use_cache=True)\n",
    "        teacher_cache = teacher_outputs.past_key_values\n",
    "        teacher_logits = teacher_outputs.logits\n",
    "    else:\n",
    "        # Pass only the new token and the cache\n",
    "        student_outputs = model(\n",
    "            next_token,\n",
    "            past_key_values=student_cache,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        early_exit_layer = set_early_exit_layer(step)\n",
    "        hidden_states = student_outputs.hidden_states\n",
    "        hidden_states = torch.stack(student_outputs.hidden_states)[1:]\n",
    "        # print(f\"Hidden states shape = {hidden_states.shape}\")\n",
    "        exit_hidden_state = hidden_states[early_exit_layer]\n",
    "        student_logits = model.lm_head(exit_hidden_state)\n",
    "        all_student_logits.append(student_logits)\n",
    "        student_cache = student_outputs.past_key_values  # updated with new token\n",
    "        # print(len(past_key_values))\n",
    "        for layer_idx in range(early_exit_layer + 1, 28):\n",
    "            layer = model.base_model.model.model.layers[layer_idx]\n",
    "            normed_hidden = layer.input_layernorm(exit_hidden_state)\n",
    "\n",
    "            # Project to K and V using this layer's projections\n",
    "            key_states = layer.self_attn.k_proj(normed_hidden)\n",
    "            value_states = layer.self_attn.v_proj(normed_hidden)\n",
    "            \n",
    "            # Reshape for multi-head attention\n",
    "            num_key_value_heads = layer.self_attn.config.num_key_value_heads\n",
    "            head_dim = layer.self_attn.head_dim\n",
    "            # print(key_states.shape, num_key_value_heads)\n",
    "            key_states = key_states.view(1, 1, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "            value_states = value_states.view(1, 1, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "            # print(student_cache[0][0].shape)\n",
    "            current_position = student_cache[0][0].shape[-2]\n",
    "            position_ids = torch.tensor([[current_position]], device=device)\n",
    "            cos, sin = model.base_model.model.model.rotary_emb(value_states, position_ids)\n",
    "            _, key_states = apply_rotary_pos_emb(key_states, key_states, cos, sin)\n",
    "            # find_updated_cache(exit_hidden_state, layer, student_cache, current_position)\n",
    "            # print(student_cache[layer_idx][0][:, :, -1:].shape, key_states.shape)\n",
    "            # student_cache[layer_idx][0][:, :, -1] = student_cache[early_exit_layer][0][:, :, -1] # keys\n",
    "            # student_cache[layer_idx][1][:, :, -1] = student_cache[early_exit_layer][1][:, :, -1] # values\n",
    "            \n",
    "            student_cache[layer_idx][0][:, :, -1:] = key_states # keys\n",
    "            student_cache[layer_idx][1][:, :, -1:] = value_states # values\n",
    "            \n",
    "        # print(len(student_cache), student_cache[0][0].shape)\n",
    "        \n",
    "        teacher_outputs = model(\n",
    "            next_token,\n",
    "            past_key_values=teacher_cache,\n",
    "            use_cache=True\n",
    "        )\n",
    "        teacher_cache = teacher_outputs.past_key_values\n",
    "        teacher_logits = teacher_outputs.logits\n",
    "        all_teacher_logits.append(teacher_logits)\n",
    "    # Take the most likely next token (greedy decoding here)\n",
    "    next_token = torch.argmax(student_logits[:, -1, :], dim=-1).unsqueeze(-1)\n",
    "    # next_token = torch.argmax(teacher_logits[:, -1, :], dim=-1).unsqueeze(-1)\n",
    "    if step > 0: \n",
    "        generated_tokens.append(next_token.item())\n",
    "        chosen_exit_layers.append(early_exit_layer)\n",
    "    else:\n",
    "        generated_tokens.append(next_token.item())\n",
    "        chosen_exit_layers.append(-1)\n",
    "    # Append to generated sequence\n",
    "    generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "print(tokenizer.decode(generated[0]))\n",
    "all_student_logits = torch.concatenate(all_student_logits, axis = 0).transpose(0,1)\n",
    "all_teacher_logits = torch.concatenate(all_teacher_logits, axis = 0).transpose(0,1)\n",
    "# all_student_logits.shape\n",
    "tokens = [tokenizer.decode([token]) for token in generated_tokens]\n",
    "layers = [27 if item == 27 or item == -1 else item for item in chosen_exit_layers]\n",
    "early_exit_layers = early_exit_layer_idxs.tolist()  # Convert tensor to list if needed\n",
    "# Display the visualization\n",
    "display(visualize_tokens_by_exit_layer(tokens, layers, early_exit_layers, \n",
    "                                     title=\"Committed Early Exit Token Generation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0c77528-f742-4c90-9aca-f5fceec7ff91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[[-2.1860e-02, -7.2696e-02, -1.1570e-01,  ...,  2.1059e+00,\n",
      "            4.7061e-01, -7.4747e-01],\n",
      "          [-1.0939e-02, -7.7366e-02, -4.7914e-02,  ...,  2.1022e+00,\n",
      "            4.8923e-01, -7.4620e-01],\n",
      "          [-4.1520e-01, -8.6979e-02, -3.5009e-01,  ...,  4.1750e+00,\n",
      "           -2.6645e+00, -1.5610e+00],\n",
      "          ...,\n",
      "          [ 5.0791e-01, -1.4946e-01, -4.3302e-02,  ...,  1.7778e+00,\n",
      "           -1.9287e+00, -1.1862e+00],\n",
      "          [-1.3289e-01, -1.6452e-01, -3.5462e-02,  ..., -1.1493e+00,\n",
      "           -1.1632e+00, -1.9846e+00],\n",
      "          [ 6.9239e-01,  2.4655e-01,  7.8054e-01,  ...,  1.5079e+00,\n",
      "           -8.3180e-01, -3.5837e+00]],\n",
      "\n",
      "         [[ 3.6603e-03, -1.3313e-03,  3.6746e-02,  ..., -1.3509e+00,\n",
      "           -1.1811e+00,  3.5928e+00],\n",
      "          [ 7.6382e-04, -9.9849e-03,  4.2674e-02,  ..., -1.3672e+00,\n",
      "           -1.1651e+00,  3.5925e+00],\n",
      "          [ 3.8066e-01, -9.2743e-02, -4.8343e-01,  ...,  2.7600e+00,\n",
      "           -2.9237e+00, -1.8019e+00],\n",
      "          ...,\n",
      "          [-1.9316e-01, -2.2150e-01,  2.7199e-02,  ...,  1.2091e+00,\n",
      "           -3.8890e-01,  7.2532e-01],\n",
      "          [ 1.9121e-01,  2.7157e-01,  2.1946e-01,  ...,  2.4159e+00,\n",
      "           -1.9215e-01, -3.7383e+00],\n",
      "          [ 1.6140e-01, -3.4182e-01,  7.4671e-02,  ...,  3.0078e+00,\n",
      "           -2.3826e-01, -6.2591e-01]]]], device='cuda:0'), tensor([[[[-5.7756e-02, -1.9311e-01, -3.8827e-02,  ..., -8.9769e-02,\n",
      "           -6.7311e-02,  8.9159e-02],\n",
      "          [-3.7829e-02, -2.3592e-01, -3.3248e-03,  ..., -1.5855e-01,\n",
      "           -3.8760e-02,  1.2309e-01],\n",
      "          [-6.8310e-01,  1.8705e+00,  1.0093e+00,  ..., -1.7879e+00,\n",
      "            3.6053e+00,  1.5913e+00],\n",
      "          ...,\n",
      "          [ 9.5976e-01, -6.2843e-01, -4.4392e-01,  ...,  5.0361e-01,\n",
      "            1.6770e+00,  1.2140e+00],\n",
      "          [-2.2638e-01,  4.0860e+00, -2.9365e+00,  ..., -3.4682e+00,\n",
      "           -8.2156e+00,  3.0514e+00],\n",
      "          [ 1.3687e+00,  3.1968e-01, -1.5099e+00,  ...,  1.8419e+00,\n",
      "           -1.1037e-02,  6.9720e-01]],\n",
      "\n",
      "         [[ 1.0274e-01,  1.7658e-01,  1.0451e-01,  ...,  1.7278e-01,\n",
      "            2.3651e-02, -2.6109e-01],\n",
      "          [ 1.1152e-01,  1.8667e-01,  1.8201e-01,  ...,  1.7290e-01,\n",
      "            5.6002e-02, -2.2036e-01],\n",
      "          [ 2.1023e+00,  6.5873e+00,  4.0502e+00,  ..., -9.0617e+00,\n",
      "           -1.1437e+00,  1.0054e+00],\n",
      "          ...,\n",
      "          [ 1.0883e-01, -7.9495e-03, -1.5030e+00,  ...,  9.0958e-02,\n",
      "            1.4489e-01, -1.4553e+00],\n",
      "          [ 1.1662e-01, -2.9776e+00,  4.4521e+00,  ...,  4.6638e+00,\n",
      "           -1.2439e-01,  4.0574e+00],\n",
      "          [ 2.2106e+00, -7.2529e-01,  4.0770e-01,  ..., -2.4632e-01,\n",
      "           -4.3831e-01,  1.5691e+00]]]], device='cuda:0'))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 112\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Then propagate the exit hidden state to future layers' caches\u001b[39;00m\n\u001b[32m    111\u001b[39m current_position = student_cache[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].shape[\u001b[32m2\u001b[39m] - \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m student_cache = \u001b[43mpropagate_exit_hidden_to_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexit_hidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_exit_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstudent_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurrent_position\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# Teacher forward pass (normal)\u001b[39;00m\n\u001b[32m    121\u001b[39m teacher_outputs = model(\n\u001b[32m    122\u001b[39m     next_token,\n\u001b[32m    123\u001b[39m     past_key_values=teacher_cache,\n\u001b[32m    124\u001b[39m     use_cache=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    125\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mpropagate_exit_hidden_to_cache\u001b[39m\u001b[34m(model, exit_hidden_state, early_exit_layer, cache, current_position)\u001b[39m\n\u001b[32m     60\u001b[39m         \u001b[38;5;28mprint\u001b[39m(cache[layer_idx])\n\u001b[32m     61\u001b[39m         \u001b[38;5;66;03m# Update cache in place\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         \u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m = old_key_cache\n\u001b[32m     63\u001b[39m         cache[layer_idx] = (old_key_cache, old_value_cache)\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cache\n",
      "\u001b[31mTypeError\u001b[39m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "def propagate_exit_hidden_to_cache(model, exit_hidden_state, early_exit_layer, cache, current_position):\n",
    "    \"\"\"\n",
    "    Propagate the same hidden state to all future layers' KV caches.\n",
    "    \n",
    "    exit_hidden_state: [batch, seq_len, hidden_dim] - the hidden state at early exit\n",
    "    cache: the current KV cache\n",
    "    current_position: the position in the sequence\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        device = exit_hidden_state.device\n",
    "        batch_size, seq_len, hidden_dim = exit_hidden_state.shape\n",
    "        \n",
    "        # For each layer after early exit\n",
    "        for layer_idx in range(early_exit_layer + 1, len(model.base_model.model.model.layers)):\n",
    "            layer = model.base_model.model.model.layers[layer_idx]\n",
    "            \n",
    "            # Apply layer norm to the EXIT hidden state\n",
    "            normed_hidden = layer.input_layernorm(exit_hidden_state)\n",
    "            \n",
    "            # Project to K and V using this layer's projections\n",
    "            key_states = layer.self_attn.k_proj(normed_hidden)\n",
    "            value_states = layer.self_attn.v_proj(normed_hidden)\n",
    "            \n",
    "            # Reshape for multi-head attention\n",
    "            num_key_value_heads = layer.self_attn.config.num_key_value_heads\n",
    "            head_dim = layer.self_attn.head_dim\n",
    "            \n",
    "            key_states = key_states.view(batch_size, seq_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "            value_states = value_states.view(batch_size, seq_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "            \n",
    "            # Apply RoPE to keys (Qwen2 needs this)\n",
    "            position_ids = torch.tensor([[current_position]], device=device)\n",
    "\n",
    "            key_states, key_states = apply_rotary_pos_emb(key_states, key_states, cos, sin)\n",
    "            \n",
    "\n",
    "            # Now update the cache directly\n",
    "            # The cache is a list of tuples (key_cache, value_cache) for each layer\n",
    "            old_key_cache, old_value_cache = cache[layer_idx]\n",
    "            \n",
    "            # Concatenate with existing cache\n",
    "            # Note: we're only updating the last position, so we can either:\n",
    "            # Option 1: Replace the last position (if it was already computed)\n",
    "            # Option 2: Append if this is a new position\n",
    "            \n",
    " \n",
    "                old_key_cache[:, :, -1:, :] = key_states\n",
    "                old_value_cache[:, :, -1:, :] = value_states\n",
    "            else:\n",
    "                # Multiple tokens - replace the last seq_len positions\n",
    "                old_key_cache[:, :, -seq_len:, :] = key_states\n",
    "                old_value_cache[:, :, -seq_len:, :] = value_states\n",
    "\n",
    "            print(cache[layer_idx])\n",
    "            # Update cache in place\n",
    "            cache[layer_idx][0] = old_key_cache\n",
    "            cache[layer_idx] = (old_key_cache, old_value_cache)\n",
    "    \n",
    "    return cache\n",
    "\n",
    "# Your generation loop with the fix:\n",
    "def set_early_exit_layer(step):\n",
    "    if step == 2: return 25\n",
    "    return 27\n",
    "\n",
    "set_transformer_early_exit_mode(model, 'off')\n",
    "generated_tokens = []\n",
    "max_new_tokens = 10\n",
    "generated = inputs[\"input_ids\"]\n",
    "chosen_exit_layers = []\n",
    "all_student_logits = []\n",
    "all_teacher_logits = []\n",
    "\n",
    "for step in range(max_new_tokens):\n",
    "    if step == 0:\n",
    "        student_outputs = model(generated, use_cache=True, output_hidden_states=True)\n",
    "        student_logits = student_outputs.logits\n",
    "        student_cache = student_outputs.past_key_values\n",
    "        teacher_outputs = model(generated, use_cache=True)\n",
    "        teacher_cache = teacher_outputs.past_key_values\n",
    "        teacher_logits = teacher_outputs.logits\n",
    "    else:\n",
    "        # Pass only the new token and the cache\n",
    "        student_outputs = model(\n",
    "            next_token,\n",
    "            past_key_values=student_cache,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        early_exit_layer = set_early_exit_layer(step)\n",
    "        hidden_states = torch.stack(student_outputs.hidden_states)[1:]  # Skip embeddings\n",
    "        \n",
    "        # Get the hidden state at early exit layer for the last token\n",
    "        exit_hidden_state = hidden_states[early_exit_layer][:, -1:, :]  # [batch, 1, hidden_dim]\n",
    "        \n",
    "        # Get student logits from early exit\n",
    "        student_logits = model.lm_head(exit_hidden_state)\n",
    "        all_student_logits.append(student_logits)\n",
    "        \n",
    "        # First, update the cache normally\n",
    "        student_cache = student_outputs.past_key_values\n",
    "        \n",
    "        # Then propagate the exit hidden state to future layers' caches\n",
    "        current_position = student_cache[0][0].shape[2] - 1\n",
    "        student_cache = propagate_exit_hidden_to_cache(\n",
    "            model,\n",
    "            exit_hidden_state,\n",
    "            early_exit_layer,\n",
    "            student_cache,\n",
    "            current_position\n",
    "        )\n",
    "        \n",
    "        # Teacher forward pass (normal)\n",
    "        teacher_outputs = model(\n",
    "            next_token,\n",
    "            past_key_values=teacher_cache,\n",
    "            use_cache=True\n",
    "        )\n",
    "        teacher_cache = teacher_outputs.past_key_values\n",
    "        teacher_logits = teacher_outputs.logits\n",
    "        all_teacher_logits.append(teacher_logits)\n",
    "    \n",
    "    # Sample next token\n",
    "    next_token = torch.argmax(teacher_logits[:, -1, :], dim=-1).unsqueeze(-1)\n",
    "    \n",
    "    if step > 0:\n",
    "        generated_tokens.append(next_token.item())\n",
    "        chosen_exit_layers.append(early_exit_layer)\n",
    "    else:\n",
    "        generated_tokens.append(next_token.item())\n",
    "        chosen_exit_layers.append(-1)\n",
    "    \n",
    "    # Append to generated sequence\n",
    "    generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "# Visualization code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff194deb-e65c-43db-af95-61959a1db7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student Token</th>\n",
       "      <th>Student Prob</th>\n",
       "      <th>Student Cache Token</th>\n",
       "      <th>Student Cache Prob</th>\n",
       "      <th>Teacher Cache Token</th>\n",
       "      <th>Teacher Cache Prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>,</td>\n",
       "      <td>1.00</td>\n",
       "      <td>,</td>\n",
       "      <td>1.00</td>\n",
       "      <td>,</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>so</td>\n",
       "      <td>0.99</td>\n",
       "      <td>so</td>\n",
       "      <td>1.00</td>\n",
       "      <td>so</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Helvetica</td>\n",
       "      <td>0.01</td>\n",
       "      <td>I</td>\n",
       "      <td>0.92</td>\n",
       "      <td>I</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>一定程度</td>\n",
       "      <td>0.01</td>\n",
       "      <td>need</td>\n",
       "      <td>0.64</td>\n",
       "      <td>need</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transparent</td>\n",
       "      <td>0.03</td>\n",
       "      <td>to</td>\n",
       "      <td>1.00</td>\n",
       "      <td>to</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lay</td>\n",
       "      <td>0.01</td>\n",
       "      <td>explain</td>\n",
       "      <td>0.74</td>\n",
       "      <td>explain</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>enses</td>\n",
       "      <td>0.01</td>\n",
       "      <td>recursion</td>\n",
       "      <td>0.50</td>\n",
       "      <td>recursion</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Helvetica</td>\n",
       "      <td>0.00</td>\n",
       "      <td>in</td>\n",
       "      <td>0.98</td>\n",
       "      <td>in</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>incoming</td>\n",
       "      <td>0.06</td>\n",
       "      <td>programming</td>\n",
       "      <td>1.00</td>\n",
       "      <td>programming</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Student Token  Student Prob Student Cache Token  Student Cache Prob  \\\n",
       "0             ,          1.00                   ,                1.00   \n",
       "1            so          0.99                  so                1.00   \n",
       "2     Helvetica          0.01                   I                0.92   \n",
       "3          一定程度          0.01                need                0.64   \n",
       "4   transparent          0.03                  to                1.00   \n",
       "5           lay          0.01             explain                0.74   \n",
       "6         enses          0.01           recursion                0.50   \n",
       "7     Helvetica          0.00                  in                0.98   \n",
       "8      incoming          0.06         programming                1.00   \n",
       "\n",
       "  Teacher Cache Token  Teacher Cache Prob  \n",
       "0                   ,                1.00  \n",
       "1                  so                0.78  \n",
       "2                   I                0.93  \n",
       "3                need                0.65  \n",
       "4                  to                1.00  \n",
       "5             explain                0.75  \n",
       "6           recursion                0.50  \n",
       "7                  in                0.98  \n",
       "8         programming                1.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chosen_exit_layers_tensor = torch.tensor(chosen_exit_layers[1:], device=device).unsqueeze(0).float()\n",
    "chosen_exit_layers_tensor = torch.where(\n",
    "                    chosen_exit_layers_tensor == 27,\n",
    "                    torch.full_like(chosen_exit_layers_tensor, float('inf')),\n",
    "                    chosen_exit_layers_tensor\n",
    "                )\n",
    "repeated_sft_teacher_generated_tokens = generated[:, :-1]\n",
    "\n",
    "set_transformer_early_exit_mode(model, 'sft_student')\n",
    "sft_student_output_scores, collected_exit_logits = model(repeated_sft_teacher_generated_tokens,\\\n",
    "                                                             prescribed_exit_layer_idxs=chosen_exit_layers_tensor)\n",
    "\n",
    "sft_student_output = sft_student_output_scores.logits.squeeze()[20:]\n",
    "kv_cache_output = all_student_logits.squeeze()\n",
    "\n",
    "student_probs = F.softmax(sft_student_output, dim=-1)\n",
    "teacher_cache_probs = F.softmax(all_teacher_logits.squeeze(), dim=-1)\n",
    "student_cache_probs = F.softmax(kv_cache_output, dim=-1)\n",
    "\n",
    "# model_outputs = model(repeated_sft_teacher_generated_tokens)\n",
    "# model_probs = F.softmax(model_outputs.logits[0, 20:], dim = -1)\n",
    "\n",
    "# set_transformer_early_exit_mode(frozen_model, 'off')\n",
    "# off_outputs = frozen_model(repeated_sft_teacher_generated_tokens)\n",
    "# off_probs = F.softmax(off_outputs.logits[0, 20:], dim = -1)\n",
    "\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "rows = []\n",
    "\n",
    "def get_prob_token(probs):\n",
    "    top_id = torch.argmax(probs).item()\n",
    "    top_prob = probs[top_id].item()\n",
    "    top_token = tokenizer.decode([top_id])\n",
    "    return top_prob, top_token\n",
    "\n",
    "for idx in range(len(student_probs)):\n",
    "    # Student\n",
    "    student_top_prob, student_top_token = get_prob_token(student_probs[idx])\n",
    "    # teacher_top_prob, teacher_top_token = get_prob_token(teacher_probs[idx])\n",
    "    \n",
    "    student_cache_top_prob, student_cache_top_token = get_prob_token(student_cache_probs[idx])\n",
    "    teacher_cache_top_prob, teacher_cache_top_token = get_prob_token(teacher_cache_probs[idx])\n",
    "\n",
    "    \n",
    "    # model_top_prob, model_top_token = get_prob_token(model_probs[idx])\n",
    "    \n",
    "    # off_top_prob, off_top_token = get_prob_token(off_probs[idx])\n",
    "\n",
    "    rows.append({\n",
    "        # \"Position\": idx,\n",
    "        \"Student Token\": student_top_token,\n",
    "        \"Student Prob\": student_top_prob,\n",
    "        # \"Teacher Token\": teacher_top_token,\n",
    "        # \"Teacher Prob\": teacher_top_prob,\n",
    "        \"Student Cache Token\": student_cache_top_token,\n",
    "        \"Student Cache Prob\": student_cache_top_prob,\n",
    "        \"Teacher Cache Token\": teacher_cache_top_token,\n",
    "        \"Teacher Cache Prob\": teacher_cache_top_prob,\n",
    "        # \"Model Token\": model_top_token,\n",
    "        # \"Model Prob\": model_top_prob,\n",
    "        # \"Off Token\": off_top_token,\n",
    "        # \"Off Prob\": off_top_prob\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd45f5-3af6-4ce1-baef-5e6434baf6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
