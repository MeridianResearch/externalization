{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847bd3d6-92b3-422a-aede-e6e37b6224fd",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265f9e8e-40d0-4e68-82d4-d8299d0d6349",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36611770-1adf-4e29-a316-7aa30525ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from shared_utils.data import CSVPromptDataset\n",
    "from early_exit.util import get_model\n",
    "from shared_utils.load import get_tokenizer, configs_from_yaml\n",
    "from shared_utils.generate import generate_text\n",
    "\n",
    "from early_exit.patching import replace_attention_layers, set_transformer_early_exit_mode\n",
    "\n",
    "# import wandb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a80c464-b1cb-4146-946e-82236d829e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOAD IN EXPERIMENT ARGS\n",
    "# num_epoch = 1                     # args.num_epoch\n",
    "num_exit_samples = 1                  # args.num_exit_samples\n",
    "device = \"cuda\"                    # args.device\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"                    # args.model_name\n",
    "model_config_path = \"../config_deepseek.yaml\"                     # args.model_config_path\n",
    "dataset_path = \"../results_and_data/early_exit_sft_dataset/test/data.csv\"                  # args.dataset_path\n",
    "prompt_config_path = \"../results_and_data/early_exit_sft_dataset/test/prompt_config.json\"                    # args.prompt_config_path\n",
    "batch_size = 1                    # args.batch_size -- might want to sort out batching, but increasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0423c4bb-ca8b-44e0-89d7-10ce3c09ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IN THE MODEL AND TOKENIZER\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "config = configs_from_yaml(model_config_path, tokenizer.eos_token_id)\n",
    "\n",
    "\n",
    "\n",
    "# LOAD IN DATASET\n",
    "dataset = CSVPromptDataset(dataset_path, prompt_config_path)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "635140fd-1879-4070-984d-c2b786456f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47bf34822a34b718a6e7aa55ecdc67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce153d7ef5af4e95821fabdc7fd44730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d364004002c04f8591bdc177c6861751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing layer model.layers.0\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.1\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.2\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.3\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.4\n",
      "replacing layer model.layers.5\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.6\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.7\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.8\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.9\n",
      "replacing layer model.layers.10\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.11\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.12\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.13\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.14\n",
      "replacing layer model.layers.15\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.16\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.17\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.18\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.19\n",
      "replacing layer model.layers.20\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.21\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.22\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.23\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.24\n",
      "replacing layer model.layers.25\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.26\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.27\n",
      "address this hack!\n",
      "trainable params: 2,179,072 || all params: 1,779,276,294 || trainable%: 0.1225\n"
     ]
    }
   ],
   "source": [
    "model = get_model(model_name, config['model'], device)\n",
    "# ENABLE EARLY EXITING\n",
    "model = replace_attention_layers(model, config['lora'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c23d1afa-2267-458e-ab4b-3e7847866d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from early_exit.util import module_name_is_transformer_layer\n",
    "\n",
    "for prompt_batch in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921a7d1-f788-4ada-b2fb-c8d1b9633e46",
   "metadata": {},
   "source": [
    "# Setup tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d2ade3-1fdb-4362-91ae-6108fd44256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationTracker:\n",
    "    def __init__(self):\n",
    "        self.hooks = []  # Store hook handles for removal\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all counters and remove existing hooks\"\"\"\n",
    "        self.layer_forward_counts = {}\n",
    "        self.attention_counts = {}\n",
    "        self.mlp_counts = {}\n",
    "        self.residual_counts = {}\n",
    "        \n",
    "        # Remove all existing hooks\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def register_hooks(self, model):\n",
    "        \"\"\"Register forward hooks on model components\"\"\"\n",
    "        self.reset()  # Clear any existing hooks first\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            # Track self-attention computations\n",
    "            if 'self_attn' in name and 'proj' not in name:\n",
    "                parts = name.split('.')\n",
    "                # Find the layer index - look for 'layers' and get the next element\n",
    "                layer_idx = -1\n",
    "                for i, part in enumerate(parts):\n",
    "                    if part == 'layers' and i + 1 < len(parts) and parts[i + 1].isdigit():\n",
    "                        layer_idx = int(parts[i + 1])\n",
    "                        break\n",
    "                \n",
    "                hook = module.register_forward_hook(\n",
    "                    lambda m, i, o, idx=layer_idx, n=name: self._log_attention(idx, n, i, o)\n",
    "                )\n",
    "                self.hooks.append(hook)\n",
    "                \n",
    "            # Track MLP computations\n",
    "            elif 'mlp' in name:\n",
    "                parts = name.split('.')\n",
    "                # Find the layer index - look for 'layers' and get the next element\n",
    "                layer_idx = -1\n",
    "                for i, part in enumerate(parts):\n",
    "                    if part == 'layers' and i + 1 < len(parts) and parts[i + 1].isdigit():\n",
    "                        layer_idx = int(parts[i + 1])\n",
    "                        break\n",
    "                \n",
    "                hook = module.register_forward_hook(\n",
    "                    lambda m, i, o, idx=layer_idx: self._log_mlp(idx, i, o)\n",
    "                )\n",
    "                self.hooks.append(hook)\n",
    "                \n",
    "            # Track layer-level forward passes\n",
    "            elif module_name_is_transformer_layer(name):\n",
    "                # Extract layer index from the last element after 'layers'\n",
    "                layer_idx = int(name.split('.')[-1])\n",
    "                \n",
    "                hook = module.register_forward_hook(\n",
    "                    lambda m, i, o, idx=layer_idx: self._log_layer(idx, i, o)\n",
    "                )\n",
    "                self.hooks.append(hook)\n",
    "    \n",
    "    def _log_attention(self, layer_idx, name, inputs, outputs):\n",
    "        \"\"\"Log attention computation\"\"\"\n",
    "        if layer_idx not in self.attention_counts:\n",
    "            self.attention_counts[layer_idx] = 0\n",
    "        self.attention_counts[layer_idx] += 1\n",
    "        \n",
    "    def _log_mlp(self, layer_idx, inputs, outputs):\n",
    "        \"\"\"Log MLP computation\"\"\"\n",
    "        if layer_idx not in self.mlp_counts:\n",
    "            self.mlp_counts[layer_idx] = 0\n",
    "        self.mlp_counts[layer_idx] += 1\n",
    "        \n",
    "    def _log_layer(self, layer_idx, inputs, outputs):\n",
    "        \"\"\"Log layer forward pass\"\"\"\n",
    "        if layer_idx not in self.layer_forward_counts:\n",
    "            self.layer_forward_counts[layer_idx] = 0\n",
    "        self.layer_forward_counts[layer_idx] += 1\n",
    "        \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get a summary of all computations\"\"\"\n",
    "        return {\n",
    "            'layer_forwards': dict(sorted(self.layer_forward_counts.items())),\n",
    "            'attention_ops': dict(sorted(self.attention_counts.items())),\n",
    "            'mlp_ops': dict(sorted(self.mlp_counts.items())),\n",
    "            'total_attention_ops': sum(self.attention_counts.values()),\n",
    "            'total_mlp_ops': sum(self.mlp_counts.values()),\n",
    "            'total_layer_ops': sum(self.layer_forward_counts.values())\n",
    "        }\n",
    "        \n",
    "def print_computation_summary(tracker):\n",
    "    summary = tracker.get_summary()\n",
    "    print(f'n_hooks={len(tracker.hooks)}')\n",
    "    print(f\"Total attention operations: {summary['total_attention_ops']}\")\n",
    "    print(f\"Total MLP operations: {summary['total_mlp_ops']}\")\n",
    "    print(f\"Attention ops per layer: {summary['attention_ops']}\")\n",
    "    print(f\"MLP ops per layer: {summary['mlp_ops']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb920980-3c4f-48b0-ba54-7d72cab23042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedComputationTracker(ComputationTracker):\n",
    "    def __init__(self, layer_pattern=r'layers\\.(\\d+)', \n",
    "                 attention_patterns=['self_attn', 'attention'],\n",
    "                 mlp_patterns=['mlp', 'ffn']):\n",
    "        self.layer_pattern = layer_pattern\n",
    "        self.attention_patterns = attention_patterns\n",
    "        self.mlp_patterns = mlp_patterns\n",
    "        self.hooks = []\n",
    "        self.reset()\n",
    "        \n",
    "    def __del__(self):\n",
    "        \"\"\"Ensure hooks are removed on deletion\"\"\"\n",
    "        self.reset()\n",
    "        \n",
    "    def extract_layer_idx(self, name):\n",
    "        \"\"\"More robust layer index extraction\"\"\"\n",
    "        import re\n",
    "        match = re.search(self.layer_pattern, name)\n",
    "        return int(match.group(1)) if match else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c878545-9e84-4286-95c6-6faf9e9b563e",
   "metadata": {},
   "source": [
    "## Model forwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0fcd736-cf92-4b71-8b33-85a181453155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_teacher(model, prompt_batch):\n",
    "    with torch.no_grad():\n",
    "        # Generate SFT targets\n",
    "        set_transformer_early_exit_mode(model, 'sft_teacher')\n",
    "        sft_teacher_response, (sft_teacher_generated_tokens, sft_teacher_final_layer_logprobs, gathered_early_exit_hidden_states) =\\\n",
    "            generate_text(\n",
    "                model=model, \n",
    "                prompt=prompt_batch.full_user_prompt, \n",
    "                system_prompt=dataset.system_prompt, \n",
    "                prefiller=dataset.prefiller, \n",
    "                tokenizer=tokenizer, \n",
    "                generation_config=config['generation'], \n",
    "                device=device\n",
    "            )\n",
    "        print(sft_teacher_response)\n",
    "\n",
    "        early_output_log_probs = model.early_exit_hidden_state_readout(gathered_early_exit_hidden_states)               # [batch, num exitable layers, gen len, vocabulary]\n",
    "        early_exit_probs = model.early_exit_target_probs(early_output_log_probs = early_output_log_probs, teacher_final_layer_log_probs = sft_teacher_final_layer_logprobs)\n",
    "        repeated_sft_teacher_final_layer_logprobs = sft_teacher_final_layer_logprobs.repeat(num_exit_samples, 1, 1)     # XXX repeat_interleave? [batch * samples, full length, vocabulary]\n",
    "\n",
    "\n",
    "    # Sample early exits\n",
    "    batch, gen_len, elayers = early_exit_probs.shape                                                                                                # [batch, generation length, exitable layers]\n",
    "    full_len = sft_teacher_generated_tokens.shape[1]\n",
    "    repeated_sft_teacher_generated_tokens = sft_teacher_generated_tokens.expand(num_exit_samples * batch, full_len)                                 # [batch * samples, full length]\n",
    "    sampled_early_exit_layer_idxs_early_with_sample_dim = torch.distributions.Categorical(probs = early_exit_probs).sample((num_exit_samples,))     # [samples, batch, generation length] \n",
    "    sampled_early_exit_layer_idxs_early = sampled_early_exit_layer_idxs_early_with_sample_dim.reshape(batch * num_exit_samples, gen_len)            # [batch * samples, generation length]\n",
    "    sampled_early_exit_layer_idxs = model.exitable_layer_idxs[sampled_early_exit_layer_idxs_early.cpu()]                                            # [batch * samples, generation length]\n",
    "\n",
    "    return sampled_early_exit_layer_idxs, repeated_sft_teacher_generated_tokens\n",
    "\n",
    "\n",
    "def forward_student(model, sampled_early_exit_layer_idxs, repeated_sft_teacher_generated_tokens):\n",
    "    # Generate with prescription\n",
    "    set_transformer_early_exit_mode(model, 'sft_student')\n",
    "    sft_student_output_scores, collected_exit_logits = model(repeated_sft_teacher_generated_tokens, prescribed_exit_layer_idxs = sampled_early_exit_layer_idxs) # [batch * samples, full length, vocabulary]\n",
    "    \n",
    "    return sft_student_output_scores, collected_exit_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f62ce32d-04e8-42cd-9613-d5cb2ec09ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 1: TEACHER MODE (Full Computation)\n",
      "============================================================\n",
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 138])\n",
      "<｜begin▁of▁sentence｜><｜Assistant｜> \n",
      "<｜User｜> I am going to give you a story and a question about the story. Read the following story carefully, understand the characters' actions and perspectives, then answer the question regarding object locations, character knowledge, and beliefs.\n",
      "\n",
      "Nicholas entered the main bar area. Matthew entered the main bar area. Addison told privately to Matthew about the bar's social media presence. Addison entered the main bar area. Matthew left the main bar area. Matthew told privately to Nicholas about the bar's menu offerings. Addison left the main bar area. Addison entered the main bar area. Avery entered the main bar area.\n",
      "\n",
      "Does Nicholas know about bar's menu offerings? Answer yes or no.\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to figure out whether Nicholas knows about the bar's menu offerings based on the story provided. Let me read through the story again carefully to understand the sequence of events and the actions of each character.\n",
      "\n",
      "First, Nicholas enters the main bar area. Then, Matthew enters. After that, Addison tells Matthew privately about the bar's social media presence. So, at this point, Matthew knows about the social media, but does he know about the menu? I don't see any mention of the menu in this part. So, Matthew probably doesn't know about the menu yet.\n",
      "\n",
      "Next, Matthew leaves the main bar area. Then, Matthew tells privately to Nicholas about the bar's menu offerings. So, Nicholas gets information from Matthew about the menu. That means Nicholas does know about the menu offerings.\n",
      "\n",
      "After that, Addison leaves the main bar area. Then, Addison enters the main bar area again. I don't see any information about the menu here, so Nicholas probably doesn't know about it either.\n",
      "\n",
      "Finally, Avery enters the main bar area. Again, no mention of the menu, so Nicholas doesn't know about it.\n",
      "\n",
      "So, the key point is that Nicholas only gets information about the menu from Matthew, who was there when Matthew told him. Since Matthew didn't know about the menu, Nicholas doesn't either. Therefore, Nicholas does not know about the bar's menu offerings.\n",
      "</think>\n",
      "\n",
      "No, Nicholas does not know about the bar's menu offerings.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "# Initialize tracker\n",
    "tracker = ComputationTracker()\n",
    "tracker = ImprovedComputationTracker()\n",
    "\n",
    "# Register hooks\n",
    "tracker.register_hooks(model)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST 1: TEACHER MODE (Full Computation)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sampled_early_exit_layer_idxs, repeated_sft_teacher_generated_tokens = forward_teacher(model, prompt_batch)\n",
    "\n",
    "teacher_summary = tracker.get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7d7f925-9775-4ffd-9e64-c0d8a29596cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teacher Computation Summary:\n",
      "n_hooks=196\n",
      "Total attention operations: 8316\n",
      "Total MLP operations: 41580\n",
      "Attention ops per layer: {0: 297, 1: 297, 2: 297, 3: 297, 4: 297, 5: 297, 6: 297, 7: 297, 8: 297, 9: 297, 10: 297, 11: 297, 12: 297, 13: 297, 14: 297, 15: 297, 16: 297, 17: 297, 18: 297, 19: 297, 20: 297, 21: 297, 22: 297, 23: 297, 24: 297, 25: 297, 26: 297, 27: 297}\n",
      "MLP ops per layer: {0: 1485, 1: 1485, 2: 1485, 3: 1485, 4: 1485, 5: 1485, 6: 1485, 7: 1485, 8: 1485, 9: 1485, 10: 1485, 11: 1485, 12: 1485, 13: 1485, 14: 1485, 15: 1485, 16: 1485, 17: 1485, 18: 1485, 19: 1485, 20: 1485, 21: 1485, 22: 1485, 23: 1485, 24: 1485, 25: 1485, 26: 1485, 27: 1485}\n",
      "\n",
      "============================================================\n",
      "TEST 2: STUDENT MODE (With Early Exits)\n",
      "============================================================\n",
      "Student Computation Summary:\n",
      "n_hooks=196\n",
      "Total attention operations: 28\n",
      "Total MLP operations: 140\n",
      "Attention ops per layer: {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1}\n",
      "MLP ops per layer: {0: 5, 1: 5, 2: 5, 3: 5, 4: 5, 5: 5, 6: 5, 7: 5, 8: 5, 9: 5, 10: 5, 11: 5, 12: 5, 13: 5, 14: 5, 15: 5, 16: 5, 17: 5, 18: 5, 19: 5, 20: 5, 21: 5, 22: 5, 23: 5, 24: 5, 25: 5, 26: 5, 27: 5}\n",
      "\n",
      "============================================================\n",
      "COMPUTATION SAVINGS\n",
      "============================================================\n",
      "Attention computation saved: 99.7%\n",
      "MLP computation saved: 99.7%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nTeacher Computation Summary:\")\n",
    "print_computation_summary(tracker)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 2: STUDENT MODE (With Early Exits)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reset tracker for student mode\n",
    "tracker.reset()\n",
    "tracker.register_hooks(model)\n",
    "\n",
    "print(\"Student Computation Summary:\")\n",
    "forward_student(model, sampled_early_exit_layer_idxs, repeated_sft_teacher_generated_tokens)\n",
    "\n",
    "student_summary = tracker.get_summary()\n",
    "print_computation_summary(tracker)\n",
    "# Calculate savings\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPUTATION SAVINGS\")\n",
    "print(\"=\"*60)\n",
    "if teacher_summary['total_attention_ops'] > 0:\n",
    "    attention_savings = 1 - (student_summary['total_attention_ops'] / teacher_summary['total_attention_ops'])\n",
    "    print(f\"Attention computation saved: {attention_savings*100:.1f}%\")\n",
    "    \n",
    "if teacher_summary['total_mlp_ops'] > 0:\n",
    "    mlp_savings = 1 - (student_summary['total_mlp_ops'] / teacher_summary['total_mlp_ops'])\n",
    "    print(f\"MLP computation saved: {mlp_savings*100:.1f}%\")\n",
    "\n",
    "# # Show exit patterns\n",
    "# print(f\"\\nExit layers used: {exit_layer_idxs[0].tolist()}\")\n",
    "# if not exit_layer_idxs[0].isinf().all():\n",
    "#     print(f\"Average exit layer: {exit_layer_idxs[0][~exit_layer_idxs[0].isinf()].float().mean():.1f}\")\n",
    "\n",
    "# Clean up - remove all hooks\n",
    "tracker.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a8a1ae-31d8-467c-8879-2a180937612c",
   "metadata": {},
   "source": [
    "**Test: force to exit at last layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde58252-ebaf-4daa-bb7e-337720de834a",
   "metadata": {},
   "source": [
    "**Run teacher**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10950d19-7086-4933-872a-4f4ba02105f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 20])\n",
      "n_hooks=196\n",
      "Total attention operations: 11200\n",
      "Total MLP operations: 56000\n",
      "Attention ops per layer: {0: 400, 1: 400, 2: 400, 3: 400, 4: 400, 5: 400, 6: 400, 7: 400, 8: 400, 9: 400, 10: 400, 11: 400, 12: 400, 13: 400, 14: 400, 15: 400, 16: 400, 17: 400, 18: 400, 19: 400, 20: 400, 21: 400, 22: 400, 23: 400, 24: 400, 25: 400, 26: 400, 27: 400}\n",
      "MLP ops per layer: {0: 2000, 1: 2000, 2: 2000, 3: 2000, 4: 2000, 5: 2000, 6: 2000, 7: 2000, 8: 2000, 9: 2000, 10: 2000, 11: 2000, 12: 2000, 13: 2000, 14: 2000, 15: 2000, 16: 2000, 17: 2000, 18: 2000, 19: 2000, 20: 2000, 21: 2000, 22: 2000, 23: 2000, 24: 2000, 25: 2000, 26: 2000, 27: 2000}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain the concept of recursion in programming.\"\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\"\n",
    "\n",
    "set_transformer_early_exit_mode(model, 'sft_teacher')\n",
    "# Reset tracker for student mode\n",
    "tracker.reset()\n",
    "tracker.register_hooks(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sft_teacher_response, (sft_teacher_generated_tokens, \n",
    "                          sft_teacher_final_layer_logprobs, \n",
    "                          gathered_early_exit_hidden_states) = generate_text(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        prefiller=prefiller,\n",
    "        tokenizer=tokenizer,\n",
    "        generation_config=config['generation'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    early_output_log_probs = model.early_exit_hidden_state_readout(gathered_early_exit_hidden_states)\n",
    "    \n",
    "    early_exit_probs = model.early_exit_target_probs(\n",
    "       early_output_log_probs=early_output_log_probs,\n",
    "       teacher_final_layer_log_probs=sft_teacher_final_layer_logprobs\n",
    "    )\n",
    "    \n",
    "    \n",
    "print_computation_summary(tracker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aaa6ab-53ed-45d4-8d09-4c9b8341ab28",
   "metadata": {},
   "source": [
    "**Run student**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "887d1707-f2d7-45c2-9b00-fdf17de57b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting exit layers to inf for sft_student\n",
      "Minimum in prescribed_exit_layer_idxs = 25.0\n",
      "n_hooks=196\n",
      "Total attention operations: 28\n",
      "Total MLP operations: 140\n",
      "Attention ops per layer: {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1}\n",
      "MLP ops per layer: {0: 5, 1: 5, 2: 5, 3: 5, 4: 5, 5: 5, 6: 5, 7: 5, 8: 5, 9: 5, 10: 5, 11: 5, 12: 5, 13: 5, 14: 5, 15: 5, 16: 5, 17: 5, 18: 5, 19: 5, 20: 5, 21: 5, 22: 5, 23: 5, 24: 5, 25: 5, 26: 5, 27: 5}\n"
     ]
    }
   ],
   "source": [
    "# Reset tracker for student mode\n",
    "tracker.reset()\n",
    "tracker.register_hooks(model)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch, gen_len, elayers = early_exit_probs.shape \n",
    "    full_len = sft_teacher_generated_tokens.shape[1]\n",
    "    repeated_sft_teacher_generated_tokens = sft_teacher_generated_tokens.expand(num_exit_samples * batch, full_len)   \n",
    "    sampled_early_exit_layer_idxs_early_with_sample_dim = torch.distributions.Categorical(probs = early_exit_probs).sample((num_exit_samples,))     # [samples, batch, generation length] \n",
    "    sampled_early_exit_layer_idxs_early = sampled_early_exit_layer_idxs_early_with_sample_dim.reshape(batch * num_exit_samples, gen_len)            # [batch * samples, generation length]\n",
    "    sampled_early_exit_layer_idxs = model.exitable_layer_idxs[sampled_early_exit_layer_idxs_early.cpu()]                       \n",
    "    \n",
    "    \n",
    "    \n",
    "    set_transformer_early_exit_mode(model, 'sft_student')\n",
    "    \n",
    "    # Create prescribed exit layer idxs filled with torch.inf (always exit on last layer)\n",
    "    batch_samples, seq_len = repeated_sft_teacher_generated_tokens.shape\n",
    "    print(\"Setting exit layers to inf for sft_student\")\n",
    "    sampled_early_exit_layer_idxs = torch.full((batch_samples, gen_len), torch.inf, \\\n",
    "                                            device=repeated_sft_teacher_generated_tokens.device)\n",
    "    sampled_early_exit_layer_idxs = torch.zeros_like(sampled_early_exit_layer_idxs) + 25\n",
    "    print(f\"Minimum in prescribed_exit_layer_idxs = {torch.min(sampled_early_exit_layer_idxs)}\")\n",
    "    sft_student_output_scores, collected_exit_logits = model(repeated_sft_teacher_generated_tokens,\\\n",
    "                                                             prescribed_exit_layer_idxs=sampled_early_exit_layer_idxs)\n",
    "print_computation_summary(tracker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f0e3d-1d2b-40e3-b3c2-97e21e52f292",
   "metadata": {},
   "source": [
    "# (WIP) Test: modify early_exit_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e809ed92-c993-49f8-ae47-cc4b6e254e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biasing base_model.model.model.layers.0.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.5.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.10.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.15.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.20.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.25.early_exit_decision_weights with  -100\n"
     ]
    }
   ],
   "source": [
    "# model.base_model.model.model.layers[0].self_attn\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    bias_val = -100\n",
    "    if 'early_exit_decision_weights' in name:\n",
    "        print('biasing', name, 'with ', bias_val)\n",
    "        bias_tensor = bias_val + torch.zeros(module.bias.shape)\n",
    "        bias_tensor = bias_tensor.to(module.weight.device)\n",
    "        module.bias = torch.nn.Parameter(bias_tensor)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31d2feb0-d827-40ab-95aa-a2365c3aa18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biasing base_model.model.model.layers.0.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.5.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.10.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.15.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.20.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.25.early_exit_decision_weights with  -100\n"
     ]
    }
   ],
   "source": [
    "# model.base_model.model.model.layers[0].self_attn\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    bias_val = -100\n",
    "    if 'early_exit_decision_weights' in name:\n",
    "        print('biasing', name, 'with ', bias_val)\n",
    "        bias_tensor = bias_val + torch.zeros(module.bias.shape)\n",
    "        bias_tensor = bias_tensor.to(module.weight.device)\n",
    "        module.bias = torch.nn.Parameter(bias_tensor)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "235a6f2f-cc63-4d71-b4c2-0abbb00d7ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 2: STUDENT MODE (With Early Exits)\n",
      "============================================================\n",
      "\n",
      "Student Mode Summary:\n",
      "Total attention operations: 28\n",
      "Total MLP operations: 140\n",
      "Attention ops per layer: {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1}\n",
      "MLP ops per layer: {0: 5, 1: 5, 2: 5, 3: 5, 4: 5, 5: 5, 6: 5, 7: 5, 8: 5, 9: 5, 10: 5, 11: 5, 12: 5, 13: 5, 14: 5, 15: 5, 16: 5, 17: 5, 18: 5, 19: 5, 20: 5, 21: 5, 22: 5, 23: 5, 24: 5, 25: 5, 26: 5, 27: 5}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 2: STUDENT MODE (With Early Exits)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reset tracker for student mode\n",
    "tracker.reset()\n",
    "tracker.register_hooks(model)\n",
    "\n",
    "forward_student(model, sampled_early_exit_layer_idxs, repeated_sft_teacher_generated_tokens)\n",
    "\n",
    "student_summary = tracker.get_summary()\n",
    "print(\"\\nStudent Mode Summary:\")\n",
    "print(f\"Total attention operations: {student_summary['total_attention_ops']}\")\n",
    "print(f\"Total MLP operations: {student_summary['total_mlp_ops']}\")\n",
    "print(f\"Attention ops per layer: {student_summary['attention_ops']}\")\n",
    "print(f\"MLP ops per layer: {student_summary['mlp_ops']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "738327a9-ae1f-4217-97bb-e7f3d7fa7fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 20])\n",
      "Free generate: Patched forward generation called at  model.layers.0\n",
      "Free generate: Patched forward generation called at  model.layers.5\n",
      "Free generate: Patched forward generation called at  model.layers.10\n",
      "Free generate: Patched forward generation called at  model.layers.15\n",
      "Free generate: Patched forward generation called at  model.layers.20\n",
      "Free generate: Patched forward generation called at  model.layers.25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m prefiller = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m set_transformer_early_exit_mode(model, \u001b[33m'\u001b[39m\u001b[33mfree_generate\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m externalised_response, (externalised_generated_tokens, gathered_early_exit_layer_idxs) =\\\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefiller\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgeneration\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(externalised_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/externalization/tests/../shared_utils/generate.py:81\u001b[39m, in \u001b[36mgenerate_text\u001b[39m\u001b[34m(model, prompt, system_prompt, prefiller, tokenizer, generation_config, device)\u001b[39m\n\u001b[32m     79\u001b[39m inputs = full_tokenize(prompts=full_prompts, tokenizer=tokenizer, device = device)\n\u001b[32m     80\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mprompt tokens shape:\u001b[39m\u001b[33m'\u001b[39m, inputs[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].shape)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m all_model_outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m decoded_responses = tokenizer.decode(all_model_outputs[\u001b[32m0\u001b[39m].squeeze())\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_responses, all_model_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:1968\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1966\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m   1967\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1968\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1970\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/externalization/tests/../early_exit/patching/method_patching.py:123\u001b[39m, in \u001b[36mpatched_generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.early_exit_mode == \u001b[33m'\u001b[39m\u001b[33mfree_generate\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    122\u001b[39m     \u001b[38;5;28mself\u001b[39m._early_exit_logs: List[ExitLogger] = []\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     gathered_early_exit_layer_idxs = [ee.readout_layer_idx \u001b[38;5;28;01mfor\u001b[39;00m ee \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._early_exit_logs]\n\u001b[32m    125\u001b[39m     gathered_early_exit_layer_idxs = torch.tensor(gathered_early_exit_layer_idxs).T\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2597\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2589\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2590\u001b[39m         input_ids=input_ids,\n\u001b[32m   2591\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2592\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2593\u001b[39m         **model_kwargs,\n\u001b[32m   2594\u001b[39m     )\n\u001b[32m   2596\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2604\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2608\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:3557\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3554\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3556\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m3557\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3558\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/externalization/tests/../early_exit/patching/method_patching.py:38\u001b[39m, in \u001b[36mpatched_forward_generation\u001b[39m\u001b[34m(self, input_ids, *args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m module.early_exit_mode == \u001b[33m'\u001b[39m\u001b[33mfree_generate\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     36\u001b[39m         module.exit_state = exit_state\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mself\u001b[39m._early_exit_logs.append(exit_state)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# No early exit occurred\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py:703\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    698\u001b[39m output_hidden_states = (\n\u001b[32m    699\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    700\u001b[39m )\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    716\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    717\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py:436\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    434\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py:48\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1805\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1802\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1803\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1807\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1808\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1809\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1810\u001b[39m     ):\n\u001b[32m   1811\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/externalization/tests/../early_exit/patching/dynamical_types.py:94\u001b[39m, in \u001b[36mgenerate_layer_type_with_early_exit_decision_head.<locals>.DynamicallyTypedLayerWithExit.forward\u001b[39m\u001b[34m(self, hidden_states, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.patched_layer_forward(\n\u001b[32m     88\u001b[39m             hidden_states = hidden_states,\n\u001b[32m     89\u001b[39m             **kwargs,\n\u001b[32m     90\u001b[39m             unfrozen_idx_or_mask = \u001b[38;5;28mself\u001b[39m.exit_state.unfrozen_batch_items,\n\u001b[32m     91\u001b[39m         )\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatched_layer_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.early_exit_mode == \u001b[33m'\u001b[39m\u001b[33msft_student\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     99\u001b[39m     \n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# XXX: CHECK LOGIT COLLECTION ALIGNMENT BY TIME\u001b[39;00m\n\u001b[32m    101\u001b[39m     readout_logits: _T = \u001b[38;5;28mself\u001b[39m.early_exit_decision_weights(hidden_states[:,-\u001b[38;5;28mself\u001b[39m.exit_state.generation_length:]).squeeze(-\u001b[32m1\u001b[39m)       \u001b[38;5;66;03m# [B, S]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/externalization/tests/../early_exit/patching/attention_mixins/qwen2.py:60\u001b[39m, in \u001b[36mQwen2DecoderLayerFakeAttentionForwardMixin.patched_layer_forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, unfrozen_idx_or_mask, **kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m hidden_states[unfrozen_elements] = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states[unfrozen_elements])\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m hidden_states, self_attn_weights, present_key_value = \u001b[38;5;28mself\u001b[39m.self_attn(\n\u001b[32m     61\u001b[39m     hidden_states=hidden_states,\n\u001b[32m     62\u001b[39m     attention_mask=attention_mask,\n\u001b[32m     63\u001b[39m     position_ids=position_ids,\n\u001b[32m     64\u001b[39m     past_key_value=past_key_value,\n\u001b[32m     65\u001b[39m     output_attentions=output_attentions,\n\u001b[32m     66\u001b[39m     use_cache=use_cache,\n\u001b[32m     67\u001b[39m     cache_position=cache_position,\n\u001b[32m     68\u001b[39m     position_embeddings=position_embeddings,\n\u001b[32m     69\u001b[39m     unfrozen_idx_or_mask=unfrozen_idx_or_mask       \u001b[38;5;66;03m# Key change\u001b[39;00m\n\u001b[32m     70\u001b[39m )\n\u001b[32m     71\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me a Zen joke about farmer\"\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\"\n",
    "\n",
    "set_transformer_early_exit_mode(model, 'free_generate')\n",
    "externalised_response, (externalised_generated_tokens, gathered_early_exit_layer_idxs) =\\\n",
    "    generate_text(model, prompt, system_prompt, prefiller, tokenizer, config['generation'], device)\n",
    "print(externalised_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a2f38-9ec0-44c4-96d1-8a3d52351417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
