{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847bd3d6-92b3-422a-aede-e6e37b6224fd",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265f9e8e-40d0-4e68-82d4-d8299d0d6349",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36611770-1adf-4e29-a316-7aa30525ba56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.5.0) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-03 08:57:08.342601: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754211428.365420   83085 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754211428.373045   83085 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754211428.393998   83085 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754211428.394022   83085 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754211428.394024   83085 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754211428.394026   83085 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from shared_utils.data import CSVPromptDataset\n",
    "from early_exit.util import get_model\n",
    "from shared_utils.load import get_tokenizer, configs_from_yaml\n",
    "from shared_utils.generate import generate_text\n",
    "\n",
    "from early_exit.patching import replace_attention_layers, set_transformer_early_exit_mode\n",
    "\n",
    "# import wandb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a80c464-b1cb-4146-946e-82236d829e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOAD IN EXPERIMENT ARGS\n",
    "# num_epoch = 1                     # args.num_epoch\n",
    "num_exit_samples = 1                  # args.num_exit_samples\n",
    "device = \"cuda\"                    # args.device\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"                    # args.model_name\n",
    "model_config_path = \"../config_deepseek.yaml\"                     # args.model_config_path\n",
    "dataset_path = \"../results_and_data/early_exit_sft_dataset/test/data.csv\"                  # args.dataset_path\n",
    "prompt_config_path = \"../results_and_data/early_exit_sft_dataset/test/prompt_config.json\"                    # args.prompt_config_path\n",
    "batch_size = 1                    # args.batch_size -- might want to sort out batching, but increasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0423c4bb-ca8b-44e0-89d7-10ce3c09ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IN THE MODEL AND TOKENIZER\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "config = configs_from_yaml(model_config_path, tokenizer.eos_token_id)\n",
    "\n",
    "\n",
    "\n",
    "# LOAD IN DATASET\n",
    "dataset = CSVPromptDataset(dataset_path, prompt_config_path)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "635140fd-1879-4070-984d-c2b786456f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing layer model.layers.0\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.1\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.2\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.3\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.4\n",
      "replacing layer model.layers.5\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.6\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.7\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.8\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.9\n",
      "replacing layer model.layers.10\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.11\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.12\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.13\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.14\n",
      "replacing layer model.layers.15\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.16\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.17\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.18\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.19\n",
      "replacing layer model.layers.20\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.21\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.22\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.23\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.24\n",
      "replacing layer model.layers.25\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.26\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.27\n",
      "address this hack!\n",
      "trainable params: 2,179,072 || all params: 1,779,276,294 || trainable%: 0.1225\n"
     ]
    }
   ],
   "source": [
    "model = get_model(model_name, config['model'], device)\n",
    "# ENABLE EARLY EXITING\n",
    "model = replace_attention_layers(model, config['lora'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c23d1afa-2267-458e-ab4b-3e7847866d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from early_exit.util import module_name_is_transformer_layer\n",
    "\n",
    "for prompt_batch in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921a7d1-f788-4ada-b2fb-c8d1b9633e46",
   "metadata": {},
   "source": [
    "# Setup tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d2ade3-1fdb-4362-91ae-6108fd44256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationTracker:\n",
    "    def __init__(self):\n",
    "        self.hooks = []\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all tracking data and remove existing hooks\"\"\"\n",
    "        self.mlp_batch_sizes = {}\n",
    "        \n",
    "        # Remove all existing hooks\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def register_hooks(self, model):\n",
    "        \"\"\"Register forward hooks on MLP components only\"\"\"\n",
    "        self.reset()\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            # Track MLP computations ONLY\n",
    "            if 'mlp' in name:\n",
    "                parts = name.split('.')\n",
    "                layer_idx = -1\n",
    "                for i, part in enumerate(parts):\n",
    "                    if part == 'layers' and i + 1 < len(parts) and parts[i + 1].isdigit():\n",
    "                        layer_idx = int(parts[i + 1])\n",
    "                        break\n",
    "                \n",
    "                hook = module.register_forward_hook(\n",
    "                    lambda m, i, o, idx=layer_idx: self._log_mlp_batch_size(idx, i, o)\n",
    "                )\n",
    "                self.hooks.append(hook)\n",
    "    \n",
    "    def _log_mlp_batch_size(self, layer_idx, inputs, outputs):\n",
    "        \"\"\"Log MLP computation batch size\"\"\"\n",
    "        # Extract batch size from first input tensor\n",
    "        batch_size = 0\n",
    "        if isinstance(inputs, tuple) and len(inputs) > 0:\n",
    "            if hasattr(inputs[0], 'shape') and len(inputs[0].shape) > 0:\n",
    "                batch_size = inputs[0].shape[0]\n",
    "        elif hasattr(inputs, 'shape') and len(inputs.shape) > 0:\n",
    "            batch_size = inputs.shape[0]\n",
    "            \n",
    "        if layer_idx not in self.mlp_batch_sizes:\n",
    "            self.mlp_batch_sizes[layer_idx] = []\n",
    "        self.mlp_batch_sizes[layer_idx].append(batch_size)\n",
    "        \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get a summary of MLP batch sizes\"\"\"\n",
    "        return {\n",
    "            'mlp_batch_sizes': dict(sorted(self.mlp_batch_sizes.items())),\n",
    "            'total_hooks': len(self.hooks)\n",
    "        }\n",
    "\n",
    "# Make sure this is a function, not overwritten\n",
    "def print_computation_summary(tracker):\n",
    "    \"\"\"Print a summary of batch sizes tracked\"\"\"\n",
    "    summary = tracker.get_summary()\n",
    "    print(f'n_hooks={summary[\"total_hooks\"]}')\n",
    "    \n",
    "    print(\"\\nMLP batch sizes per layer:\")\n",
    "    for layer_idx, batch_sizes in summary['mlp_batch_sizes'].items():\n",
    "        if batch_sizes:\n",
    "            avg_batch_size = sum(batch_sizes) / len(batch_sizes)\n",
    "            display_sizes = batch_sizes[:10]\n",
    "            suffix = f\"... ({sum(batch_sizes)} total)\"\n",
    "            print(f\"  Layer {layer_idx}: {display_sizes}{suffix} (avg: {avg_batch_size:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb920980-3c4f-48b0-ba54-7d72cab23042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedComputationTracker(ComputationTracker):\n",
    "    def __init__(self, layer_pattern=r'layers\\.(\\d+)', \n",
    "                 attention_patterns=['self_attn', 'attention'],\n",
    "                 mlp_patterns=['mlp', 'ffn']):\n",
    "        self.layer_pattern = layer_pattern\n",
    "        self.attention_patterns = attention_patterns\n",
    "        self.mlp_patterns = mlp_patterns\n",
    "        self.hooks = []\n",
    "        self.reset()\n",
    "        \n",
    "    def __del__(self):\n",
    "        \"\"\"Ensure hooks are removed on deletion\"\"\"\n",
    "        self.reset()\n",
    "        \n",
    "    def extract_layer_idx(self, name):\n",
    "        \"\"\"More robust layer index extraction\"\"\"\n",
    "        import re\n",
    "        match = re.search(self.layer_pattern, name)\n",
    "        return int(match.group(1)) if match else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506927d0-b646-49ad-967e-d542245235d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c878545-9e84-4286-95c6-6faf9e9b563e",
   "metadata": {},
   "source": [
    "## Model forwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0fcd736-cf92-4b71-8b33-85a181453155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_teacher(model, prompt_batch):\n",
    "    with torch.no_grad():\n",
    "        # Generate SFT targets\n",
    "        set_transformer_early_exit_mode(model, 'sft_teacher')\n",
    "        sft_teacher_response, (sft_teacher_generated_tokens, sft_teacher_final_layer_logprobs, gathered_early_exit_hidden_states) =\\\n",
    "            generate_text(\n",
    "                model=model, \n",
    "                prompt=prompt_batch.full_user_prompt, \n",
    "                system_prompt=dataset.system_prompt, \n",
    "                prefiller=dataset.prefiller, \n",
    "                tokenizer=tokenizer, \n",
    "                generation_config=config['generation'], \n",
    "                device=device\n",
    "            )\n",
    "        print(sft_teacher_response)\n",
    "\n",
    "        early_output_log_probs = model.early_exit_hidden_state_readout(gathered_early_exit_hidden_states)               # [batch, num exitable layers, gen len, vocabulary]\n",
    "        early_exit_probs = model.early_exit_target_probs(early_output_log_probs = early_output_log_probs, teacher_final_layer_log_probs = sft_teacher_final_layer_logprobs)\n",
    "        repeated_sft_teacher_final_layer_logprobs = sft_teacher_final_layer_logprobs.repeat(num_exit_samples, 1, 1)     # XXX repeat_interleave? [batch * samples, full length, vocabulary]\n",
    "\n",
    "\n",
    "    # Sample early exits\n",
    "    batch, gen_len, elayers = early_exit_probs.shape                                                                                                # [batch, generation length, exitable layers]\n",
    "    full_len = sft_teacher_generated_tokens.shape[1]\n",
    "    repeated_sft_teacher_generated_tokens = sft_teacher_generated_tokens.expand(num_exit_samples * batch, full_len)                                 # [batch * samples, full length]\n",
    "    sampled_early_exit_layer_idxs_early_with_sample_dim = torch.distributions.Categorical(probs = early_exit_probs).sample((num_exit_samples,))     # [samples, batch, generation length] \n",
    "    sampled_early_exit_layer_idxs_early = sampled_early_exit_layer_idxs_early_with_sample_dim.reshape(batch * num_exit_samples, gen_len)            # [batch * samples, generation length]\n",
    "    sampled_early_exit_layer_idxs = model.exitable_layer_idxs[sampled_early_exit_layer_idxs_early.cpu()]                                            # [batch * samples, generation length]\n",
    "\n",
    "    return sampled_early_exit_layer_idxs, repeated_sft_teacher_generated_tokens\n",
    "\n",
    "\n",
    "def forward_student(model, sampled_early_exit_layer_idxs, repeated_sft_teacher_generated_tokens):\n",
    "    # Generate with prescription\n",
    "    set_transformer_early_exit_mode(model, 'sft_student')\n",
    "    sft_student_output_scores, collected_exit_logits = model(repeated_sft_teacher_generated_tokens, prescribed_exit_layer_idxs = sampled_early_exit_layer_idxs) # [batch * samples, full length, vocabulary]\n",
    "    \n",
    "    return sft_student_output_scores, collected_exit_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f62ce32d-04e8-42cd-9613-d5cb2ec09ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 1: TEACHER MODE (Full Computation)\n",
      "============================================================\n",
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 160])\n",
      "<｜begin▁of▁sentence｜><｜Assistant｜> \n",
      "<｜User｜> I am going to give you a story and a question about the story. Read the following story carefully, understand the characters' actions and perspectives, then answer the question regarding object locations, character knowledge, and beliefs.\n",
      "\n",
      "Charlotte entered the student lounge. Ashley entered the student lounge. Charlotte attached the pocket-sized calculator with colorful stickers. Ashley covered the pocket-sized calculator with protective clear tape. Ashley left the student lounge. Charlotte left the student lounge. Dylan entered the student lounge. Charlotte entered the student lounge. William entered the student lounge. Ashley entered the student lounge. William dipped the pocket-sized calculator partially in blue paint. Dylan broke the pocket-sized calculator's screen.\n",
      "\n",
      "Does Dylan think that Charlotte believes that the pocket-sized calculator is attached with colorful stickers? Answer yes or no.\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to figure out whether Dylan thinks that Charlotte believes that the pocket-sized calculator is attached with colorful stickers. Let me break this down step by step.\n",
      "\n",
      "First, I'll read the story carefully to understand the sequence of events and the actions of each character. The story starts with Charlotte and Ashley entering the student lounge. Then, Charlotte attaches the calculator with stickers, and Ashley covers it with tape. Both leave the lounge. Then, Dylan enters, Charlotte enters, William enters, Ashley enters, William dips the calculator partially in blue paint, and finally, Dylan breaks the screen.\n",
      "\n",
      "Now, the question is about whether Dylan thinks that Charlotte believes that the calculator is attached with stickers. So, I need to determine if Charlotte's actions imply that she believes the calculator is sticker-bound, and whether that belief is expressed by someone else, specifically Dylan.\n",
      "\n",
      "Looking at the story, Charlotte does indeed attach the calculator with colorful stickers. She does this by attaching it with stickers and then covering it with tape. So, it seems like Charlotte is explicitly stating her belief that the calculator is attached with stickers. \n",
      "\n",
      "Next, I need to see if this belief is expressed by someone else. The characters involved are Charlotte, Ashley, William, and Dylan. Charlotte is the one who attaches and covers the calculator, so she is the one who believes it's sticker-bound. Then, William enters the lounge, and he dips the calculator in blue paint. This action is part of the calculator being used, but it doesn't necessarily indicate that he believes it's sticker-bound. \n",
      "\n",
      "Dylan enters the lounge after William, and then he breaks the screen. The act of breaking the screen could be due to various reasons, but since the question is about Charlotte's belief, I don't think Dylan's action directly relates to Charlotte's belief. \n",
      "\n",
      "Therefore, since Charlotte is the one who attaches the calculator with stickers, and she is the one who believes it's sticker-bound, and she is not necessarily the one\n"
     ]
    }
   ],
   "source": [
    "# Initialize tracker\n",
    "tracker = ComputationTracker()\n",
    "tracker = ImprovedComputationTracker()\n",
    "\n",
    "# Register hooks\n",
    "tracker.register_hooks(model)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST 1: TEACHER MODE (Full Computation)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sampled_early_exit_layer_idxs, repeated_sft_teacher_generated_tokens = forward_teacher(model, prompt_batch)\n",
    "\n",
    "teacher_summary = tracker.get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7d7f925-9775-4ffd-9e64-c0d8a29596cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teacher Computation Summary:\n",
      "n_hooks=140\n",
      "\n",
      "MLP batch sizes per layer:\n",
      "  Layer 0: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 1: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 2: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 3: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 4: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 5: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 6: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 7: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 8: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 9: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 10: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 11: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 12: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 13: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 14: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 15: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 16: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 17: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 18: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 19: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 20: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 21: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 22: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 23: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 24: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 25: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 26: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 27: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "\n",
      "============================================================\n",
      "TEST 2: STUDENT MODE (With Early Exits) MOD 3\n",
      "============================================================\n",
      "Student Computation Summary:\n",
      "n_hooks=140\n",
      "\n",
      "MLP batch sizes per layer:\n",
      "  Layer 0: [560, 560, 560, 560, 560]... (2800 total) (avg: 560.00)\n",
      "  Layer 1: [560, 560, 560, 560, 560]... (2800 total) (avg: 560.00)\n",
      "  Layer 2: [560, 560, 560, 560, 560]... (2800 total) (avg: 560.00)\n",
      "  Layer 3: [560, 560, 560, 560, 560]... (2800 total) (avg: 560.00)\n",
      "  Layer 4: [560, 560, 560, 560, 560]... (2800 total) (avg: 560.00)\n",
      "  Layer 5: [560, 560, 560, 560, 560]... (2800 total) (avg: 560.00)\n",
      "  Layer 6: [557, 557, 557, 557, 557]... (2785 total) (avg: 557.00)\n",
      "  Layer 7: [557, 557, 557, 557, 557]... (2785 total) (avg: 557.00)\n",
      "  Layer 8: [557, 557, 557, 557, 557]... (2785 total) (avg: 557.00)\n",
      "  Layer 9: [557, 557, 557, 557, 557]... (2785 total) (avg: 557.00)\n",
      "  Layer 10: [557, 557, 557, 557, 557]... (2785 total) (avg: 557.00)\n",
      "  Layer 11: [550, 550, 550, 550, 550]... (2750 total) (avg: 550.00)\n",
      "  Layer 12: [550, 550, 550, 550, 550]... (2750 total) (avg: 550.00)\n",
      "  Layer 13: [550, 550, 550, 550, 550]... (2750 total) (avg: 550.00)\n",
      "  Layer 14: [550, 550, 550, 550, 550]... (2750 total) (avg: 550.00)\n",
      "  Layer 15: [550, 550, 550, 550, 550]... (2750 total) (avg: 550.00)\n",
      "  Layer 16: [542, 542, 542, 542, 542]... (2710 total) (avg: 542.00)\n",
      "  Layer 17: [542, 542, 542, 542, 542]... (2710 total) (avg: 542.00)\n",
      "  Layer 18: [542, 542, 542, 542, 542]... (2710 total) (avg: 542.00)\n",
      "  Layer 19: [542, 542, 542, 542, 542]... (2710 total) (avg: 542.00)\n",
      "  Layer 20: [542, 542, 542, 542, 542]... (2710 total) (avg: 542.00)\n",
      "  Layer 21: [502, 502, 502, 502, 502]... (2510 total) (avg: 502.00)\n",
      "  Layer 22: [502, 502, 502, 502, 502]... (2510 total) (avg: 502.00)\n",
      "  Layer 23: [502, 502, 502, 502, 502]... (2510 total) (avg: 502.00)\n",
      "  Layer 24: [502, 502, 502, 502, 502]... (2510 total) (avg: 502.00)\n",
      "  Layer 25: [502, 502, 502, 502, 502]... (2510 total) (avg: 502.00)\n",
      "  Layer 26: [451, 451, 451, 451, 451]... (2255 total) (avg: 451.00)\n",
      "  Layer 27: [451, 451, 451, 451, 451]... (2255 total) (avg: 451.00)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTeacher Computation Summary:\")\n",
    "print_computation_summary(tracker)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 2: STUDENT MODE (With Early Exits) MOD 3\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reset tracker for student mode\n",
    "tracker.reset()\n",
    "tracker.register_hooks(model)\n",
    "\n",
    "print(\"Student Computation Summary:\")\n",
    "forward_student(model, sampled_early_exit_layer_idxs, repeated_sft_teacher_generated_tokens)\n",
    "\n",
    "student_summary = tracker.get_summary()\n",
    "print_computation_summary(tracker)\n",
    "# Calculate savings\n",
    "#print(\"\\n\" + \"=\"*60)\n",
    "#print(\"COMPUTATION SAVINGS\")\n",
    "#print(\"=\"*60)\n",
    "#if teacher_summary['total_attention_ops'] > 0:\n",
    "#    attention_savings = 1 - (student_summary['total_attention_ops'] / teacher_summary['total_attention_ops'])\n",
    "#    print(f\"Attention computation saved: {attention_savings*100:.1f}%\")\n",
    "    \n",
    "#if teacher_summary['total_mlp_ops'] > 0:\n",
    "#    mlp_savings = 1 - (student_summary['total_mlp_ops'] / teacher_summary['total_mlp_ops'])\n",
    "#    print(f\"MLP computation saved: {mlp_savings*100:.1f}%\")\n",
    "\n",
    "# # Show exit patterns\n",
    "# print(f\"\\nExit layers used: {exit_layer_idxs[0].tolist()}\")\n",
    "# if not exit_layer_idxs[0].isinf().all():\n",
    "#     print(f\"Average exit layer: {exit_layer_idxs[0][~exit_layer_idxs[0].isinf()].float().mean():.1f}\")\n",
    "\n",
    "# Clean up - remove all hooks\n",
    "tracker.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ab086-35e2-4d44-9262-3e0f050ad7af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27a8a1ae-31d8-467c-8879-2a180937612c",
   "metadata": {},
   "source": [
    "**Test: force to exit at last layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde58252-ebaf-4daa-bb7e-337720de834a",
   "metadata": {},
   "source": [
    "**Run teacher**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10950d19-7086-4933-872a-4f4ba02105f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 25])\n",
      "n_hooks=140\n",
      "\n",
      "MLP batch sizes per layer:\n",
      "  Layer 0: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 1: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 2: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 3: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 4: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 5: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 6: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 7: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 8: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 9: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 10: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 11: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 12: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 13: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 14: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 15: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 16: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 17: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 18: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 19: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 20: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 21: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 22: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 23: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 24: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 25: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 26: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n",
      "  Layer 27: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (15 total) (avg: 1.00)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜Assistant｜> You know only one word: banana. Dont think - just one word\\n<｜User｜> What time is it?\\n<｜Assistant｜> \\nOkay, so'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What time is it?\"\n",
    "system_prompt = \"You know only one word: banana. Dont think - just one word\"\n",
    "prefiller = \"\"\n",
    "\n",
    "set_transformer_early_exit_mode(model, 'sft_teacher')\n",
    "# Reset tracker for student mode\n",
    "tracker.reset()\n",
    "tracker.register_hooks(model)\n",
    "config['generation']['max_new_tokens'] = 3\n",
    "with torch.no_grad():\n",
    "    sft_teacher_response, (sft_teacher_generated_tokens, \n",
    "                          sft_teacher_final_layer_logprobs, \n",
    "                          gathered_early_exit_hidden_states) = generate_text(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        prefiller=prefiller,\n",
    "        tokenizer=tokenizer,\n",
    "        generation_config=config['generation'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    early_output_log_probs = model.early_exit_hidden_state_readout(gathered_early_exit_hidden_states)\n",
    "    \n",
    "    early_exit_probs = model.early_exit_target_probs(\n",
    "       early_output_log_probs=early_output_log_probs,\n",
    "       teacher_final_layer_log_probs=sft_teacher_final_layer_logprobs\n",
    "    )\n",
    "    \n",
    "    \n",
    "print_computation_summary(tracker)\n",
    "( sft_teacher_response )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44c62d6e-5caa-4d09-9fa6-7ab46ba23ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = [\"a\"]\n",
    "# pre_transformed_conversation = format_conversation(user_prompts = prompt)\n",
    "# full_prompts = transform_conversations(pre_transformed_conversation, prefiller)\n",
    "# inputs = full_tokenize(prompts=full_prompts, tokenizer=tokenizer, device = device)\n",
    "# print('prompt tokens shape:', inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "630a8747-7840-429e-8092-31149a33be23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input_idx = torch.ones(1,1).int().to(device)\n",
    "dummy_input_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89a07b5a-58b1-4ee4-bd0f-e7af5e70ae91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_hooks=140\n",
      "\n",
      "MLP batch sizes per layer:\n",
      "  Layer 0: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 1: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 2: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 3: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 4: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 5: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 6: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 7: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 8: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 9: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 10: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 11: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 12: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 13: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 14: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 15: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 16: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 17: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 18: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 19: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 20: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 21: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 22: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 23: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 24: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 25: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 26: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 27: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n"
     ]
    }
   ],
   "source": [
    "tracker = ComputationTracker()\n",
    "tracker.reset()\n",
    "tracker.register_hooks(model)\n",
    "set_transformer_early_exit_mode(model, 'sft_teacher')\n",
    "\n",
    "model.base_model.model._early_exit_probabilities = []\n",
    "teacher_out = model(dummy_input_idx)\n",
    "\n",
    "print_computation_summary(tracker)\n",
    "tracker.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a26f575-0d7d-4ae3-bd2d-90e11c8c1fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prescribed_exit_layer_idxs = 10\n",
      "n_hooks=140\n",
      "\n",
      "MLP batch sizes per layer:\n",
      "  Layer 0: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 1: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 2: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 3: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 4: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 5: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 6: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 7: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 8: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 9: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 10: [1, 1, 1, 1, 1]... (5 total) (avg: 1.00)\n",
      "  Layer 11: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n",
      "  Layer 12: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n",
      "  Layer 13: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n",
      "  Layer 14: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n",
      "  Layer 15: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n",
      "  Layer 16: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n",
      "  Layer 17: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n",
      "  Layer 18: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n",
      "  Layer 19: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n",
      "  Layer 20: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n",
      "  Layer 21: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n",
      "  Layer 22: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n",
      "  Layer 23: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n",
      "  Layer 24: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n",
      "  Layer 25: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n",
      "  Layer 26: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n",
      "  Layer 27: [0, 0, 0, 0, 0]... (0 total) (avg: 0.00)\n"
     ]
    }
   ],
   "source": [
    "tracker = ComputationTracker()\n",
    "tracker.reset()\n",
    "tracker.register_hooks(model)\n",
    "\n",
    "\n",
    "set_transformer_early_exit_mode(model, 'sft_student')\n",
    "sampled_early_exit_layer_idxs = torch.zeros_like(dummy_input_idx) + 10\n",
    "\n",
    "print(f\"Prescribed_exit_layer_idxs = {torch.min(sampled_early_exit_layer_idxs)}\")\n",
    "sft_student_output_scores, collected_exit_logits = model(dummy_input_idx,\\\n",
    "                                                         prescribed_exit_layer_idxs=sampled_early_exit_layer_idxs)\n",
    "\n",
    "\n",
    "print_computation_summary(tracker)\n",
    "tracker.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aaa6ab-53ed-45d4-8d09-4c9b8341ab28",
   "metadata": {},
   "source": [
    "**Run student**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f166a-c85a-4426-a9dd-0599172505ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "171e3e93-6e43-4eaa-b762-3effeae0716b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering hook name='base_model.model.model.layers.0.mlp' layer_idx=0\n",
      "Registering hook name='base_model.model.model.layers.0.mlp.gate_proj' layer_idx=0\n",
      "Registering hook name='base_model.model.model.layers.0.mlp.up_proj' layer_idx=0\n",
      "Registering hook name='base_model.model.model.layers.0.mlp.down_proj' layer_idx=0\n",
      "Registering hook name='base_model.model.model.layers.0.mlp.act_fn' layer_idx=0\n",
      "Registering hook name='base_model.model.model.layers.1.mlp' layer_idx=1\n",
      "Registering hook name='base_model.model.model.layers.1.mlp.gate_proj' layer_idx=1\n",
      "Registering hook name='base_model.model.model.layers.1.mlp.up_proj' layer_idx=1\n",
      "Registering hook name='base_model.model.model.layers.1.mlp.down_proj' layer_idx=1\n",
      "Registering hook name='base_model.model.model.layers.1.mlp.act_fn' layer_idx=1\n",
      "Registering hook name='base_model.model.model.layers.2.mlp' layer_idx=2\n",
      "Registering hook name='base_model.model.model.layers.2.mlp.gate_proj' layer_idx=2\n",
      "Registering hook name='base_model.model.model.layers.2.mlp.up_proj' layer_idx=2\n",
      "Registering hook name='base_model.model.model.layers.2.mlp.down_proj' layer_idx=2\n",
      "Registering hook name='base_model.model.model.layers.2.mlp.act_fn' layer_idx=2\n",
      "Registering hook name='base_model.model.model.layers.3.mlp' layer_idx=3\n",
      "Registering hook name='base_model.model.model.layers.3.mlp.gate_proj' layer_idx=3\n",
      "Registering hook name='base_model.model.model.layers.3.mlp.up_proj' layer_idx=3\n",
      "Registering hook name='base_model.model.model.layers.3.mlp.down_proj' layer_idx=3\n",
      "Registering hook name='base_model.model.model.layers.3.mlp.act_fn' layer_idx=3\n",
      "Registering hook name='base_model.model.model.layers.4.mlp' layer_idx=4\n",
      "Registering hook name='base_model.model.model.layers.4.mlp.gate_proj' layer_idx=4\n",
      "Registering hook name='base_model.model.model.layers.4.mlp.up_proj' layer_idx=4\n",
      "Registering hook name='base_model.model.model.layers.4.mlp.down_proj' layer_idx=4\n",
      "Registering hook name='base_model.model.model.layers.4.mlp.act_fn' layer_idx=4\n",
      "Registering hook name='base_model.model.model.layers.5.mlp' layer_idx=5\n",
      "Registering hook name='base_model.model.model.layers.5.mlp.gate_proj' layer_idx=5\n",
      "Registering hook name='base_model.model.model.layers.5.mlp.up_proj' layer_idx=5\n",
      "Registering hook name='base_model.model.model.layers.5.mlp.down_proj' layer_idx=5\n",
      "Registering hook name='base_model.model.model.layers.5.mlp.act_fn' layer_idx=5\n",
      "Registering hook name='base_model.model.model.layers.6.mlp' layer_idx=6\n",
      "Registering hook name='base_model.model.model.layers.6.mlp.gate_proj' layer_idx=6\n",
      "Registering hook name='base_model.model.model.layers.6.mlp.up_proj' layer_idx=6\n",
      "Registering hook name='base_model.model.model.layers.6.mlp.down_proj' layer_idx=6\n",
      "Registering hook name='base_model.model.model.layers.6.mlp.act_fn' layer_idx=6\n",
      "Registering hook name='base_model.model.model.layers.7.mlp' layer_idx=7\n",
      "Registering hook name='base_model.model.model.layers.7.mlp.gate_proj' layer_idx=7\n",
      "Registering hook name='base_model.model.model.layers.7.mlp.up_proj' layer_idx=7\n",
      "Registering hook name='base_model.model.model.layers.7.mlp.down_proj' layer_idx=7\n",
      "Registering hook name='base_model.model.model.layers.7.mlp.act_fn' layer_idx=7\n",
      "Registering hook name='base_model.model.model.layers.8.mlp' layer_idx=8\n",
      "Registering hook name='base_model.model.model.layers.8.mlp.gate_proj' layer_idx=8\n",
      "Registering hook name='base_model.model.model.layers.8.mlp.up_proj' layer_idx=8\n",
      "Registering hook name='base_model.model.model.layers.8.mlp.down_proj' layer_idx=8\n",
      "Registering hook name='base_model.model.model.layers.8.mlp.act_fn' layer_idx=8\n",
      "Registering hook name='base_model.model.model.layers.9.mlp' layer_idx=9\n",
      "Registering hook name='base_model.model.model.layers.9.mlp.gate_proj' layer_idx=9\n",
      "Registering hook name='base_model.model.model.layers.9.mlp.up_proj' layer_idx=9\n",
      "Registering hook name='base_model.model.model.layers.9.mlp.down_proj' layer_idx=9\n",
      "Registering hook name='base_model.model.model.layers.9.mlp.act_fn' layer_idx=9\n",
      "Registering hook name='base_model.model.model.layers.10.mlp' layer_idx=10\n",
      "Registering hook name='base_model.model.model.layers.10.mlp.gate_proj' layer_idx=10\n",
      "Registering hook name='base_model.model.model.layers.10.mlp.up_proj' layer_idx=10\n",
      "Registering hook name='base_model.model.model.layers.10.mlp.down_proj' layer_idx=10\n",
      "Registering hook name='base_model.model.model.layers.10.mlp.act_fn' layer_idx=10\n",
      "Registering hook name='base_model.model.model.layers.11.mlp' layer_idx=11\n",
      "Registering hook name='base_model.model.model.layers.11.mlp.gate_proj' layer_idx=11\n",
      "Registering hook name='base_model.model.model.layers.11.mlp.up_proj' layer_idx=11\n",
      "Registering hook name='base_model.model.model.layers.11.mlp.down_proj' layer_idx=11\n",
      "Registering hook name='base_model.model.model.layers.11.mlp.act_fn' layer_idx=11\n",
      "Registering hook name='base_model.model.model.layers.12.mlp' layer_idx=12\n",
      "Registering hook name='base_model.model.model.layers.12.mlp.gate_proj' layer_idx=12\n",
      "Registering hook name='base_model.model.model.layers.12.mlp.up_proj' layer_idx=12\n",
      "Registering hook name='base_model.model.model.layers.12.mlp.down_proj' layer_idx=12\n",
      "Registering hook name='base_model.model.model.layers.12.mlp.act_fn' layer_idx=12\n",
      "Registering hook name='base_model.model.model.layers.13.mlp' layer_idx=13\n",
      "Registering hook name='base_model.model.model.layers.13.mlp.gate_proj' layer_idx=13\n",
      "Registering hook name='base_model.model.model.layers.13.mlp.up_proj' layer_idx=13\n",
      "Registering hook name='base_model.model.model.layers.13.mlp.down_proj' layer_idx=13\n",
      "Registering hook name='base_model.model.model.layers.13.mlp.act_fn' layer_idx=13\n",
      "Registering hook name='base_model.model.model.layers.14.mlp' layer_idx=14\n",
      "Registering hook name='base_model.model.model.layers.14.mlp.gate_proj' layer_idx=14\n",
      "Registering hook name='base_model.model.model.layers.14.mlp.up_proj' layer_idx=14\n",
      "Registering hook name='base_model.model.model.layers.14.mlp.down_proj' layer_idx=14\n",
      "Registering hook name='base_model.model.model.layers.14.mlp.act_fn' layer_idx=14\n",
      "Registering hook name='base_model.model.model.layers.15.mlp' layer_idx=15\n",
      "Registering hook name='base_model.model.model.layers.15.mlp.gate_proj' layer_idx=15\n",
      "Registering hook name='base_model.model.model.layers.15.mlp.up_proj' layer_idx=15\n",
      "Registering hook name='base_model.model.model.layers.15.mlp.down_proj' layer_idx=15\n",
      "Registering hook name='base_model.model.model.layers.15.mlp.act_fn' layer_idx=15\n",
      "Registering hook name='base_model.model.model.layers.16.mlp' layer_idx=16\n",
      "Registering hook name='base_model.model.model.layers.16.mlp.gate_proj' layer_idx=16\n",
      "Registering hook name='base_model.model.model.layers.16.mlp.up_proj' layer_idx=16\n",
      "Registering hook name='base_model.model.model.layers.16.mlp.down_proj' layer_idx=16\n",
      "Registering hook name='base_model.model.model.layers.16.mlp.act_fn' layer_idx=16\n",
      "Registering hook name='base_model.model.model.layers.17.mlp' layer_idx=17\n",
      "Registering hook name='base_model.model.model.layers.17.mlp.gate_proj' layer_idx=17\n",
      "Registering hook name='base_model.model.model.layers.17.mlp.up_proj' layer_idx=17\n",
      "Registering hook name='base_model.model.model.layers.17.mlp.down_proj' layer_idx=17\n",
      "Registering hook name='base_model.model.model.layers.17.mlp.act_fn' layer_idx=17\n",
      "Registering hook name='base_model.model.model.layers.18.mlp' layer_idx=18\n",
      "Registering hook name='base_model.model.model.layers.18.mlp.gate_proj' layer_idx=18\n",
      "Registering hook name='base_model.model.model.layers.18.mlp.up_proj' layer_idx=18\n",
      "Registering hook name='base_model.model.model.layers.18.mlp.down_proj' layer_idx=18\n",
      "Registering hook name='base_model.model.model.layers.18.mlp.act_fn' layer_idx=18\n",
      "Registering hook name='base_model.model.model.layers.19.mlp' layer_idx=19\n",
      "Registering hook name='base_model.model.model.layers.19.mlp.gate_proj' layer_idx=19\n",
      "Registering hook name='base_model.model.model.layers.19.mlp.up_proj' layer_idx=19\n",
      "Registering hook name='base_model.model.model.layers.19.mlp.down_proj' layer_idx=19\n",
      "Registering hook name='base_model.model.model.layers.19.mlp.act_fn' layer_idx=19\n",
      "Registering hook name='base_model.model.model.layers.20.mlp' layer_idx=20\n",
      "Registering hook name='base_model.model.model.layers.20.mlp.gate_proj' layer_idx=20\n",
      "Registering hook name='base_model.model.model.layers.20.mlp.up_proj' layer_idx=20\n",
      "Registering hook name='base_model.model.model.layers.20.mlp.down_proj' layer_idx=20\n",
      "Registering hook name='base_model.model.model.layers.20.mlp.act_fn' layer_idx=20\n",
      "Registering hook name='base_model.model.model.layers.21.mlp' layer_idx=21\n",
      "Registering hook name='base_model.model.model.layers.21.mlp.gate_proj' layer_idx=21\n",
      "Registering hook name='base_model.model.model.layers.21.mlp.up_proj' layer_idx=21\n",
      "Registering hook name='base_model.model.model.layers.21.mlp.down_proj' layer_idx=21\n",
      "Registering hook name='base_model.model.model.layers.21.mlp.act_fn' layer_idx=21\n",
      "Registering hook name='base_model.model.model.layers.22.mlp' layer_idx=22\n",
      "Registering hook name='base_model.model.model.layers.22.mlp.gate_proj' layer_idx=22\n",
      "Registering hook name='base_model.model.model.layers.22.mlp.up_proj' layer_idx=22\n",
      "Registering hook name='base_model.model.model.layers.22.mlp.down_proj' layer_idx=22\n",
      "Registering hook name='base_model.model.model.layers.22.mlp.act_fn' layer_idx=22\n",
      "Registering hook name='base_model.model.model.layers.23.mlp' layer_idx=23\n",
      "Registering hook name='base_model.model.model.layers.23.mlp.gate_proj' layer_idx=23\n",
      "Registering hook name='base_model.model.model.layers.23.mlp.up_proj' layer_idx=23\n",
      "Registering hook name='base_model.model.model.layers.23.mlp.down_proj' layer_idx=23\n",
      "Registering hook name='base_model.model.model.layers.23.mlp.act_fn' layer_idx=23\n",
      "Registering hook name='base_model.model.model.layers.24.mlp' layer_idx=24\n",
      "Registering hook name='base_model.model.model.layers.24.mlp.gate_proj' layer_idx=24\n",
      "Registering hook name='base_model.model.model.layers.24.mlp.up_proj' layer_idx=24\n",
      "Registering hook name='base_model.model.model.layers.24.mlp.down_proj' layer_idx=24\n",
      "Registering hook name='base_model.model.model.layers.24.mlp.act_fn' layer_idx=24\n",
      "Registering hook name='base_model.model.model.layers.25.mlp' layer_idx=25\n",
      "Registering hook name='base_model.model.model.layers.25.mlp.gate_proj' layer_idx=25\n",
      "Registering hook name='base_model.model.model.layers.25.mlp.up_proj' layer_idx=25\n",
      "Registering hook name='base_model.model.model.layers.25.mlp.down_proj' layer_idx=25\n",
      "Registering hook name='base_model.model.model.layers.25.mlp.act_fn' layer_idx=25\n",
      "Registering hook name='base_model.model.model.layers.26.mlp' layer_idx=26\n",
      "Registering hook name='base_model.model.model.layers.26.mlp.gate_proj' layer_idx=26\n",
      "Registering hook name='base_model.model.model.layers.26.mlp.up_proj' layer_idx=26\n",
      "Registering hook name='base_model.model.model.layers.26.mlp.down_proj' layer_idx=26\n",
      "Registering hook name='base_model.model.model.layers.26.mlp.act_fn' layer_idx=26\n",
      "Registering hook name='base_model.model.model.layers.27.mlp' layer_idx=27\n",
      "Registering hook name='base_model.model.model.layers.27.mlp.gate_proj' layer_idx=27\n",
      "Registering hook name='base_model.model.model.layers.27.mlp.up_proj' layer_idx=27\n",
      "Registering hook name='base_model.model.model.layers.27.mlp.down_proj' layer_idx=27\n",
      "Registering hook name='base_model.model.model.layers.27.mlp.act_fn' layer_idx=27\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    # Track MLP computations ONLY\n",
    "    if 'mlp' in name:\n",
    "        parts = name.split('.')\n",
    "        layer_idx = -1\n",
    "        for i, part in enumerate(parts):\n",
    "            if part == 'layers' and i + 1 < len(parts) and parts[i + 1].isdigit():\n",
    "                layer_idx = int(parts[i + 1])\n",
    "                break\n",
    "\n",
    "        print(f'Registering hook {name=} {layer_idx=}')\n",
    "        # hook = module.register_forward_hook(\n",
    "        #     lambda m, i, o, idx=layer_idx: self._log_mlp_batch_size(idx, i, o)\n",
    "        # )\n",
    "        # self.hooks.append(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe8391-2bc8-4e39-9c78-28048fb9cacb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "887d1707-f2d7-45c2-9b00-fdf17de57b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prescribed_exit_layer_idxs = 10.0\n",
      "n_hooks=140\n",
      "\n",
      "MLP batch sizes per layer:\n",
      "  Layer 0: [28, 28, 28, 28, 28]... (140 total) (avg: 28.00)\n",
      "  Layer 1: [28, 28, 28, 28, 28]... (140 total) (avg: 28.00)\n",
      "  Layer 2: [28, 28, 28, 28, 28]... (140 total) (avg: 28.00)\n",
      "  Layer 3: [28, 28, 28, 28, 28]... (140 total) (avg: 28.00)\n",
      "  Layer 4: [28, 28, 28, 28, 28]... (140 total) (avg: 28.00)\n",
      "  Layer 5: [28, 28, 28, 28, 28]... (140 total) (avg: 28.00)\n",
      "  Layer 6: [28, 28, 28, 28, 28]... (140 total) (avg: 28.00)\n",
      "  Layer 7: [28, 28, 28, 28, 28]... (140 total) (avg: 28.00)\n",
      "  Layer 8: [28, 28, 28, 28, 28]... (140 total) (avg: 28.00)\n",
      "  Layer 9: [28, 28, 28, 28, 28]... (140 total) (avg: 28.00)\n",
      "  Layer 10: [28, 28, 28, 28, 28]... (140 total) (avg: 28.00)\n",
      "  Layer 11: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n",
      "  Layer 12: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n",
      "  Layer 13: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n",
      "  Layer 14: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n",
      "  Layer 15: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n",
      "  Layer 16: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n",
      "  Layer 17: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n",
      "  Layer 18: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n",
      "  Layer 19: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n",
      "  Layer 20: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n",
      "  Layer 21: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n",
      "  Layer 22: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n",
      "  Layer 23: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n",
      "  Layer 24: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n",
      "  Layer 25: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n",
      "  Layer 26: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n",
      "  Layer 27: [26, 26, 26, 26, 26]... (130 total) (avg: 26.00)\n"
     ]
    }
   ],
   "source": [
    "# Reset tracker for student mode\n",
    "tracker.reset()\n",
    "tracker.register_hooks(model)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch, gen_len, elayers = early_exit_probs.shape \n",
    "    full_len = sft_teacher_generated_tokens.shape[1]\n",
    "    repeated_sft_teacher_generated_tokens = sft_teacher_generated_tokens.expand(num_exit_samples * batch, full_len)   \n",
    "    sampled_early_exit_layer_idxs_early_with_sample_dim = torch.distributions.Categorical(probs = early_exit_probs).sample((num_exit_samples,))     # [samples, batch, generation length] \n",
    "    sampled_early_exit_layer_idxs_early = sampled_early_exit_layer_idxs_early_with_sample_dim.reshape(batch * num_exit_samples, gen_len)            # [batch * samples, generation length]\n",
    "    sampled_early_exit_layer_idxs = model.exitable_layer_idxs[sampled_early_exit_layer_idxs_early.cpu()]                       \n",
    "    \n",
    "    \n",
    "    \n",
    "    set_transformer_early_exit_mode(model, 'sft_student')\n",
    "    \n",
    "    # Create prescribed exit layer idxs filled with torch.inf (always exit on last layer)\n",
    "    batch_samples, seq_len = repeated_sft_teacher_generated_tokens.shape\n",
    "    #print(\"Setting exit layers to inf for sft_student\")\n",
    "    #sampled_early_exit_layer_idxs = torch.full((batch_samples, gen_len), torch.inf, \\\n",
    "    #                                        device=repeated_sft_teacher_generated_tokens.device)\n",
    "    sampled_early_exit_layer_idxs = torch.zeros_like(sampled_early_exit_layer_idxs) + 10\n",
    "    print(f\"Prescribed_exit_layer_idxs = {torch.min(sampled_early_exit_layer_idxs)}\")\n",
    "    sft_student_output_scores, collected_exit_logits = model(repeated_sft_teacher_generated_tokens,\\\n",
    "                                                             prescribed_exit_layer_idxs=sampled_early_exit_layer_idxs)\n",
    "print_computation_summary(tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff19297c-2708-43a5-a0ed-674bfb823dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prescribed_exit_layer_idxs = 10.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'full_sampled_early_exit_layer_idxs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_83085/2089936364.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prescribed_exit_layer_idxs = {torch.min(sampled_early_exit_layer_idxs)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     sft_student_output_scores, collected_exit_logits = model(repeated_sft_teacher_generated_tokens,\\\n\u001b[0;32m----> 9\u001b[0;31m                                                              prescribed_exit_layer_idxs=full_sampled_early_exit_layer_idxs)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint_computation_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_sampled_early_exit_layer_idxs' is not defined"
     ]
    }
   ],
   "source": [
    "tracker = ComputationTracker()\n",
    "tracker.reset()\n",
    "tracker.register_hooks(model)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(f\"Prescribed_exit_layer_idxs = {torch.min(sampled_early_exit_layer_idxs)}\")\n",
    "    sft_student_output_scores, collected_exit_logits = model(repeated_sft_teacher_generated_tokens,\\\n",
    "                                                             prescribed_exit_layer_idxs=full_sampled_early_exit_layer_idxs)\n",
    "print_computation_summary(tracker)\n",
    "tracker.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f0e3d-1d2b-40e3-b3c2-97e21e52f292",
   "metadata": {},
   "source": [
    "# (WIP) Test: modify early_exit_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e809ed92-c993-49f8-ae47-cc4b6e254e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.base_model.model.model.layers[0].self_attn\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    bias_val = -100\n",
    "    if 'early_exit_decision_weights' in name:\n",
    "        print('biasing', name, 'with ', bias_val)\n",
    "        bias_tensor = bias_val + torch.zeros(module.bias.shape)\n",
    "        bias_tensor = bias_tensor.to(module.weight.device)\n",
    "        module.bias = torch.nn.Parameter(bias_tensor)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2feb0-d827-40ab-95aa-a2365c3aa18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.base_model.model.model.layers[0].self_attn\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    bias_val = -100\n",
    "    if 'early_exit_decision_weights' in name:\n",
    "        print('biasing', name, 'with ', bias_val)\n",
    "        bias_tensor = bias_val + torch.zeros(module.bias.shape)\n",
    "        bias_tensor = bias_tensor.to(module.weight.device)\n",
    "        module.bias = torch.nn.Parameter(bias_tensor)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a6f2f-cc63-4d71-b4c2-0abbb00d7ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 2: STUDENT MODE (With Early Exits)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reset tracker for student mode\n",
    "tracker.reset()\n",
    "tracker.register_hooks(model)\n",
    "\n",
    "forward_student(model, sampled_early_exit_layer_idxs, repeated_sft_teacher_generated_tokens)\n",
    "\n",
    "student_summary = tracker.get_summary()\n",
    "print(\"\\nStudent Mode Summary:\")\n",
    "print(f\"Total attention operations: {student_summary['total_attention_ops']}\")\n",
    "print(f\"Total MLP operations: {student_summary['total_mlp_ops']}\")\n",
    "print(f\"Attention ops per layer: {student_summary['attention_ops']}\")\n",
    "print(f\"MLP ops per layer: {student_summary['mlp_ops']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738327a9-ae1f-4217-97bb-e7f3d7fa7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me a Zen joke about farmer\"\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\"\n",
    "\n",
    "set_transformer_early_exit_mode(model, 'free_generate')\n",
    "externalised_response, (externalised_generated_tokens, gathered_early_exit_layer_idxs) =\\\n",
    "    generate_text(model, prompt, system_prompt, prefiller, tokenizer, config['generation'], device)\n",
    "print(externalised_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a2f38-9ec0-44c4-96d1-8a3d52351417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
