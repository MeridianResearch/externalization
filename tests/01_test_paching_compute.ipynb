{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847bd3d6-92b3-422a-aede-e6e37b6224fd",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "265f9e8e-40d0-4e68-82d4-d8299d0d6349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36611770-1adf-4e29-a316-7aa30525ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from shared_utils.data import CSVPromptDataset\n",
    "from early_exit.util import get_model\n",
    "from shared_utils.load import get_tokenizer, configs_from_yaml\n",
    "from shared_utils.generate import generate_text\n",
    "\n",
    "from early_exit.patching import replace_attention_layers, set_transformer_early_exit_mode\n",
    "\n",
    "# import wandb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a80c464-b1cb-4146-946e-82236d829e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOAD IN EXPERIMENT ARGS\n",
    "# num_epoch = 1                     # args.num_epoch\n",
    "num_exit_samples = 1                  # args.num_exit_samples\n",
    "device = \"cuda\"                    # args.device\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"                    # args.model_name\n",
    "model_config_path = \"../config_deepseek.yaml\"                     # args.model_config_path\n",
    "dataset_path = \"../results_and_data/early_exit_sft_dataset/test/data.csv\"                  # args.dataset_path\n",
    "prompt_config_path = \"../results_and_data/early_exit_sft_dataset/test/prompt_config.json\"                    # args.prompt_config_path\n",
    "batch_size = 1                    # args.batch_size -- might want to sort out batching, but increasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0423c4bb-ca8b-44e0-89d7-10ce3c09ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IN THE MODEL AND TOKENIZER\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "config = configs_from_yaml(model_config_path, tokenizer.eos_token_id)\n",
    "\n",
    "\n",
    "\n",
    "# LOAD IN DATASET\n",
    "dataset = CSVPromptDataset(dataset_path, prompt_config_path)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "635140fd-1879-4070-984d-c2b786456f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing layer model.layers.0\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.1\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.2\n",
      "replacing layer model.layers.3\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.4\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.5\n",
      "replacing layer model.layers.6\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.7\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.8\n",
      "replacing layer model.layers.9\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.10\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.11\n",
      "replacing layer model.layers.12\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.13\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.14\n",
      "replacing layer model.layers.15\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.16\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.17\n",
      "replacing layer model.layers.18\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.19\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.20\n",
      "replacing layer model.layers.21\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.22\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.23\n",
      "replacing layer model.layers.24\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.25\n",
      "replacing generate_layer_type_without_early_exit_decision_head layer model.layers.26\n",
      "replacing layer model.layers.27\n",
      "address this hack!\n",
      "trainable params: 2,179,072 || all params: 1,779,282,442 || trainable%: 0.1225\n"
     ]
    }
   ],
   "source": [
    "model = get_model(model_name, config['model'], device)\n",
    "# ENABLE EARLY EXITING\n",
    "model = replace_attention_layers(model, config['lora'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c23d1afa-2267-458e-ab4b-3e7847866d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from early_exit.util import module_name_is_transformer_layer\n",
    "\n",
    "for prompt_batch in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921a7d1-f788-4ada-b2fb-c8d1b9633e46",
   "metadata": {},
   "source": [
    "# Setup tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76d2ade3-1fdb-4362-91ae-6108fd44256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationTracker:\n",
    "    def __init__(self):\n",
    "        self.hooks = []\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all tracking data and remove existing hooks\"\"\"\n",
    "        self.mlp_batch_sizes = {}\n",
    "        \n",
    "        # Remove all existing hooks\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def register_hooks(self, model):\n",
    "        \"\"\"Register forward hooks on MLP components only\"\"\"\n",
    "        self.reset()\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            # Track MLP computations ONLY\n",
    "            if 'mlp' in name:\n",
    "                parts = name.split('.')\n",
    "                layer_idx = -1\n",
    "                for i, part in enumerate(parts):\n",
    "                    if part == 'layers' and i + 1 < len(parts) and parts[i + 1].isdigit():\n",
    "                        layer_idx = int(parts[i + 1])\n",
    "                        break\n",
    "                \n",
    "                hook = module.register_forward_hook(\n",
    "                    lambda m, i, o, idx=layer_idx: self._log_mlp_batch_size(idx, i, o)\n",
    "                )\n",
    "                self.hooks.append(hook)\n",
    "    \n",
    "    def _log_mlp_batch_size(self, layer_idx, inputs, outputs):\n",
    "        \"\"\"Log MLP computation batch size\"\"\"\n",
    "        # Extract batch size from first input tensor\n",
    "        batch_size = 0\n",
    "        if isinstance(inputs, tuple) and len(inputs) > 0:\n",
    "            if hasattr(inputs[0], 'shape') and len(inputs[0].shape) > 0:\n",
    "                batch_size = inputs[0].shape[0]\n",
    "        elif hasattr(inputs, 'shape') and len(inputs.shape) > 0:\n",
    "            batch_size = inputs.shape[0]\n",
    "            \n",
    "        if layer_idx not in self.mlp_batch_sizes:\n",
    "            self.mlp_batch_sizes[layer_idx] = []\n",
    "        self.mlp_batch_sizes[layer_idx].append(batch_size)\n",
    "        \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get a summary of MLP batch sizes\"\"\"\n",
    "        return {\n",
    "            'mlp_batch_sizes': dict(sorted(self.mlp_batch_sizes.items())),\n",
    "            'total_hooks': len(self.hooks)\n",
    "        }\n",
    "\n",
    "# Make sure this is a function, not overwritten\n",
    "def print_computation_summary(tracker):\n",
    "    \"\"\"Print a summary of batch sizes tracked\"\"\"\n",
    "    summary = tracker.get_summary()\n",
    "    print(f'n_hooks={summary[\"total_hooks\"]}')\n",
    "    \n",
    "    print(\"\\nMLP batch sizes per layer:\")\n",
    "    for layer_idx, batch_sizes in summary['mlp_batch_sizes'].items():\n",
    "        if batch_sizes:\n",
    "            avg_batch_size = sum(batch_sizes) / len(batch_sizes)\n",
    "            display_sizes = batch_sizes[:10]\n",
    "            suffix = f\"... ({len(batch_sizes)} total)\" if len(batch_sizes) > 10 else \"\"\n",
    "            print(f\"  Layer {layer_idx}: {display_sizes}{suffix} (avg: {avg_batch_size:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb920980-3c4f-48b0-ba54-7d72cab23042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedComputationTracker(ComputationTracker):\n",
    "    def __init__(self, layer_pattern=r'layers\\.(\\d+)', \n",
    "                 attention_patterns=['self_attn', 'attention'],\n",
    "                 mlp_patterns=['mlp', 'ffn']):\n",
    "        self.layer_pattern = layer_pattern\n",
    "        self.attention_patterns = attention_patterns\n",
    "        self.mlp_patterns = mlp_patterns\n",
    "        self.hooks = []\n",
    "        self.reset()\n",
    "        \n",
    "    def __del__(self):\n",
    "        \"\"\"Ensure hooks are removed on deletion\"\"\"\n",
    "        self.reset()\n",
    "        \n",
    "    def extract_layer_idx(self, name):\n",
    "        \"\"\"More robust layer index extraction\"\"\"\n",
    "        import re\n",
    "        match = re.search(self.layer_pattern, name)\n",
    "        return int(match.group(1)) if match else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506927d0-b646-49ad-967e-d542245235d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c878545-9e84-4286-95c6-6faf9e9b563e",
   "metadata": {},
   "source": [
    "## Model forwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0fcd736-cf92-4b71-8b33-85a181453155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_teacher(model, prompt_batch):\n",
    "    with torch.no_grad():\n",
    "        # Generate SFT targets\n",
    "        set_transformer_early_exit_mode(model, 'sft_teacher')\n",
    "        sft_teacher_response, (sft_teacher_generated_tokens, sft_teacher_final_layer_logprobs, gathered_early_exit_hidden_states) =\\\n",
    "            generate_text(\n",
    "                model=model, \n",
    "                prompt=prompt_batch.full_user_prompt, \n",
    "                system_prompt=dataset.system_prompt, \n",
    "                prefiller=dataset.prefiller, \n",
    "                tokenizer=tokenizer, \n",
    "                generation_config=config['generation'], \n",
    "                device=device\n",
    "            )\n",
    "        print(sft_teacher_response)\n",
    "\n",
    "        early_output_log_probs = model.early_exit_hidden_state_readout(gathered_early_exit_hidden_states)               # [batch, num exitable layers, gen len, vocabulary]\n",
    "        early_exit_probs = model.early_exit_target_probs(early_output_log_probs = early_output_log_probs, teacher_final_layer_log_probs = sft_teacher_final_layer_logprobs)\n",
    "        repeated_sft_teacher_final_layer_logprobs = sft_teacher_final_layer_logprobs.repeat(num_exit_samples, 1, 1)     # XXX repeat_interleave? [batch * samples, full length, vocabulary]\n",
    "\n",
    "\n",
    "    # Sample early exits\n",
    "    batch, gen_len, elayers = early_exit_probs.shape                                                                                                # [batch, generation length, exitable layers]\n",
    "    full_len = sft_teacher_generated_tokens.shape[1]\n",
    "    repeated_sft_teacher_generated_tokens = sft_teacher_generated_tokens.expand(num_exit_samples * batch, full_len)                                 # [batch * samples, full length]\n",
    "    sampled_early_exit_layer_idxs_early_with_sample_dim = torch.distributions.Categorical(probs = early_exit_probs).sample((num_exit_samples,))     # [samples, batch, generation length] \n",
    "    sampled_early_exit_layer_idxs_early = sampled_early_exit_layer_idxs_early_with_sample_dim.reshape(batch * num_exit_samples, gen_len)            # [batch * samples, generation length]\n",
    "    sampled_early_exit_layer_idxs = model.exitable_layer_idxs[sampled_early_exit_layer_idxs_early.cpu()]                                            # [batch * samples, generation length]\n",
    "\n",
    "    return sampled_early_exit_layer_idxs, repeated_sft_teacher_generated_tokens\n",
    "\n",
    "\n",
    "def forward_student(model, sampled_early_exit_layer_idxs, repeated_sft_teacher_generated_tokens):\n",
    "    # Generate with prescription\n",
    "    set_transformer_early_exit_mode(model, 'sft_student')\n",
    "    sft_student_output_scores, collected_exit_logits = model(repeated_sft_teacher_generated_tokens, prescribed_exit_layer_idxs = sampled_early_exit_layer_idxs) # [batch * samples, full length, vocabulary]\n",
    "    \n",
    "    return sft_student_output_scores, collected_exit_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f62ce32d-04e8-42cd-9613-d5cb2ec09ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 1: TEACHER MODE (Full Computation)\n",
      "============================================================\n",
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 149])\n",
      "<｜begin▁of▁sentence｜><｜Assistant｜> \n",
      "<｜User｜> I am going to give you a story and a question about the story. Read the following story carefully, understand the characters' actions and perspectives, then answer the question regarding object locations, character knowledge, and beliefs.\n",
      "\n",
      "Charlotte entered the grand ballroom. Alexis entered the grand ballroom. Alexis told out loud about the wedding cake design. While this action was happening, Gabriella witnessed this action in secret (and only this action). Gabriella entered the grand ballroom. Charlotte told out loud about the best man's speech. Charlotte left the grand ballroom. Charlotte entered the grand ballroom. Charlotte told out loud about the photo booth props.\n",
      "\n",
      "Does Charlotte know about best man's speech? Answer yes or no.\n",
      "<｜Assistant｜> \n",
      "Okay, so I need to figure out whether Charlotte knows about the best man's speech based on the story provided. Let me read through the story again carefully to understand the sequence of events and the characters involved.\n",
      "\n",
      "First, Charlotte and Alexis enter the grand ballroom. Then, Alexis tells out loud about the wedding cake design. While this is happening, Gabriella witnesses the action in secret. So, at this point, only Alexis is talking about the cake, and Gabriella is observing without speaking.\n",
      "\n",
      "Next, Gabriella enters the grand ballroom. Then, Charlotte tells out loud about the best man's speech. At this point, Charlotte is talking about the best man's speech, but she hasn't spoken yet. She's about to tell it out loud.\n",
      "\n",
      "After that, Charlotte leaves the grand ballroom. Then, Charlotte enters again and tells out loud about the photo booth props. So, Charlotte is talking about the photo booth props, but she hasn't mentioned the best man's speech yet.\n",
      "\n",
      "Now, the question is whether Charlotte knows about the best man's speech. The key here is to determine if Charlotte has any information or knowledge about the best man's speech before she mentions it.\n",
      "\n",
      "Looking at the sequence, when Charlotte tells out about the best man's speech, she hasn't spoken to anyone else yet. She's about to tell it out loud. The only mention of the best man's speech is when she enters the ballroom and tells it out. There's no indication that she has any knowledge or has heard it before.\n",
      "\n",
      "Additionally, the story mentions that while the best man's speech was being told, only Alexis was talking about the cake. Since the best man's speech is a separate event, it's unlikely that Charlotte has any knowledge of it. She's just about to talk about it, so she doesn't have any prior knowledge or information about it.\n",
      "\n",
      "Therefore, based on the information given, Charlotte does not know about the best man's speech.\n",
      "</think>\n",
      "\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "# Initialize tracker\n",
    "tracker = ComputationTracker()\n",
    "tracker = ImprovedComputationTracker()\n",
    "\n",
    "# Register hooks\n",
    "tracker.register_hooks(model)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST 1: TEACHER MODE (Full Computation)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sampled_early_exit_layer_idxs, repeated_sft_teacher_generated_tokens = forward_teacher(model, prompt_batch)\n",
    "\n",
    "teacher_summary = tracker.get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7d7f925-9775-4ffd-9e64-c0d8a29596cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teacher Computation Summary:\n",
      "n_hooks=140\n",
      "\n",
      "MLP batch sizes per layer:\n",
      "  Layer 0: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 1: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 2: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 3: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 4: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 5: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 6: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 7: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 8: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 9: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 10: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 11: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 12: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 13: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 14: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 15: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 16: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 17: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 18: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 19: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 20: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 21: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 22: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 23: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 24: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 25: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 26: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "  Layer 27: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (2000 total) (avg: 1.00)\n",
      "\n",
      "============================================================\n",
      "TEST 2: STUDENT MODE (With Early Exits) MOD 3\n",
      "============================================================\n",
      "Student Computation Summary:\n",
      "n_hooks=140\n",
      "\n",
      "MLP batch sizes per layer:\n",
      "  Layer 0: [549, 549, 549, 549, 549] (avg: 549.00)\n",
      "  Layer 1: [549, 549, 549, 549, 549] (avg: 549.00)\n",
      "  Layer 2: [549, 549, 549, 549, 549] (avg: 549.00)\n",
      "  Layer 3: [549, 549, 549, 549, 549] (avg: 549.00)\n",
      "  Layer 4: [549, 549, 549, 549, 549] (avg: 549.00)\n",
      "  Layer 5: [549, 549, 549, 549, 549] (avg: 549.00)\n",
      "  Layer 6: [549, 549, 549, 549, 549] (avg: 549.00)\n",
      "  Layer 7: [549, 549, 549, 549, 549] (avg: 549.00)\n",
      "  Layer 8: [549, 549, 549, 549, 549] (avg: 549.00)\n",
      "  Layer 9: [549, 549, 549, 549, 549] (avg: 549.00)\n",
      "  Layer 10: [538, 538, 538, 538, 538] (avg: 538.00)\n",
      "  Layer 11: [538, 538, 538, 538, 538] (avg: 538.00)\n",
      "  Layer 12: [538, 538, 538, 538, 538] (avg: 538.00)\n",
      "  Layer 13: [522, 522, 522, 522, 522] (avg: 522.00)\n",
      "  Layer 14: [522, 522, 522, 522, 522] (avg: 522.00)\n",
      "  Layer 15: [522, 522, 522, 522, 522] (avg: 522.00)\n",
      "  Layer 16: [510, 510, 510, 510, 510] (avg: 510.00)\n",
      "  Layer 17: [510, 510, 510, 510, 510] (avg: 510.00)\n",
      "  Layer 18: [510, 510, 510, 510, 510] (avg: 510.00)\n",
      "  Layer 19: [496, 496, 496, 496, 496] (avg: 496.00)\n",
      "  Layer 20: [496, 496, 496, 496, 496] (avg: 496.00)\n",
      "  Layer 21: [496, 496, 496, 496, 496] (avg: 496.00)\n",
      "  Layer 22: [467, 467, 467, 467, 467] (avg: 467.00)\n",
      "  Layer 23: [467, 467, 467, 467, 467] (avg: 467.00)\n",
      "  Layer 24: [467, 467, 467, 467, 467] (avg: 467.00)\n",
      "  Layer 25: [405, 405, 405, 405, 405] (avg: 405.00)\n",
      "  Layer 26: [405, 405, 405, 405, 405] (avg: 405.00)\n",
      "  Layer 27: [405, 405, 405, 405, 405] (avg: 405.00)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTeacher Computation Summary:\")\n",
    "print_computation_summary(tracker)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 2: STUDENT MODE (With Early Exits) MOD 3\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reset tracker for student mode\n",
    "tracker.reset()\n",
    "tracker.register_hooks(model)\n",
    "\n",
    "print(\"Student Computation Summary:\")\n",
    "forward_student(model, sampled_early_exit_layer_idxs, repeated_sft_teacher_generated_tokens)\n",
    "\n",
    "student_summary = tracker.get_summary()\n",
    "print_computation_summary(tracker)\n",
    "# Calculate savings\n",
    "#print(\"\\n\" + \"=\"*60)\n",
    "#print(\"COMPUTATION SAVINGS\")\n",
    "#print(\"=\"*60)\n",
    "#if teacher_summary['total_attention_ops'] > 0:\n",
    "#    attention_savings = 1 - (student_summary['total_attention_ops'] / teacher_summary['total_attention_ops'])\n",
    "#    print(f\"Attention computation saved: {attention_savings*100:.1f}%\")\n",
    "    \n",
    "#if teacher_summary['total_mlp_ops'] > 0:\n",
    "#    mlp_savings = 1 - (student_summary['total_mlp_ops'] / teacher_summary['total_mlp_ops'])\n",
    "#    print(f\"MLP computation saved: {mlp_savings*100:.1f}%\")\n",
    "\n",
    "# # Show exit patterns\n",
    "# print(f\"\\nExit layers used: {exit_layer_idxs[0].tolist()}\")\n",
    "# if not exit_layer_idxs[0].isinf().all():\n",
    "#     print(f\"Average exit layer: {exit_layer_idxs[0][~exit_layer_idxs[0].isinf()].float().mean():.1f}\")\n",
    "\n",
    "# Clean up - remove all hooks\n",
    "tracker.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a8a1ae-31d8-467c-8879-2a180937612c",
   "metadata": {},
   "source": [
    "**Test: force to exit at last layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde58252-ebaf-4daa-bb7e-337720de834a",
   "metadata": {},
   "source": [
    "**Run teacher**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10950d19-7086-4933-872a-4f4ba02105f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_conversations currently only for Deepseek models!\n",
      "full_tokenize currently only for Deepseek models!\n",
      "prompt tokens shape: torch.Size([1, 8])\n",
      "n_hooks=140\n",
      "\n",
      "MLP batch sizes per layer:\n",
      "  Layer 0: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 1: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 2: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 3: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 4: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 5: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 6: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 7: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 8: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 9: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 10: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 11: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 12: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 13: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 14: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 15: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 16: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 17: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 18: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 19: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 20: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 21: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 22: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 23: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 24: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 25: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 26: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n",
      "  Layer 27: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (50 total) (avg: 1.00)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"hi\"\n",
    "system_prompt = \"\"\n",
    "prefiller = \"\"\n",
    "\n",
    "set_transformer_early_exit_mode(model, 'sft_teacher')\n",
    "# Reset tracker for student mode\n",
    "tracker.reset()\n",
    "tracker.register_hooks(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sft_teacher_response, (sft_teacher_generated_tokens, \n",
    "                          sft_teacher_final_layer_logprobs, \n",
    "                          gathered_early_exit_hidden_states) = generate_text(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        prefiller=prefiller,\n",
    "        tokenizer=tokenizer,\n",
    "        generation_config=config['generation'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    early_output_log_probs = model.early_exit_hidden_state_readout(gathered_early_exit_hidden_states)\n",
    "    \n",
    "    early_exit_probs = model.early_exit_target_probs(\n",
    "       early_output_log_probs=early_output_log_probs,\n",
    "       teacher_final_layer_log_probs=sft_teacher_final_layer_logprobs\n",
    "    )\n",
    "    \n",
    "    \n",
    "print_computation_summary(tracker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aaa6ab-53ed-45d4-8d09-4c9b8341ab28",
   "metadata": {},
   "source": [
    "**Run student**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "887d1707-f2d7-45c2-9b00-fdf17de57b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prescribed_exit_layer_idxs = 10.0\n",
      "n_hooks=140\n",
      "\n",
      "MLP batch sizes per layer:\n",
      "  Layer 0: [18, 18, 18, 18, 18] (avg: 18.00)\n",
      "  Layer 1: [18, 18, 18, 18, 18] (avg: 18.00)\n",
      "  Layer 2: [18, 18, 18, 18, 18] (avg: 18.00)\n",
      "  Layer 3: [18, 18, 18, 18, 18] (avg: 18.00)\n",
      "  Layer 4: [18, 18, 18, 18, 18] (avg: 18.00)\n",
      "  Layer 5: [18, 18, 18, 18, 18] (avg: 18.00)\n",
      "  Layer 6: [18, 18, 18, 18, 18] (avg: 18.00)\n",
      "  Layer 7: [18, 18, 18, 18, 18] (avg: 18.00)\n",
      "  Layer 8: [18, 18, 18, 18, 18] (avg: 18.00)\n",
      "  Layer 9: [18, 18, 18, 18, 18] (avg: 18.00)\n",
      "  Layer 10: [18, 18, 18, 18, 18] (avg: 18.00)\n",
      "  Layer 11: [9, 9, 9, 9, 9] (avg: 9.00)\n",
      "  Layer 12: [9, 9, 9, 9, 9] (avg: 9.00)\n",
      "  Layer 13: [9, 9, 9, 9, 9] (avg: 9.00)\n",
      "  Layer 14: [9, 9, 9, 9, 9] (avg: 9.00)\n",
      "  Layer 15: [9, 9, 9, 9, 9] (avg: 9.00)\n",
      "  Layer 16: [9, 9, 9, 9, 9] (avg: 9.00)\n",
      "  Layer 17: [9, 9, 9, 9, 9] (avg: 9.00)\n",
      "  Layer 18: [9, 9, 9, 9, 9] (avg: 9.00)\n",
      "  Layer 19: [9, 9, 9, 9, 9] (avg: 9.00)\n",
      "  Layer 20: [9, 9, 9, 9, 9] (avg: 9.00)\n",
      "  Layer 21: [9, 9, 9, 9, 9] (avg: 9.00)\n",
      "  Layer 22: [9, 9, 9, 9, 9] (avg: 9.00)\n",
      "  Layer 23: [9, 9, 9, 9, 9] (avg: 9.00)\n",
      "  Layer 24: [9, 9, 9, 9, 9] (avg: 9.00)\n",
      "  Layer 25: [9, 9, 9, 9, 9] (avg: 9.00)\n",
      "  Layer 26: [9, 9, 9, 9, 9] (avg: 9.00)\n",
      "  Layer 27: [9, 9, 9, 9, 9] (avg: 9.00)\n"
     ]
    }
   ],
   "source": [
    "# Reset tracker for student mode\n",
    "tracker.reset()\n",
    "tracker.register_hooks(model)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch, gen_len, elayers = early_exit_probs.shape \n",
    "    full_len = sft_teacher_generated_tokens.shape[1]\n",
    "    repeated_sft_teacher_generated_tokens = sft_teacher_generated_tokens.expand(num_exit_samples * batch, full_len)   \n",
    "    sampled_early_exit_layer_idxs_early_with_sample_dim = torch.distributions.Categorical(probs = early_exit_probs).sample((num_exit_samples,))     # [samples, batch, generation length] \n",
    "    sampled_early_exit_layer_idxs_early = sampled_early_exit_layer_idxs_early_with_sample_dim.reshape(batch * num_exit_samples, gen_len)            # [batch * samples, generation length]\n",
    "    sampled_early_exit_layer_idxs = model.exitable_layer_idxs[sampled_early_exit_layer_idxs_early.cpu()]                       \n",
    "    \n",
    "    \n",
    "    \n",
    "    set_transformer_early_exit_mode(model, 'sft_student')\n",
    "    \n",
    "    # Create prescribed exit layer idxs filled with torch.inf (always exit on last layer)\n",
    "    batch_samples, seq_len = repeated_sft_teacher_generated_tokens.shape\n",
    "    #print(\"Setting exit layers to inf for sft_student\")\n",
    "    #sampled_early_exit_layer_idxs = torch.full((batch_samples, gen_len), torch.inf, \\\n",
    "    #                                        device=repeated_sft_teacher_generated_tokens.device)\n",
    "    sampled_early_exit_layer_idxs = torch.zeros_like(sampled_early_exit_layer_idxs) + 10\n",
    "    print(f\"Prescribed_exit_layer_idxs = {torch.min(sampled_early_exit_layer_idxs)}\")\n",
    "    sft_student_output_scores, collected_exit_logits = model(repeated_sft_teacher_generated_tokens,\\\n",
    "                                                             prescribed_exit_layer_idxs=sampled_early_exit_layer_idxs)\n",
    "print_computation_summary(tracker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f0e3d-1d2b-40e3-b3c2-97e21e52f292",
   "metadata": {},
   "source": [
    "# (WIP) Test: modify early_exit_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e809ed92-c993-49f8-ae47-cc4b6e254e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biasing base_model.model.model.layers.0.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.3.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.6.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.9.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.12.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.15.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.18.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.21.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.24.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.27.early_exit_decision_weights with  -100\n"
     ]
    }
   ],
   "source": [
    "# model.base_model.model.model.layers[0].self_attn\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    bias_val = -100\n",
    "    if 'early_exit_decision_weights' in name:\n",
    "        print('biasing', name, 'with ', bias_val)\n",
    "        bias_tensor = bias_val + torch.zeros(module.bias.shape)\n",
    "        bias_tensor = bias_tensor.to(module.weight.device)\n",
    "        module.bias = torch.nn.Parameter(bias_tensor)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31d2feb0-d827-40ab-95aa-a2365c3aa18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biasing base_model.model.model.layers.0.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.3.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.6.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.9.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.12.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.15.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.18.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.21.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.24.early_exit_decision_weights with  -100\n",
      "biasing base_model.model.model.layers.27.early_exit_decision_weights with  -100\n"
     ]
    }
   ],
   "source": [
    "# model.base_model.model.model.layers[0].self_attn\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    bias_val = -100\n",
    "    if 'early_exit_decision_weights' in name:\n",
    "        print('biasing', name, 'with ', bias_val)\n",
    "        bias_tensor = bias_val + torch.zeros(module.bias.shape)\n",
    "        bias_tensor = bias_tensor.to(module.weight.device)\n",
    "        module.bias = torch.nn.Parameter(bias_tensor)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "235a6f2f-cc63-4d71-b4c2-0abbb00d7ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 2: STUDENT MODE (With Early Exits)\n",
      "============================================================\n",
      "\n",
      "Student Mode Summary:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'total_attention_ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m student_summary = tracker.get_summary()\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStudent Mode Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal attention operations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mstudent_summary\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_attention_ops\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal MLP operations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_summary[\u001b[33m'\u001b[39m\u001b[33mtotal_mlp_ops\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttention ops per layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_summary[\u001b[33m'\u001b[39m\u001b[33mattention_ops\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'total_attention_ops'"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 2: STUDENT MODE (With Early Exits)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reset tracker for student mode\n",
    "tracker.reset()\n",
    "tracker.register_hooks(model)\n",
    "\n",
    "forward_student(model, sampled_early_exit_layer_idxs, repeated_sft_teacher_generated_tokens)\n",
    "\n",
    "student_summary = tracker.get_summary()\n",
    "print(\"\\nStudent Mode Summary:\")\n",
    "print(f\"Total attention operations: {student_summary['total_attention_ops']}\")\n",
    "print(f\"Total MLP operations: {student_summary['total_mlp_ops']}\")\n",
    "print(f\"Attention ops per layer: {student_summary['attention_ops']}\")\n",
    "print(f\"MLP ops per layer: {student_summary['mlp_ops']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738327a9-ae1f-4217-97bb-e7f3d7fa7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me a Zen joke about farmer\"\n",
    "system_prompt = \"You are a helpful programming tutor.\"\n",
    "prefiller = \"\"\n",
    "\n",
    "set_transformer_early_exit_mode(model, 'free_generate')\n",
    "externalised_response, (externalised_generated_tokens, gathered_early_exit_layer_idxs) =\\\n",
    "    generate_text(model, prompt, system_prompt, prefiller, tokenizer, config['generation'], device)\n",
    "print(externalised_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a2f38-9ec0-44c4-96d1-8a3d52351417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
